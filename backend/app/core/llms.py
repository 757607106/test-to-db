import os
import logging
from typing import Optional
from langchain_deepseek import ChatDeepSeek
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from sqlalchemy.orm import Session

from app.core.config import settings
from app.db.session import SessionLocal
from app.models.llm_config import LLMConfiguration

logger = logging.getLogger(__name__)

def get_active_llm_config(model_type: str = "chat") -> Optional[LLMConfiguration]:
    """
    Fetch the active LLM configuration from the database.
    æŒ‰ ID é™åºè¿”å›ç¬¬ä¸€ä¸ªæ´»è·ƒé…ç½®ï¼ˆæœ€æ–°åˆ›å»ºçš„ï¼‰
    """
    db: Session = SessionLocal()
    try:
        config = db.query(LLMConfiguration).filter(
            LLMConfiguration.is_active == True,
            LLMConfiguration.model_type == model_type
        ).order_by(LLMConfiguration.id.desc()).first()  # æŒ‰ ID é™åºï¼Œä½¿ç”¨æœ€æ–°é…ç½®
        # If specific type not found, try any active one (fallback logic could be better)
        if not config and model_type == "chat":
             config = db.query(LLMConfiguration).filter(LLMConfiguration.is_active == True).order_by(LLMConfiguration.id.desc()).first()
        
        if config:
            logger.info(f"Found active LLM config in DB: provider={config.provider}, model={config.model_name}, base_url={config.base_url}, id={config.id}")
        else:
            logger.info(f"No active LLM config found in DB for type {model_type}")
            
        return config
    except Exception as e:
        logger.error(f"Error fetching LLM config from DB: {e}")
        return None
    finally:
        db.close()

def get_default_model(config_override: Optional[LLMConfiguration] = None, caller: str = None):
    """
    Get LLM model instance.
    
    Args:
        config_override: æŒ‡å®šçš„ LLM é…ç½®
        caller: è°ƒç”¨è€…æ ‡è¯†ï¼Œç”¨äºæ—¥å¿—è¿½è¸ª
    
    Returns:
        BaseChatModelå®ä¾‹
    
    Raises:
        Exception: å½“æ¨¡å‹åˆå§‹åŒ–å¤±è´¥æ—¶
    """
    try:
        # Try to get from DB if no override
        config = config_override or get_active_llm_config(model_type="chat")

        if config:
            api_key = config.api_key
            api_base = config.base_url
            model_name = config.model_name
            provider = config.provider.lower()
            
            # æ‰“å°è¯¦ç»†çš„æ¨¡å‹åˆå§‹åŒ–æ—¥å¿—
            print(f"\nğŸ“¡ LLM æ¨¡å‹åˆå§‹åŒ–")
            print(f"   æä¾›å•†: {config.provider}")
            print(f"   æ¨¡å‹: {model_name}")
            print(f"   API Base: {api_base or 'é»˜è®¤'}")
            if caller:
                print(f"   è°ƒç”¨è€…: {caller}")
            
            logger.info(
                f"Initializing LLM model: provider={provider}, "
                f"model={model_name}, base_url={api_base or 'default'}, "
                f"caller={caller or 'unknown'}"
            )
        else:
            # Fallback to settings
            api_key = settings.OPENAI_API_KEY
            api_base = settings.OPENAI_API_BASE
            model_name = settings.LLM_MODEL
            provider = settings.LLM_PROVIDER.lower()
            
            print(f"\nğŸ“¡ LLM æ¨¡å‹åˆå§‹åŒ– (ç¯å¢ƒå˜é‡é…ç½®)")
            print(f"   æä¾›å•†: {provider}")
            print(f"   æ¨¡å‹: {model_name}")
            print(f"   API Base: {api_base or 'é»˜è®¤'}")
            if caller:
                print(f"   è°ƒç”¨è€…: {caller}")
            
            logger.info(
                f"Initializing LLM model from env: provider={provider}, "
                f"model={model_name}, caller={caller or 'unknown'}"
            )

        # Common parameters
        max_tokens = 8192
        temperature = 0.2

        if provider == "openai" or provider == "aliyun" or provider == "volcengine": 
            return ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=30.0,
                max_retries=3
            )
        elif provider == "deepseek":
            os.environ["DEEPSEEK_API_KEY"] = api_key
            if api_base:
                os.environ["DEEPSEEK_API_BASE"] = api_base
            
            return ChatDeepSeek(
                model=model_name,
                max_tokens=max_tokens,
                temperature=temperature,
                api_key=api_key,
                api_base=api_base,
                timeout=30.0,
                max_retries=3
            )
        else:
            # Default fallback to ChatOpenAI
            logger.warning(f"Unknown provider '{provider}', falling back to ChatOpenAI")
            return ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=30.0,
                max_retries=3
            )
    except Exception as e:
        logger.error(
            f"Failed to initialize LLM model: {e}, "
            f"provider={provider if 'provider' in locals() else 'unknown'}, "
            f"model={model_name if 'model_name' in locals() else 'unknown'}",
            exc_info=True
        )
        raise

def get_default_embedding_model():
    """
    Get Embedding model instance.
    """
    config = get_active_llm_config(model_type="embedding")
    
    if config:
        api_key = config.api_key
        api_base = config.base_url
        model_name = config.model_name
        provider = config.provider.lower()
        
        print(f"\nğŸ“¡ Embedding æ¨¡å‹åˆå§‹åŒ–")
        print(f"   æä¾›å•†: {config.provider}")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   API Base: {api_base or 'é»˜è®¤'}")
    else:
        # Fallback based on VECTOR_SERVICE_TYPE
        if settings.VECTOR_SERVICE_TYPE == "aliyun":
             api_key = settings.DASHSCOPE_API_KEY
             api_base = settings.DASHSCOPE_BASE_URL
             model_name = settings.DASHSCOPE_EMBEDDING_MODEL
        else:
             api_key = settings.OPENAI_API_KEY
             api_base = settings.OPENAI_API_BASE
             model_name = "text-embedding-3-small" # Default fallback
        
        print(f"\nğŸ“¡ Embedding æ¨¡å‹åˆå§‹åŒ– (ç¯å¢ƒå˜é‡é…ç½®)")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   API Base: {api_base or 'é»˜è®¤'}")
        
    return OpenAIEmbeddings(
        model=model_name,
        api_key=api_key,
        base_url=api_base
    )


def create_llm_from_config(config: LLMConfiguration):
    """
    æ ¹æ®LLMConfigurationåˆ›å»ºLLMå®ä¾‹ã€‚
    è¿™æ˜¯get_default_modelçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨ç”¨äºä»é…ç½®åˆ›å»ºæ¨¡å‹ã€‚
    
    Args:
        config: LLMé…ç½®å¯¹è±¡
    
    Returns:
        BaseChatModelå®ä¾‹
    
    Raises:
        ValueError: å½“é…ç½®æ— æ•ˆæ—¶
        Exception: å½“æ¨¡å‹åˆå§‹åŒ–å¤±è´¥æ—¶
    """
    if not config:
        raise ValueError("LLM configuration cannot be None")
    
    if not config.is_active:
        raise ValueError(f"LLM configuration (id={config.id}) is not active")
    
    try:
        api_key = config.api_key
        api_base = config.base_url
        model_name = config.model_name
        provider = config.provider.lower()
        
        logger.info(
            f"Creating LLM from config: id={config.id}, "
            f"provider={provider}, model={model_name}"
        )
        
        # Common parameters
        max_tokens = 8192
        temperature = 0.2
        
        if provider == "openai" or provider == "aliyun" or provider == "volcengine":
            return ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=30.0,
                max_retries=3
            )
        elif provider == "deepseek":
            os.environ["DEEPSEEK_API_KEY"] = api_key
            if api_base:
                os.environ["DEEPSEEK_API_BASE"] = api_base
            
            return ChatDeepSeek(
                model=model_name,
                max_tokens=max_tokens,
                temperature=temperature,
                api_key=api_key,
                api_base=api_base,
                timeout=30.0,
                max_retries=3
            )
        else:
            logger.warning(f"Unknown provider '{provider}', falling back to ChatOpenAI")
            return ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=30.0,
                max_retries=3
            )
    except Exception as e:
        logger.error(
            f"Failed to create LLM from config (id={config.id}): {e}",
            exc_info=True
        )
        raise
