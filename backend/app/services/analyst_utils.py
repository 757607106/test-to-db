"""
åˆ†æå·¥å…·å‡½æ•°æ¨¡å—
æä¾›æ•°æ®åˆ†æã€ç»Ÿè®¡è®¡ç®—ã€å¼‚å¸¸æ£€æµ‹ç­‰åŠŸèƒ½
"""
from typing import Dict, Any, List, Optional, Tuple
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


def calculate_statistics(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è®¡ç®—æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        data: æŸ¥è¯¢ç»“æœæ•°æ®åˆ—è¡¨
        
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    try:
        if not data:
            return {"error": "ç©ºæ•°æ®é›†"}
        
        df = pd.DataFrame(data)
        
        stats = {
            "total_rows": len(df),
            "columns": list(df.columns),
            "numeric_columns": [],
            "text_columns": [],
            "date_columns": [],
            "summary": {}
        }
        
        # åˆ†ç±»åˆ—
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                stats["numeric_columns"].append(col)
                # æ•°å€¼åˆ—ç»Ÿè®¡
                stats["summary"][col] = {
                    "type": "numeric",
                    "count": int(df[col].count()),
                    "mean": float(df[col].mean()) if df[col].count() > 0 else None,
                    "median": float(df[col].median()) if df[col].count() > 0 else None,
                    "min": float(df[col].min()) if df[col].count() > 0 else None,
                    "max": float(df[col].max()) if df[col].count() > 0 else None,
                    "std": float(df[col].std()) if df[col].count() > 1 else None,
                    "sum": float(df[col].sum()) if df[col].count() > 0 else None
                }
            elif pd.api.types.is_datetime64_any_dtype(df[col]):
                stats["date_columns"].append(col)
                stats["summary"][col] = {
                    "type": "datetime",
                    "count": int(df[col].count()),
                    "min": str(df[col].min()) if df[col].count() > 0 else None,
                    "max": str(df[col].max()) if df[col].count() > 0 else None
                }
            else:
                stats["text_columns"].append(col)
                stats["summary"][col] = {
                    "type": "text",
                    "count": int(df[col].count()),
                    "unique": int(df[col].nunique()),
                    "top_values": df[col].value_counts().head(5).to_dict() if df[col].count() > 0 else {}
                }
        
        return stats
        
    except Exception as e:
        return {"error": f"ç»Ÿè®¡è®¡ç®—é”™è¯¯: {str(e)}"}


def detect_time_series(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """
    æ£€æµ‹æ•°æ®æ˜¯å¦åŒ…å«æ—¶é—´åºåˆ—ï¼Œå¹¶è¯†åˆ«æ—¶é—´åˆ—
    
    Args:
        data: æŸ¥è¯¢ç»“æœæ•°æ®
        
    Returns:
        æ—¶é—´åºåˆ—ä¿¡æ¯æˆ–None
    """
    try:
        if not data:
            return None
        
        df = pd.DataFrame(data)
        
        # å°è¯•è¯†åˆ«æ—¥æœŸåˆ—
        date_columns = []
        for col in df.columns:
            # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸ
            try:
                pd.to_datetime(df[col])
                date_columns.append(col)
            except:
                # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¥æœŸç›¸å…³å…³é”®è¯
                if any(keyword in str(col).lower() for keyword in ['date', 'time', 'day', 'month', 'year', 'æ—¥æœŸ', 'æ—¶é—´']):
                    try:
                        pd.to_datetime(df[col], errors='coerce')
                        if df[col].notna().sum() > 0:
                            date_columns.append(col)
                    except:
                        pass
        
        if not date_columns:
            return None
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—
        date_col = date_columns[0]
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col])
        df = df.sort_values(date_col)
        
        if len(df) < 2:
            return None
        
        return {
            "has_time_series": True,
            "date_column": date_col,
            "date_range": {
                "start": str(df[date_col].min()),
                "end": str(df[date_col].max())
            },
            "data_points": len(df),
            "all_date_columns": date_columns
        }
        
    except Exception as e:
        print(f"æ—¶é—´åºåˆ—æ£€æµ‹é”™è¯¯: {str(e)}")
        return None


def calculate_growth_rate(data: List[Dict[str, Any]], date_col: str, value_col: str) -> Dict[str, Any]:
    """
    è®¡ç®—æ—¶é—´åºåˆ—æ•°æ®çš„å¢é•¿ç‡
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        date_col: æ—¥æœŸåˆ—å
        value_col: æ•°å€¼åˆ—å
        
    Returns:
        å¢é•¿ç‡åˆ†æç»“æœ
    """
    try:
        df = pd.DataFrame(data)
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col, value_col])
        df = df.sort_values(date_col)
        
        if len(df) < 2:
            return {"error": "æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è®¡ç®—å¢é•¿ç‡"}
        
        # è®¡ç®—ç¯æ¯”å¢é•¿ç‡
        df['growth_rate'] = df[value_col].pct_change() * 100
        
        # è®¡ç®—æ€»ä½“å¢é•¿ç‡
        first_value = df[value_col].iloc[0]
        last_value = df[value_col].iloc[-1]
        total_growth = ((last_value - first_value) / first_value * 100) if first_value != 0 else 0
        
        # å¹³å‡å¢é•¿ç‡
        avg_growth = df['growth_rate'].mean() if len(df) > 1 else 0
        
        # è¯†åˆ«è¶‹åŠ¿
        if total_growth > 10:
            trend = "ä¸Šå‡"
        elif total_growth < -10:
            trend = "ä¸‹é™"
        else:
            trend = "å¹³ç¨³"
        
        return {
            "total_growth_rate": float(total_growth),
            "average_growth_rate": float(avg_growth),
            "trend": trend,
            "period_count": len(df),
            "first_value": float(first_value),
            "last_value": float(last_value),
            "max_growth": float(df['growth_rate'].max()) if len(df) > 1 else 0,
            "min_growth": float(df['growth_rate'].min()) if len(df) > 1 else 0
        }
        
    except Exception as e:
        return {"error": f"å¢é•¿ç‡è®¡ç®—é”™è¯¯: {str(e)}"}


def detect_outliers(data: List[Dict[str, Any]], column: str, method: str = "iqr") -> Dict[str, Any]:
    """
    æ£€æµ‹æ•°æ®ä¸­çš„ç¦»ç¾¤å€¼
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        column: è¦æ£€æµ‹çš„åˆ—å
        method: æ£€æµ‹æ–¹æ³• ('iqr' æˆ– 'zscore')
        
    Returns:
        ç¦»ç¾¤å€¼æ£€æµ‹ç»“æœ
    """
    try:
        df = pd.DataFrame(data)
        
        if column not in df.columns:
            return {"error": f"åˆ— {column} ä¸å­˜åœ¨"}
        
        if not pd.api.types.is_numeric_dtype(df[column]):
            return {"error": f"åˆ— {column} ä¸æ˜¯æ•°å€¼ç±»å‹"}
        
        values = df[column].dropna()
        
        if len(values) < 4:
            return {"outliers": [], "count": 0}
        
        outliers = []
        
        if method == "iqr":
            # IQRæ–¹æ³•
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outlier_mask = (values < lower_bound) | (values > upper_bound)
            outliers = values[outlier_mask].tolist()
            
        elif method == "zscore":
            # Z-scoreæ–¹æ³•
            mean = values.mean()
            std = values.std()
            if std > 0:
                z_scores = np.abs((values - mean) / std)
                outlier_mask = z_scores > 3
                outliers = values[outlier_mask].tolist()
        
        return {
            "outliers": [float(x) for x in outliers],
            "count": len(outliers),
            "percentage": (len(outliers) / len(values) * 100) if len(values) > 0 else 0,
            "method": method
        }
        
    except Exception as e:
        return {"error": f"ç¦»ç¾¤å€¼æ£€æµ‹é”™è¯¯: {str(e)}"}


def format_insights_for_display(insights: Dict[str, Any]) -> str:
    """
    å°†æ´å¯Ÿç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ–‡æœ¬
    
    Args:
        insights: æ´å¯Ÿç»“æœå­—å…¸
        
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    try:
        output = []
        
        # æ•°æ®æ‘˜è¦
        if "summary" in insights:
            output.append("ğŸ“Š æ•°æ®æ‘˜è¦")
            summary = insights["summary"]
            if "total_rows" in summary:
                output.append(f"  - æ€»è¡Œæ•°: {summary['total_rows']}")
            if "key_metrics" in summary:
                for metric, value in summary["key_metrics"].items():
                    output.append(f"  - {metric}: {value}")
        
        # è¶‹åŠ¿åˆ†æ
        if "trends" in insights and insights["trends"]:
            output.append("\nğŸ“ˆ è¶‹åŠ¿åˆ†æ")
            trends = insights["trends"]
            if "trend_direction" in trends:
                output.append(f"  - æ•´ä½“è¶‹åŠ¿: {trends['trend_direction']}")
            if "growth_rate" in trends:
                output.append(f"  - å¢é•¿ç‡: {trends['growth_rate']:.2f}%")
        
        # å¼‚å¸¸æ£€æµ‹
        if "anomalies" in insights and insights["anomalies"]:
            output.append("\nâš ï¸ å¼‚å¸¸æ£€æµ‹")
            for i, anomaly in enumerate(insights["anomalies"][:3], 1):
                output.append(f"  {i}. {anomaly.get('description', 'æœªçŸ¥å¼‚å¸¸')}")
        
        # ä¸šåŠ¡å»ºè®®
        if "recommendations" in insights and insights["recommendations"]:
            output.append("\nğŸ’¡ ä¸šåŠ¡å»ºè®®")
            for i, rec in enumerate(insights["recommendations"][:3], 1):
                output.append(f"  {i}. {rec}")
        
        return "\n".join(output) if output else "æš‚æ— åˆ†ææ´å¯Ÿ"
        
    except Exception as e:
        return f"æ ¼å¼åŒ–é”™è¯¯: {str(e)}"


def analyze_distribution(data: List[Dict[str, Any]], column: str) -> Dict[str, Any]:
    """
    åˆ†ææ•°å€¼åˆ—çš„åˆ†å¸ƒæƒ…å†µ
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        column: åˆ—å
        
    Returns:
        åˆ†å¸ƒåˆ†æç»“æœ
    """
    try:
        df = pd.DataFrame(data)
        
        if column not in df.columns:
            return {"error": f"åˆ— {column} ä¸å­˜åœ¨"}
        
        if not pd.api.types.is_numeric_dtype(df[column]):
            return {"error": f"åˆ— {column} ä¸æ˜¯æ•°å€¼ç±»å‹"}
        
        values = df[column].dropna()
        
        if len(values) == 0:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆæ•°æ®"}
        
        # è®¡ç®—åˆ†ä½æ•°
        quartiles = {
            "q25": float(values.quantile(0.25)),
            "q50": float(values.quantile(0.50)),
            "q75": float(values.quantile(0.75))
        }
        
        # ååº¦å’Œå³°åº¦
        skewness = float(values.skew())
        kurtosis = float(values.kurt())
        
        # åˆ†å¸ƒç±»å‹åˆ¤æ–­
        if abs(skewness) < 0.5:
            distribution_type = "è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ"
        elif skewness > 0.5:
            distribution_type = "å³ååˆ†å¸ƒ"
        else:
            distribution_type = "å·¦ååˆ†å¸ƒ"
        
        return {
            "quartiles": quartiles,
            "skewness": skewness,
            "kurtosis": kurtosis,
            "distribution_type": distribution_type,
            "range": float(values.max() - values.min())
        }
        
    except Exception as e:
        return {"error": f"åˆ†å¸ƒåˆ†æé”™è¯¯: {str(e)}"}


def find_correlations(data: List[Dict[str, Any]], threshold: float = 0.5) -> Dict[str, Any]:
    """
    æŸ¥æ‰¾æ•°å€¼åˆ—ä¹‹é—´çš„ç›¸å…³æ€§
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        threshold: ç›¸å…³æ€§é˜ˆå€¼
        
    Returns:
        ç›¸å…³æ€§åˆ†æç»“æœ
    """
    try:
        df = pd.DataFrame(data)
        
        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_df = df.select_dtypes(include=[np.number])
        
        if numeric_df.shape[1] < 2:
            return {"correlations": [], "message": "æ•°å€¼åˆ—å°‘äº2ä¸ªï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§"}
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_matrix = numeric_df.corr()
        
        # æå–å¼ºç›¸å…³æ€§
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) >= threshold:
                    strong_correlations.append({
                        "column1": corr_matrix.columns[i],
                        "column2": corr_matrix.columns[j],
                        "correlation": float(corr_value),
                        "strength": "å¼ºæ­£ç›¸å…³" if corr_value > threshold else "å¼ºè´Ÿç›¸å…³"
                    })
        
        return {
            "correlations": strong_correlations,
            "count": len(strong_correlations),
            "threshold": threshold
        }
        
    except Exception as e:
        return {"error": f"ç›¸å…³æ€§åˆ†æé”™è¯¯: {str(e)}"}
