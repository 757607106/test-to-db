"""
Dashboardæ´å¯Ÿåˆ†ææœåŠ¡
è´Ÿè´£æ•°æ®èšåˆã€æ¡ä»¶åº”ç”¨ã€æ´å¯Ÿç”Ÿæˆç¼–æ’
ä¼˜åŒ–ï¼šæ”¯æŒå¼‚æ­¥åå°å¤„ç†
"""
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
from sqlalchemy.orm import Session

from app import crud, schemas
from app.models.dashboard_widget import DashboardWidget
from app.services.graph_relationship_service import graph_relationship_service
from app.db.session import SessionLocal
from app.services.text2sql_utils import retrieve_relevant_schema, format_schema_for_prompt
from app.core.agent_config import get_agent_llm, CORE_AGENT_SQL_GENERATOR
from langchain_core.messages import SystemMessage, HumanMessage

class DashboardInsightService:
    """Dashboardæ´å¯Ÿåˆ†ææœåŠ¡"""
    
    def _build_table_column_whitelist(self, schema_context: dict) -> tuple[str, set, dict]:
        """
        æ„å»ºè¡¨/åˆ—ç™½åå•ï¼Œé˜²æ­¢ LLM å¹»è§‰
        
        Returns:
            whitelist_str: æ ¼å¼åŒ–çš„ç™½åå•å­—ç¬¦ä¸²
            valid_tables: æœ‰æ•ˆè¡¨åé›†åˆ
            valid_columns: {table_name: [column_names]} å­—å…¸
        """
        valid_tables = set()
        valid_columns = {}  # {table_name: [column_names]}
        
        tables = schema_context.get("tables", [])
        columns = schema_context.get("columns", [])
        relationships = schema_context.get("relationships", [])
        
        # æ„å»ºè¡¨åé›†åˆ
        for t in tables:
            table_name = t.get("name", "")
            if table_name:
                valid_tables.add(table_name)
                valid_columns[table_name] = []
        
        # æ„å»ºåˆ—åæ˜ å°„
        for c in columns:
            table_name = c.get("table_name", "")
            col_name = c.get("name", "")
            if table_name and col_name:
                if table_name not in valid_columns:
                    valid_columns[table_name] = []
                valid_columns[table_name].append(col_name)
        
        # æ„å»ºç™½åå•å­—ç¬¦ä¸²
        whitelist_parts = []
        whitelist_parts.append("=" * 60)
        whitelist_parts.append("ã€é‡è¦ã€‘å¯ç”¨è¡¨å’Œå­—æ®µç™½åå•ï¼ˆä»…å…è®¸ä½¿ç”¨ä»¥ä¸‹è¡¨å’Œå­—æ®µï¼‰")
        whitelist_parts.append("=" * 60)
        
        for table_name in sorted(valid_tables):
            cols = valid_columns.get(table_name, [])
            # æ‰¾åˆ°è¡¨çš„æè¿°
            table_desc = ""
            for t in tables:
                if t.get("name") == table_name:
                    table_desc = t.get("description", "")
                    break
            
            whitelist_parts.append(f"\nè¡¨å: {table_name}")
            if table_desc:
                whitelist_parts.append(f"  æè¿°: {table_desc}")
            whitelist_parts.append(f"  å¯ç”¨å­—æ®µ: {', '.join(cols)}")
        
        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if relationships:
            whitelist_parts.append("\n" + "-" * 40)
            whitelist_parts.append("è¡¨é—´å…³ç³»ï¼ˆJOIN æ—¶å¿…é¡»ä½¿ç”¨è¿™äº›å…³è”å­—æ®µï¼‰:")
            # éå†æ‰€æœ‰å…³ç³»ï¼Œä¸é™åˆ¶æ•°é‡ä»¥ç¡®ä¿ JOIN å‡†ç¡®æ€§
            for rel in relationships:
                src = f"{rel.get('source_table', '')}.{rel.get('source_column', '')}"
                tgt = f"{rel.get('target_table', '')}.{rel.get('target_column', '')}"
                whitelist_parts.append(f"  - {src} -> {tgt}")
        
        whitelist_parts.append("\n" + "=" * 60)
        whitelist_parts.append("ã€è­¦å‘Šã€‘ä¸¥ç¦ä½¿ç”¨ä¸Šè¿°ç™½åå•ä¹‹å¤–çš„ä»»ä½•è¡¨æˆ–å­—æ®µï¼")
        whitelist_parts.append("=" * 60)
        
        return "\n".join(whitelist_parts), valid_tables, valid_columns
    
    def _validate_sql_against_whitelist(
        self, 
        sql: str, 
        valid_tables: set, 
        valid_columns: dict,
        db_type: str = "MYSQL"
    ) -> tuple[bool, str, list]:
        """
        éªŒè¯ SQL æ˜¯å¦åªä½¿ç”¨äº†ç™½åå•ä¸­çš„è¡¨å’Œåˆ—
        
        Returns:
            is_valid: æ˜¯å¦æœ‰æ•ˆ
            error_msg: é”™è¯¯ä¿¡æ¯
            invalid_refs: æ— æ•ˆå¼•ç”¨åˆ—è¡¨
        """
        import re
        
        sql_upper = sql.upper()
        invalid_refs = []
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ SELECT è¯­å¥
        if not sql_upper.strip().startswith("SELECT"):
            return False, "SQL å¿…é¡»æ˜¯ SELECT è¯­å¥", ["non-select"]
        
        # 2. æ£€æŸ¥å±é™©å…³é”®è¯
        dangerous_keywords = ["DROP", "DELETE", "TRUNCATE", "UPDATE", "INSERT", "ALTER", "CREATE"]
        for kw in dangerous_keywords:
            if kw in sql_upper and "SELECT" not in sql_upper[:20]:
                return False, f"æ£€æµ‹åˆ°å±é™©æ“ä½œ: {kw}", [kw]
        
        # 3. æå– SQL ä¸­çš„è¡¨å
        # åŒ¹é… FROM/JOIN åçš„è¡¨å
        table_pattern = r'(?:FROM|JOIN)\s+[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_tables = re.findall(table_pattern, sql, re.IGNORECASE)
        
        # æ£€æŸ¥è¡¨åæ˜¯å¦åœ¨ç™½åå•ä¸­
        valid_tables_lower = {t.lower() for t in valid_tables}
        for table in found_tables:
            if table.lower() not in valid_tables_lower:
                invalid_refs.append(f"è¡¨ '{table}' ä¸åœ¨ç™½åå•ä¸­")
        
        # 4. æå–å¹¶æ£€æŸ¥åˆ—åï¼ˆç®€åŒ–æ£€æŸ¥ï¼Œåªæ£€æŸ¥ table.column æ ¼å¼ï¼‰
        # åŒ¹é… table.column æˆ– alias.column æ ¼å¼
        col_pattern = r'([a-zA-Z_][a-zA-Z0-9_]*)\s*\.\s*[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_cols = re.findall(col_pattern, sql, re.IGNORECASE)
        
        # æ„å»ºæ‰€æœ‰æœ‰æ•ˆåˆ—åçš„å°å†™é›†åˆ
        all_valid_cols_lower = set()
        for cols in valid_columns.values():
            for col in cols:
                all_valid_cols_lower.add(col.lower())
        
        # æ£€æŸ¥åˆ—åï¼ˆå®¹å¿ä¸€äº›å¸¸è§çš„åˆ«åï¼‰
        common_aliases = {'t', 't1', 't2', 'a', 'b', 'c', 's', 'm', 'o', 'p', 'd', 'main', 'sub'}
        for table_or_alias, col in found_cols:
            # å¦‚æœæ˜¯å¸¸è§åˆ«åï¼Œåªæ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
            if table_or_alias.lower() in common_aliases:
                if col.lower() not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{col}' ä¸åœ¨ç™½åå•ä¸­")
            else:
                # æ£€æŸ¥è¡¨åå’Œåˆ—å
                table_lower = table_or_alias.lower()
                col_lower = col.lower()
                
                # åœ¨æ‰€æœ‰è¡¨ä¸­æŸ¥æ‰¾è¯¥åˆ—
                col_found = False
                for t_name, t_cols in valid_columns.items():
                    if col_lower in [c.lower() for c in t_cols]:
                        col_found = True
                        break
                
                if not col_found and col_lower not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{table_or_alias}.{col}' ä¸åœ¨ç™½åå•ä¸­")
        
        if invalid_refs:
            return False, f"å‘ç° {len(invalid_refs)} ä¸ªæ— æ•ˆå¼•ç”¨", invalid_refs
        
        return True, "", []
    
    async def generate_mining_suggestions(self, db: Session, request: schemas.MiningRequest) -> schemas.MiningResponse:
        """ç”Ÿæˆæ™ºèƒ½æŒ–æ˜å»ºè®®ï¼ˆä¼˜åŒ–ç‰ˆï¼šé˜²å¹»è§‰ + SQL éªŒè¯ï¼‰"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"[Mining] å¼€å§‹ç”ŸæˆæŒ–æ˜å»ºè®®, connection_id={request.connection_id}, intent={request.intent}")
        
        # 0. è·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
        from app.models.db_connection import DBConnection
        connection = db.query(DBConnection).filter(DBConnection.id == request.connection_id).first()
        db_type = connection.db_type.upper() if connection else "MYSQL"
        logger.info(f"[Mining] æ•°æ®åº“ç±»å‹: {db_type}")
        
        # 1. è·å–ä¸Šä¸‹æ–‡
        if request.intent:
            schema_context = retrieve_relevant_schema(db, request.connection_id, request.intent)
        else:
            tables = crud.schema_table.get_by_connection(db=db, connection_id=request.connection_id)
            
            if not tables:
                logger.warning(f"[Mining] æœªæ‰¾åˆ°è¡¨, connection_id={request.connection_id}")
                return schemas.MiningResponse(suggestions=[])
            
            logger.info(f"[Mining] æ‰¾åˆ° {len(tables)} ä¸ªè¡¨")
            
            tables_list = []
            columns_list = []
            table_names = []
            
            # éå†æ‰€æœ‰è¡¨ï¼Œä¸é™åˆ¶æ•°é‡ä»¥ç¡®ä¿ SQL ç”Ÿæˆå‡†ç¡®æ€§
            for table in tables:
                table_names.append(table.table_name)
                tables_list.append({
                    "id": table.id,
                    "name": table.table_name,
                    "description": table.description or ""
                })
                
                columns = crud.schema_column.get_by_table(db=db, table_id=table.id)
                for col in columns:
                    columns_list.append({
                        "id": col.id,
                        "name": col.column_name,
                        "type": col.data_type,
                        "description": col.description or "",
                        "is_primary_key": col.is_primary_key,
                        "is_foreign_key": col.is_foreign_key,
                        "table_id": table.id,
                        "table_name": table.table_name
                    })
            
            # è·å–è¡¨ä¹‹é—´çš„å…³ç³»
            relationships = []
            try:
                relationship_context = graph_relationship_service.query_table_relationships(
                    connection_id=request.connection_id,
                    table_names=table_names
                )
                if relationship_context.get("direct_relationships"):
                    for rel in relationship_context["direct_relationships"]:
                        relationships.append({
                            "source_table": rel.get("source_table"),
                            "source_column": rel.get("source_column"),
                            "target_table": rel.get("target_table"),
                            "target_column": rel.get("target_column"),
                            "relationship_type": rel.get("relationship_type", "references")
                        })
                    logger.info(f"[Mining] æ‰¾åˆ° {len(relationships)} ä¸ªè¡¨é—´å…³ç³»")
            except Exception as e:
                logger.warning(f"[Mining] è·å–è¡¨å…³ç³»å¤±è´¥: {e}")
            
            schema_context = {
                "tables": tables_list,
                "columns": columns_list,
                "relationships": relationships
            }
        
        if not schema_context.get("tables"):
            logger.warning("[Mining] schema_context ä¸­æ— è¡¨")
            return schemas.MiningResponse(suggestions=[])
        
        # 2. æ„å»ºè¡¨/åˆ—ç™½åå•ï¼ˆé˜²å¹»è§‰æ ¸å¿ƒï¼‰
        whitelist_str, valid_tables, valid_columns = self._build_table_column_whitelist(schema_context)
        logger.info(f"[Mining] ç™½åå•åŒ…å« {len(valid_tables)} ä¸ªè¡¨, å…± {sum(len(cols) for cols in valid_columns.values())} ä¸ªå­—æ®µ")
        
        # 3. æ ¼å¼åŒ– Schema
        schema_str = format_schema_for_prompt(schema_context)
        
        # 3. æ„å»º Promptï¼ˆè¦æ±‚è¿”å› JSON æ ¼å¼ï¼‰
        # æ ¹æ®æ•°æ®åº“ç±»å‹æä¾› SQL è¯­æ³•æŒ‡å—
        sql_syntax_guides = {
            "MYSQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMySQLï¼‰ï¼š
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOINï¼Œè¯·ä½¿ç”¨ LEFT JOIN æˆ– RIGHT JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—
- å¸ƒå°”å€¼ä½¿ç”¨ 1/0 æˆ– TRUE/FALSE""",
            "POSTGRESQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆPostgreSQLï¼‰ï¼š
- å¯ä½¿ç”¨ LIMIT æˆ– FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- æ”¯æŒ ARRAY ç±»å‹å’Œ JSON æ“ä½œ""",
            "SQLITE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQLiteï¼‰ï¼š
- ä½¿ç”¨ LIMITï¼Œä¸æ”¯æŒ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ strftime()
- ä¸æ”¯æŒ FULL OUTER JOIN å’Œ RIGHT JOIN
- ä½¿ç”¨åŒå¼•å· " æˆ–æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ç±»å‹ç³»ç»Ÿçµæ´»ï¼Œæ— ä¸¥æ ¼ç±»å‹æ£€æŸ¥""",
            "SQLSERVER": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQL Server / MSSQLï¼‰ï¼š
- ä½¿ç”¨ TOP N æˆ– OFFSET...FETCH
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ + æ“ä½œç¬¦æˆ– CONCAT()
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ FORMAT() æˆ– CONVERT()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ GETDATE() è·å–å½“å‰æ—¶é—´""",
            "ORACLE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆOracleï¼‰ï¼š
- ä½¿ç”¨ ROWNUM æˆ– FETCH FIRSTï¼ˆ12c+ï¼‰
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ SYSDATE è·å–å½“å‰æ—¶é—´
- FROM å­å¥å¿…é¡»æœ‰è¡¨ï¼ˆå¯ç”¨ DUALï¼‰""",
            "MARIADB": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMariaDBï¼‰ï¼š
- è¯­æ³•ä¸ MySQL åŸºæœ¬å…¼å®¹
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—""",
            "CLICKHOUSE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆClickHouseï¼‰ï¼š
- ä½¿ç”¨ LIMIT è¿›è¡Œåˆ†é¡µ
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ concat() æˆ– ||
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ formatDateTime()
- æ”¯æŒ FULL OUTER JOIN (éƒ¨åˆ†ç‰ˆæœ¬)
- åŒºåˆ†å¤§å°å†™ï¼Œä½¿ç”¨åŒå¼•å·åŒ…è£¹
- ä¸“ä¸º OLAP ä¼˜åŒ–ï¼ŒèšåˆæŸ¥è¯¢æ€§èƒ½ä¼˜å¼‚""",
        }
        
        db_type_upper = db_type.upper()
        # å°è¯•åŒ¹é…æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…
        sql_syntax_guide = ""
        for key, guide in sql_syntax_guides.items():
            if key in db_type_upper or db_type_upper in key:
                sql_syntax_guide = guide
                break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œæä¾›é€šç”¨æŒ‡å—
        if not sql_syntax_guide:
            sql_syntax_guide = f"""
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆ{db_type}ï¼‰ï¼š
- è¯·ä½¿ç”¨æ ‡å‡† ANSI SQL è¯­æ³•
- é¿å…ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„æ‰©å±•è¯­æ³•
- ä½¿ç”¨é€šç”¨çš„èšåˆå‡½æ•°ï¼ˆSUM, COUNT, AVG, MAX, MINï¼‰
- ä½¿ç”¨æ ‡å‡†çš„ JOIN è¯­æ³•ï¼ˆINNER JOIN, LEFT JOINï¼‰
- æ—¥æœŸå‡½æ•°è¯·æ ¹æ®å®é™…æ•°æ®åº“è°ƒæ•´"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æ•°æ®åº“ç»“æ„ï¼Œæ¨è {request.limit} ä¸ªæœ‰ä»·å€¼çš„æ•°æ®åˆ†æè§†è§’ï¼ˆå›¾è¡¨ï¼‰ã€‚

ç›®æ ‡æ•°æ®åº“ç±»å‹ï¼š{db_type}
{sql_syntax_guide}

ç”¨æˆ·æ„å›¾ï¼š{request.intent or "è‡ªåŠ¨å‘ç°å…³é”®ä¸šåŠ¡æŒ‡æ ‡å’Œè¶‹åŠ¿"}

{whitelist_str}

æ•°æ®åº“ç»“æ„è¯¦æƒ…ï¼š
{schema_str}

æŒ–æ˜ç»´åº¦è¦æ±‚ï¼ˆè¯·è¦†ç›–å¤šä¸ªç»´åº¦ï¼‰ï¼š
- businessï¼ˆä¸šåŠ¡æ•°æ®ï¼‰ï¼šæ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ã€KPI
- metricï¼ˆæŒ‡æ ‡åˆ†æï¼‰ï¼šå…³é”®æ•°å€¼çš„ç»Ÿè®¡åˆ†å¸ƒ
- trendï¼ˆè¶‹åŠ¿åˆ†æï¼‰ï¼šæ—¶é—´åºåˆ—å˜åŒ–
- semanticï¼ˆè¯­ä¹‰å…³è”ï¼‰ï¼šåŸºäºå­—æ®µè¯­ä¹‰å‘ç°çš„å…³è”åˆ†æ

ã€æ ¸å¿ƒçº¦æŸ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘ï¼š
1. SQL ä¸­çš„è¡¨åå’Œåˆ—åå¿…é¡»ä¸¥æ ¼åŒ¹é…ä¸Šè¿°ç™½åå•ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•ç™½åå•ä¹‹å¤–çš„è¡¨æˆ–å­—æ®µ
2. JOIN æ—¶å¿…é¡»ä½¿ç”¨ç™½åå•ä¸­æŒ‡å®šçš„å…³è”å­—æ®µï¼Œä¸å¾—è‡ªè¡Œæ¨æµ‹
3. æ¨èçš„ SQL å¿…é¡»æ˜¯åˆæ³•çš„ {db_type} SELECT è¯­å¥
4. å›¾è¡¨ç±»å‹ä»ä»¥ä¸‹é€‰æ‹©ï¼šbar, line, pie, scatter, table
5. æ¯ä¸ªæ¨èéƒ½è¦æœ‰æ˜ç¡®çš„ä¸šåŠ¡ä»·å€¼å’Œæ¨èç†ç”±
6. SQL å°½é‡åŒ…å«èšåˆåˆ†æï¼ˆSUM, COUNT, AVG, GROUP BYï¼‰
7. ä¸¥æ ¼éµå¾ª {db_type} çš„ SQL è¯­æ³•è§„èŒƒ

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{{{
  "suggestions": [
    {{{{
      "title": "å›¾è¡¨æ ‡é¢˜",
      "description": "ç®€çŸ­æè¿°ï¼ˆä¸€å¥è¯ï¼‰",
      "reasoning": "è¯¦ç»†æ¨èç†ç”±ï¼šä¸ºä»€ä¹ˆè¿™ä¸ªåˆ†æå¯¹ä¸šåŠ¡æœ‰ä»·å€¼ï¼Œæ•°æ®é€»è¾‘æ˜¯ä»€ä¹ˆ",
      "mining_dimension": "business|metric|trend|semantic",
      "confidence": 0.85,
      "chart_type": "bar|line|pie|scatter|table",
      "sql": "SELECT ...",
      "source_tables": ["è¡¨å1", "è¡¨å2"],
      "key_fields": ["å…³é”®å­—æ®µ1", "å…³é”®å­—æ®µ2"],
      "business_value": "è¿™ä¸ªåˆ†æèƒ½å¸®åŠ©ä¸šåŠ¡åšä»€ä¹ˆå†³ç­–",
      "suggested_actions": ["å»ºè®®åŠ¨ä½œ1", "å»ºè®®åŠ¨ä½œ2"],
      "analysis_intent": "åˆ†ææ„å›¾æè¿°"
    }}}}
  ]
}}}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""
        
        # 4. è°ƒç”¨ LLMï¼ˆä½¿ç”¨ SQL Generator Agent é…ç½®çš„æ¨¡å‹ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´ï¼‰
        try:
            import json
            from app.core.model_registry import create_chat_model
            from app.models.agent_profile import AgentProfile
            from app.models.llm_config import LLMConfiguration
            from app.core.config import settings
            
            # è·å– Agent é…ç½®
            profile = db.query(AgentProfile).filter(AgentProfile.name == CORE_AGENT_SQL_GENERATOR).first()
            
            # æ„å»º LLM å‚æ•°
            api_key = settings.OPENAI_API_KEY
            api_base = settings.OPENAI_API_BASE
            model_name = settings.LLM_MODEL
            provider = settings.LLM_PROVIDER.lower()
            
            if profile and profile.llm_config_id:
                llm_config = db.query(LLMConfiguration).filter(
                    LLMConfiguration.id == profile.llm_config_id,
                    LLMConfiguration.is_active == True
                ).first()
                if llm_config:
                    api_key = llm_config.api_key
                    api_base = llm_config.base_url
                    model_name = llm_config.model_name
                    provider = llm_config.provider.lower()
            
            # åˆ›å»º LLMï¼ˆå¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’ï¼‰
            llm = create_chat_model(
                provider=provider,
                model_name=model_name,
                api_key=api_key,
                base_url=api_base,
                temperature=0.3,
                max_tokens=8192,
                timeout=120.0,  # æŒ–æ˜ä»»åŠ¡éœ€è¦æ›´é•¿è¶…æ—¶
                max_retries=2
            )
            
            response = await llm.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚åªè¿”å› JSON æ ¼å¼çš„å“åº”ã€‚"),
                HumanMessage(content=prompt)
            ])
            
            # è§£æ LLM è¿”å›çš„ JSON
            response_text = response.content if hasattr(response, 'content') else str(response)
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            parsed = json.loads(response_text)
            raw_suggestions = parsed.get("suggestions", [])
            logger.info(f"[Mining] LLM è¿”å› {len(raw_suggestions)} ä¸ªåŸå§‹å»ºè®®")
            
            # 5. éªŒè¯æ¯ä¸ª SQL å¹¶è¿‡æ»¤æ— æ•ˆçš„
            validated_suggestions = []
            invalid_count = 0
            
            for idx, s in enumerate(raw_suggestions):
                sql = s.get("sql", "")
                title = s.get("title", f"å»ºè®®{idx+1}")
                
                if not sql:
                    logger.warning(f"[Mining] å»ºè®® '{title}' æ—  SQLï¼Œè·³è¿‡")
                    invalid_count += 1
                    continue
                
                # éªŒè¯ SQL
                is_valid, error_msg, invalid_refs = self._validate_sql_against_whitelist(
                    sql, valid_tables, valid_columns, db_type
                )
                
                if not is_valid:
                    logger.warning(f"[Mining] å»ºè®® '{title}' SQL éªŒè¯å¤±è´¥: {error_msg}")
                    for ref in invalid_refs[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ— æ•ˆå¼•ç”¨
                        logger.warning(f"[Mining]   - {ref}")
                    invalid_count += 1
                    # é™ä½ç½®ä¿¡åº¦ä½†ä»ç„¶ä¿ç•™ï¼ˆè®©ç”¨æˆ·å†³å®šï¼‰
                    s["confidence"] = max(0.3, float(s.get("confidence", 0.8)) - 0.4)
                    s["reasoning"] = f"ã€è­¦å‘Šã€‘{error_msg}\n\n" + s.get("reasoning", "")
                
                validated_suggestions.append(
                    schemas.MiningSuggestion(
                        title=s.get("title", ""),
                        description=s.get("description", ""),
                        chart_type=s.get("chart_type", "bar"),
                        sql=sql,
                        analysis_intent=s.get("analysis_intent", s.get("title", "æ•°æ®åˆ†æ")),
                        reasoning=s.get("reasoning", s.get("description", "")),
                        mining_dimension=s.get("mining_dimension", "business"),
                        confidence=float(s.get("confidence", 0.8)),
                        source_tables=s.get("source_tables", []),
                        key_fields=s.get("key_fields", []),
                        business_value=s.get("business_value", ""),
                        suggested_actions=s.get("suggested_actions", [])
                    )
                )
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œé«˜ç½®ä¿¡åº¦çš„æ’åœ¨å‰é¢
            validated_suggestions.sort(key=lambda x: x.confidence, reverse=True)
            
            logger.info(f"[Mining] æœ€ç»ˆè¿”å› {len(validated_suggestions)} ä¸ªå»ºè®®, {invalid_count} ä¸ª SQL éªŒè¯å¤±è´¥")
            return schemas.MiningResponse(suggestions=validated_suggestions)
            
        except json.JSONDecodeError as e:
            logger.error(f"[Mining] JSON è§£æå¤±è´¥: {e}")
            logger.error(f"[Mining] åŸå§‹å“åº”: {response_text[:500]}...")
            return schemas.MiningResponse(suggestions=[])
        except Exception as e:
            logger.error(f"[Mining] å»ºè®®ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return schemas.MiningResponse(suggestions=[])

    def trigger_dashboard_insights(
        self,
        db: Session,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest
    ) -> schemas.DashboardInsightResponse:
        """
        è§¦å‘çœ‹æ¿æ´å¯Ÿç”Ÿæˆï¼ˆåˆ›å»ºå ä½Widgetï¼Œåç»­ç”±åå°ä»»åŠ¡å¤„ç†ï¼‰
        """
        # 1. æ£€æŸ¥æƒé™
        self._check_permission(db, dashboard_id, user_id)
        
        # 2. è·å–Dashboard
        dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
        if not dashboard:
            raise ValueError(f"Dashboard {dashboard_id} not found")
            
        # 3. åˆ›å»ºæˆ–æ›´æ–°Widgetä¸º"åˆ†æä¸­"çŠ¶æ€
        # åˆ›å»ºåˆå§‹çš„ç©ºç»“æœ
        initial_result = schemas.InsightResult(
            summary=schemas.InsightSummary(total_rows=0, key_metrics={}, time_range="åˆ†æä¸­..."),
            trends=None, anomalies=[], correlations=[], recommendations=[]
        )
        
        # åˆ›å»ºæˆ–æ›´æ–° Widget (åŒæ­¥)
        widget_id = self._create_or_update_insight_widget(
            db,
            dashboard_id,
            initial_result,
            request.conditions,
            request.use_graph_relationships,
            analyzed_widget_count=0,
            status="processing" # æ ‡è®°ä¸ºå¤„ç†ä¸­
        )
        
        return schemas.DashboardInsightResponse(
            widget_id=widget_id,
            insights=initial_result,
            analyzed_widget_count=0,
            analysis_timestamp=datetime.utcnow(),
            applied_conditions=request.conditions,
            relationship_count=0,
            status="processing" # æ–°å¢çŠ¶æ€å­—æ®µ
        )

    async def process_dashboard_insights_task(
        self,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest,
        widget_id: int
    ):
        """
        åå°ä»»åŠ¡ï¼šæ‰§è¡Œå®é™…çš„æ´å¯Ÿåˆ†æé€»è¾‘
        """
        db = SessionLocal()
        try:
            print(f"ğŸš€ å¼€å§‹åå°æ´å¯Ÿåˆ†æ Task (Dashboard: {dashboard_id})")
            
            # 1. è·å–æ•°æ®
            dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
            widgets = dashboard.widgets
            
            # ç­›é€‰Widgets
            if request.included_widget_ids:
                widgets = [w for w in widgets if w.id in request.included_widget_ids]
            
            data_widgets = [w for w in widgets if w.widget_type != "insight_analysis"]
            
            if not data_widgets:
                print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®ç»„ä»¶ï¼Œè·³è¿‡åˆ†æ")
                return

            # 2. èšåˆæ•°æ®
            aggregated_data = self._aggregate_widget_data(data_widgets, request.conditions)
            
            # 3. å›¾è°±æŸ¥è¯¢
            relationship_context = None
            relationship_count = 0
            if request.use_graph_relationships and aggregated_data["table_names"]:
                try:
                    connection_id = data_widgets[0].connection_id
                    relationship_context = graph_relationship_service.query_table_relationships(
                        connection_id,
                        aggregated_data["table_names"]
                    )
                    relationship_count = relationship_context.get("relationship_count", 0)
                except Exception as e:
                    print(f"âš ï¸ å›¾è°±å…³ç³»æŸ¥è¯¢å¤±è´¥: {e}")

            # 4. ç®€åŒ–çš„æ´å¯Ÿåˆ†æï¼ˆä¸ä½¿ç”¨dashboard_analyst_agentï¼‰
            insights = schemas.InsightResult(
                summary=schemas.InsightSummary(
                    total_rows=aggregated_data["total_rows"],
                    key_metrics={},
                    time_range="å·²åˆ†æ"
                ),
                trends=None,
                anomalies=[],
                correlations=[],
                recommendations=[
                    schemas.InsightRecommendation(
                        type="info",
                        content=f"å·²åˆ†æ {len(data_widgets)} ä¸ªæ•°æ®ç»„ä»¶",
                        priority="medium"
                    )
                ]
            )
            
            # 5. æ›´æ–° Widget çŠ¶æ€ä¸ºå®Œæˆ
            self._update_insight_widget_result(
                db, 
                widget_id, 
                insights, 
                len(data_widgets),
                status="completed"
            )
            
            print(f"âœ… åå°æ´å¯Ÿåˆ†æå®Œæˆ (Widget: {widget_id})")
            
        except Exception as e:
            print(f"âŒ åå°æ´å¯Ÿåˆ†æå¤±è´¥: {str(e)}")
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            self._update_widget_status(db, widget_id, "failed", str(e))
        finally:
            db.close()

    def _check_permission(self, db: Session, dashboard_id: int, user_id: int):
        has_permission = crud.crud_dashboard.check_permission(
            db, dashboard_id=dashboard_id, user_id=user_id, required_level="viewer"
        )
        if not has_permission:
            raise PermissionError("No permission to view this dashboard")

    def _aggregate_widget_data(
        self,
        widgets: List[DashboardWidget],
        conditions: Optional[schemas.InsightConditions]
    ) -> Dict[str, Any]:
        """èšåˆWidgetæ•°æ®"""
        aggregated_rows = []
        table_names = set()
        numeric_columns = set()
        date_columns = set()
        widget_summaries = []
        
        for widget in widgets:
            # æå–widgetæ•°æ®
            if not widget.data_cache or "data" not in widget.data_cache:
                continue
            
            data = widget.data_cache["data"]
            if not data or not isinstance(data, list):
                continue
            
            # åº”ç”¨æ¡ä»¶è¿‡æ»¤
            filtered_data = self._apply_conditions(data, conditions)
            
            aggregated_rows.extend(filtered_data)
            
            # æå–è¡¨å
            if widget.query_config:
                if "table_name" in widget.query_config:
                    table_names.add(widget.query_config["table_name"])
            
            # æå–åˆ—ä¿¡æ¯
            if filtered_data:
                first_row = filtered_data[0]
                for key, value in first_row.items():
                    if isinstance(value, (int, float)):
                        numeric_columns.add(key)
                    elif isinstance(value, str):
                        if any(keyword in key.lower() for keyword in ["date", "time", "created", "updated"]):
                            date_columns.add(key)
            
            widget_summaries.append({
                "id": widget.id,
                "type": widget.widget_type,
                "title": widget.title,
                "row_count": len(filtered_data)
            })
        
        return {
            "data": aggregated_rows,
            "total_rows": len(aggregated_rows),
            "table_names": list(table_names),
            "numeric_columns": list(numeric_columns),
            "date_columns": list(date_columns),
            "widget_summaries": widget_summaries
        }
    
    def _apply_conditions(
        self,
        data: List[Dict[str, Any]],
        conditions: Optional[schemas.InsightConditions]
    ) -> List[Dict[str, Any]]:
        """åº”ç”¨æŸ¥è¯¢æ¡ä»¶è¿‡æ»¤æ•°æ®"""
        if not conditions:
            return data
        
        filtered_data = data.copy()
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if conditions.time_range:
            date_column = None
            if filtered_data:
                first_row = filtered_data[0]
                for key in first_row.keys():
                    if any(keyword in key.lower() for keyword in ["date", "time", "created"]):
                        date_column = key
                        break
            
            if date_column and conditions.time_range.start and conditions.time_range.end:
                filtered_data = [
                    row for row in filtered_data
                    if conditions.time_range.start <= str(row.get(date_column, "")) <= conditions.time_range.end
                ]
        
        # ç»´åº¦ç­›é€‰
        if conditions.dimension_filters:
            for column, value in conditions.dimension_filters.items():
                filtered_data = [
                    row for row in filtered_data
                    if row.get(column) == value
                ]
        
        return filtered_data
    
    def _create_or_update_insight_widget(
        self,
        db: Session,
        dashboard_id: int,
        insights: schemas.InsightResult,
        conditions: Optional[schemas.InsightConditions],
        use_graph_relationships: bool,
        analyzed_widget_count: int,
        status: str = "completed",
        lineage: Optional[Dict[str, Any]] = None
    ) -> int:
        """åˆ›å»ºæˆ–æ›´æ–°æ´å¯ŸWidgetï¼Œä¿å­˜æº¯æºä¿¡æ¯"""
        existing_widgets = crud.crud_dashboard_widget.get_by_dashboard(db, dashboard_id=dashboard_id)
        
        insight_widget = None
        for widget in existing_widgets:
            if widget.widget_type == "insight_analysis":
                insight_widget = widget
                break
        
        # P0: å°†æº¯æºä¿¡æ¯åˆå¹¶åˆ° query_config
        query_config = {
            "analysis_scope": "all_widgets",
            "analysis_dimensions": ["summary", "trends", "correlations", "recommendations"],
            "refresh_strategy": "manual",
            "last_analysis_at": datetime.utcnow().isoformat(),
            "use_graph_relationships": use_graph_relationships,
            "analyzed_widget_count": analyzed_widget_count,
            "status": status,
        }
        
        if conditions:
            query_config["current_conditions"] = conditions.dict(exclude_none=True)
        
        # P0: ä¿å­˜æº¯æºä¿¡æ¯
        if lineage:
            query_config["source_tables"] = lineage.get("source_tables", [])
            query_config["generated_sql"] = lineage.get("generated_sql")
            query_config["user_intent"] = lineage.get("sql_generation_trace", {}).get("user_intent")
            query_config["few_shot_samples_count"] = lineage.get("sql_generation_trace", {}).get("few_shot_samples_count", 0)
            query_config["generation_method"] = lineage.get("sql_generation_trace", {}).get("generation_method", "standard")
            query_config["execution_time_ms"] = lineage.get("execution_metadata", {}).get("execution_time_ms", 0)
            query_config["from_cache"] = lineage.get("execution_metadata", {}).get("from_cache", False)
            query_config["row_count"] = lineage.get("execution_metadata", {}).get("row_count", 0)
            query_config["db_type"] = lineage.get("execution_metadata", {}).get("db_type")
            query_config["data_transformations"] = lineage.get("data_transformations", [])
            query_config["confidence_score"] = lineage.get("confidence_score", 0.8)
            query_config["analysis_method"] = lineage.get("analysis_method", "auto")
        
        data_cache = insights.dict(exclude_none=True)
        
        if insight_widget:
            crud.crud_dashboard_widget.update(
                db,
                db_obj=insight_widget,
                obj_in=schemas.WidgetUpdate(title="çœ‹æ¿æ´å¯Ÿåˆ†æ")
            )
            insight_widget.query_config = query_config
            insight_widget.data_cache = data_cache
            insight_widget.last_refresh_at = datetime.utcnow()
            db.commit()
            db.refresh(insight_widget)
            return insight_widget.id
        else:
            widget_create = schemas.WidgetCreate(
                widget_type="insight_analysis",
                title="çœ‹æ¿æ´å¯Ÿåˆ†æ",
                connection_id=1,
                query_config=query_config,
                chart_config=None,
                position_config={"x": 0, "y": 0, "w": 12, "h": 6},
                refresh_interval=0
            )
            
            new_widget = crud.crud_dashboard_widget.create_widget(
                db,
                dashboard_id=dashboard_id,
                obj_in=widget_create
            )
            new_widget.data_cache = data_cache
            db.commit()
            db.refresh(new_widget)
            return new_widget.id

    def _update_insight_widget_result(self, db: Session, widget_id: int, insights: schemas.InsightResult, count: int, status: str):
        widget = crud.crud_dashboard_widget.get(db, id=widget_id)
        if widget:
            query_config = widget.query_config or {}
            query_config["status"] = status
            query_config["analyzed_widget_count"] = count
            query_config["last_analysis_at"] = datetime.utcnow().isoformat()
            
            widget.query_config = query_config
            widget.data_cache = insights.dict(exclude_none=True)
            widget.last_refresh_at = datetime.utcnow()
            db.commit()

    def _update_widget_status(self, db: Session, widget_id: int, status: str, error: str = None):
        widget = crud.crud_dashboard_widget.get(db, id=widget_id)
        if widget:
            query_config = widget.query_config or {}
            query_config["status"] = status
            if error:
                query_config["error"] = error
            widget.query_config = query_config
            db.commit()

# åˆ›å»ºå…¨å±€å®ä¾‹
dashboard_insight_service = DashboardInsightService()
