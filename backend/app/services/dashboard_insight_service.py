"""
Dashboardæ´å¯Ÿåˆ†ææœåŠ¡
è´Ÿè´£æ•°æ®èšåˆã€æ¡ä»¶åº”ç”¨ã€æ´å¯Ÿç”Ÿæˆç¼–æ’
ä¼˜åŒ–ï¼šæ”¯æŒå¼‚æ­¥åå°å¤„ç†
"""
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
from sqlalchemy.orm import Session

from app import crud, schemas
from app.models.dashboard_widget import DashboardWidget
from app.services.graph_relationship_service import graph_relationship_service
from app.db.session import SessionLocal
from app.services.text2sql_utils import retrieve_relevant_schema, format_schema_for_prompt
from app.core.agent_config import get_agent_llm, CORE_AGENT_SQL_GENERATOR
from langchain_core.messages import SystemMessage, HumanMessage

class DashboardInsightService:
    """Dashboardæ´å¯Ÿåˆ†ææœåŠ¡"""
    
    def _build_table_column_whitelist(self, schema_context: dict) -> tuple[str, set, dict]:
        """
        æ„å»ºè¡¨/åˆ—ç™½åå•ï¼Œé˜²æ­¢ LLM å¹»è§‰
        
        Returns:
            whitelist_str: æ ¼å¼åŒ–çš„ç™½åå•å­—ç¬¦ä¸²
            valid_tables: æœ‰æ•ˆè¡¨åé›†åˆ
            valid_columns: {table_name: [column_names]} å­—å…¸
        """
        valid_tables = set()
        valid_columns = {}  # {table_name: [column_names]}
        
        tables = schema_context.get("tables", [])
        columns = schema_context.get("columns", [])
        relationships = schema_context.get("relationships", [])
        
        # æ„å»ºè¡¨åé›†åˆ
        for t in tables:
            table_name = t.get("name", "")
            if table_name:
                valid_tables.add(table_name)
                valid_columns[table_name] = []
        
        # æ„å»ºåˆ—åæ˜ å°„
        for c in columns:
            table_name = c.get("table_name", "")
            col_name = c.get("name", "")
            if table_name and col_name:
                if table_name not in valid_columns:
                    valid_columns[table_name] = []
                valid_columns[table_name].append(col_name)
        
        # æ„å»ºç™½åå•å­—ç¬¦ä¸²
        whitelist_parts = []
        whitelist_parts.append("=" * 60)
        whitelist_parts.append("ã€é‡è¦ã€‘å¯ç”¨è¡¨å’Œå­—æ®µç™½åå•ï¼ˆä»…å…è®¸ä½¿ç”¨ä»¥ä¸‹è¡¨å’Œå­—æ®µï¼‰")
        whitelist_parts.append("=" * 60)
        
        for table_name in sorted(valid_tables):
            cols = valid_columns.get(table_name, [])
            # æ‰¾åˆ°è¡¨çš„æè¿°
            table_desc = ""
            for t in tables:
                if t.get("name") == table_name:
                    table_desc = t.get("description", "")
                    break
            
            whitelist_parts.append(f"\nè¡¨å: {table_name}")
            if table_desc:
                whitelist_parts.append(f"  æè¿°: {table_desc}")
            whitelist_parts.append(f"  å¯ç”¨å­—æ®µ: {', '.join(cols)}")
        
        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if relationships:
            whitelist_parts.append("\n" + "-" * 40)
            whitelist_parts.append("è¡¨é—´å…³ç³»ï¼ˆJOIN æ—¶å¿…é¡»ä½¿ç”¨è¿™äº›å…³è”å­—æ®µï¼‰:")
            # éå†æ‰€æœ‰å…³ç³»ï¼Œä¸é™åˆ¶æ•°é‡ä»¥ç¡®ä¿ JOIN å‡†ç¡®æ€§
            for rel in relationships:
                src = f"{rel.get('source_table', '')}.{rel.get('source_column', '')}"
                tgt = f"{rel.get('target_table', '')}.{rel.get('target_column', '')}"
                whitelist_parts.append(f"  - {src} -> {tgt}")
        
        whitelist_parts.append("\n" + "=" * 60)
        whitelist_parts.append("ã€è­¦å‘Šã€‘ä¸¥ç¦ä½¿ç”¨ä¸Šè¿°ç™½åå•ä¹‹å¤–çš„ä»»ä½•è¡¨æˆ–å­—æ®µï¼")
        whitelist_parts.append("=" * 60)
        
        return "\n".join(whitelist_parts), valid_tables, valid_columns
    
    def _validate_sql_against_whitelist(
        self, 
        sql: str, 
        valid_tables: set, 
        valid_columns: dict,
        db_type: str = "MYSQL"
    ) -> tuple[bool, str, list]:
        """
        éªŒè¯ SQL æ˜¯å¦åªä½¿ç”¨äº†ç™½åå•ä¸­çš„è¡¨å’Œåˆ—
        
        Returns:
            is_valid: æ˜¯å¦æœ‰æ•ˆ
            error_msg: é”™è¯¯ä¿¡æ¯
            invalid_refs: æ— æ•ˆå¼•ç”¨åˆ—è¡¨
        """
        import re
        
        sql_upper = sql.upper()
        invalid_refs = []
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ SELECT è¯­å¥ï¼ˆæ”¯æŒ WITH CTEï¼‰
        sql_stripped = sql_upper.strip()
        if not (sql_stripped.startswith("SELECT") or sql_stripped.startswith("WITH")):
            return False, "SQL å¿…é¡»æ˜¯ SELECT è¯­å¥æˆ– WITH CTE", ["non-select"]
        
        # 2. æ£€æŸ¥å±é™©å…³é”®è¯
        dangerous_keywords = ["DROP", "DELETE", "TRUNCATE", "UPDATE", "INSERT", "ALTER", "CREATE"]
        for kw in dangerous_keywords:
            if kw in sql_upper and "SELECT" not in sql_upper[:20]:
                return False, f"æ£€æµ‹åˆ°å±é™©æ“ä½œ: {kw}", [kw]
        
        # 3. æå– SQL ä¸­çš„è¡¨å
        # åŒ¹é… FROM/JOIN åçš„è¡¨å
        table_pattern = r'(?:FROM|JOIN)\s+[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_tables = re.findall(table_pattern, sql, re.IGNORECASE)
        
        # æ£€æŸ¥è¡¨åæ˜¯å¦åœ¨ç™½åå•ä¸­
        valid_tables_lower = {t.lower() for t in valid_tables}
        for table in found_tables:
            if table.lower() not in valid_tables_lower:
                invalid_refs.append(f"è¡¨ '{table}' ä¸åœ¨ç™½åå•ä¸­")
        
        # 4. æå–å¹¶æ£€æŸ¥åˆ—åï¼ˆç®€åŒ–æ£€æŸ¥ï¼Œåªæ£€æŸ¥ table.column æ ¼å¼ï¼‰
        # åŒ¹é… table.column æˆ– alias.column æ ¼å¼
        col_pattern = r'([a-zA-Z_][a-zA-Z0-9_]*)\s*\.\s*[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_cols = re.findall(col_pattern, sql, re.IGNORECASE)
        
        # æ„å»ºæ‰€æœ‰æœ‰æ•ˆåˆ—åçš„å°å†™é›†åˆ
        all_valid_cols_lower = set()
        for cols in valid_columns.values():
            for col in cols:
                all_valid_cols_lower.add(col.lower())
        
        # æ£€æŸ¥åˆ—åï¼ˆå®¹å¿ä¸€äº›å¸¸è§çš„åˆ«åï¼‰
        common_aliases = {'t', 't1', 't2', 'a', 'b', 'c', 's', 'm', 'o', 'p', 'd', 'main', 'sub'}
        for table_or_alias, col in found_cols:
            # å¦‚æœæ˜¯å¸¸è§åˆ«åï¼Œåªæ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
            if table_or_alias.lower() in common_aliases:
                if col.lower() not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{col}' ä¸åœ¨ç™½åå•ä¸­")
            else:
                # æ£€æŸ¥è¡¨åå’Œåˆ—å
                table_lower = table_or_alias.lower()
                col_lower = col.lower()
                
                # åœ¨æ‰€æœ‰è¡¨ä¸­æŸ¥æ‰¾è¯¥åˆ—
                col_found = False
                for t_name, t_cols in valid_columns.items():
                    if col_lower in [c.lower() for c in t_cols]:
                        col_found = True
                        break
                
                if not col_found and col_lower not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{table_or_alias}.{col}' ä¸åœ¨ç™½åå•ä¸­")
        
        if invalid_refs:
            return False, f"å‘ç° {len(invalid_refs)} ä¸ªæ— æ•ˆå¼•ç”¨", invalid_refs
        
        return True, "", []
    
    def _validate_sql_syntax(
        self,
        sql: str,
        connection_id: int,
        db_type: str = "MYSQL"
    ) -> tuple[bool, str]:
        """
        ä½¿ç”¨ EXPLAIN éªŒè¯ SQL è¯­æ³•æ­£ç¡®æ€§ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
        
        Returns:
            is_valid: æ˜¯å¦æœ‰æ•ˆ
            error_msg: é”™è¯¯ä¿¡æ¯
        """
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            from app.services.db_service import get_db_connection_by_id, get_db_engine
            import sqlalchemy
            
            connection = get_db_connection_by_id(connection_id)
            if not connection:
                return False, "æ•°æ®åº“è¿æ¥ä¸å­˜åœ¨"
            
            engine = get_db_engine(connection, timeout_seconds=10)
            
            # æ ¹æ®æ•°æ®åº“ç±»å‹æ„é€  EXPLAIN è¯­å¥
            db_type_upper = db_type.upper()
            if db_type_upper in ("MYSQL", "MARIADB"):
                explain_sql = f"EXPLAIN {sql}"
            elif db_type_upper == "POSTGRESQL":
                explain_sql = f"EXPLAIN {sql}"
            elif db_type_upper == "SQLITE":
                explain_sql = f"EXPLAIN QUERY PLAN {sql}"
            elif db_type_upper in ("SQLSERVER", "MSSQL"):
                explain_sql = f"SET SHOWPLAN_TEXT ON; {sql}; SET SHOWPLAN_TEXT OFF;"
            elif db_type_upper == "ORACLE":
                explain_sql = f"EXPLAIN PLAN FOR {sql}"
            else:
                # é»˜è®¤ä½¿ç”¨ EXPLAIN
                explain_sql = f"EXPLAIN {sql}"
            
            with engine.connect() as conn:
                # æ‰§è¡Œ EXPLAINï¼Œå¦‚æœSQLæœ‰è¯­æ³•é”™è¯¯ä¼šæŠ›å‡ºå¼‚å¸¸
                conn.execute(sqlalchemy.text(explain_sql))
            
            return True, ""
            
        except Exception as e:
            error_msg = str(e)
            # æå–å…³é”®é”™è¯¯ä¿¡æ¯
            if "aggregate function calls cannot be nested" in error_msg:
                return False, "SQLè¯­æ³•é”™è¯¯: èšåˆå‡½æ•°ä¸èƒ½åµŒå¥—ä½¿ç”¨"
            elif "does not exist" in error_msg:
                return False, f"SQLè¯­æ³•é”™è¯¯: å‡½æ•°æˆ–åˆ—ä¸å­˜åœ¨ - {error_msg[:200]}"
            elif "syntax error" in error_msg.lower():
                return False, f"SQLè¯­æ³•é”™è¯¯: {error_msg[:200]}"
            elif "GroupingError" in error_msg or "grouping" in error_msg.lower():
                return False, f"SQLåˆ†ç»„é”™è¯¯: {error_msg[:200]}"
            else:
                return False, f"SQLéªŒè¯å¤±è´¥: {error_msg[:200]}"
    
    async def generate_mining_suggestions(
        self, 
        db: Session, 
        request: schemas.MiningRequest,
        dashboard_id: Optional[int] = None,  # æ–°å¢ï¼šDashboardä¸Šä¸‹æ–‡
        user_id: Optional[int] = None  # æ–°å¢ï¼šç”¨æˆ·ç”»åƒ
    ) -> schemas.MiningResponse:
        """ç”Ÿæˆæ™ºèƒ½æŒ–æ˜å»ºè®®ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥ + ä¸ªæ€§åŒ– + SQLéªŒè¯ï¼‰"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"[Mining] å¼€å§‹ç”ŸæˆæŒ–æ˜å»ºè®®, connection_id={request.connection_id}, dashboard_id={dashboard_id}, user_id={user_id}")
        
        # 0. è·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
        from app.models.db_connection import DBConnection
        connection = db.query(DBConnection).filter(DBConnection.id == request.connection_id).first()
        db_type = connection.db_type.upper() if connection else "MYSQL"
        logger.info(f"[Mining] æ•°æ®åº“ç±»å‹: {db_type}")
        
        # âœ¨ 1. æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
        context_info = await self._build_mining_context(
            db, request.connection_id, dashboard_id, user_id, request.intent
        )
        logger.info(f"[Mining] ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: {context_info.get('context_description', 'N/A')}")
        
        # 2. è·å– Schemaï¼ˆæ™ºèƒ½ç­›é€‰ï¼‰
        if request.intent or context_info.get("suggested_tables"):
            # æœ‰æ„å›¾æˆ–æ¨èè¡¨æ—¶ï¼Œä½¿ç”¨ç›¸å…³Schema
            schema_context = await self._get_relevant_schema_enhanced(
                db, request.connection_id, request.intent, context_info
            )
        else:
            # æ— æ„å›¾æ—¶ï¼Œä½¿ç”¨å…¨é‡Schema
            schema_context = self._get_full_schema(db, request.connection_id)
        
        if not schema_context.get("tables"):
            logger.warning("[Mining] schema_context ä¸­æ— è¡¨")
            return schemas.MiningResponse(suggestions=[])
        
        logger.info(f"[Mining] Schema åŒ…å« {len(schema_context.get('tables', []))} ä¸ªè¡¨")
        
        # 3. æ„å»ºè¡¨/åˆ—ç™½åå•ï¼ˆé˜²å¹»è§‰æ ¸å¿ƒï¼‰
        whitelist_str, valid_tables, valid_columns = self._build_table_column_whitelist(schema_context)
        logger.info(f"[Mining] ç™½åå•åŒ…å« {len(valid_tables)} ä¸ªè¡¨, å…± {sum(len(cols) for cols in valid_columns.values())} ä¸ªå­—æ®µ")
        
        # 4. æ ¼å¼åŒ– Schema
        schema_str = format_schema_for_prompt(schema_context)
        
        # âœ¨ 5. æ„å»ºå¢å¼ºçš„ Promptï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        prompt = self._build_mining_prompt_enhanced(
            db_type=db_type,
            schema_str=schema_str,
            whitelist_str=whitelist_str,
            context_info=context_info,
            request=request
        )
        try:
            import json
            from app.core.llm_wrapper import LLMWrapper, LLMWrapperConfig
            from app.core.llms import get_default_model
            from app.models.agent_profile import AgentProfile
            from app.models.llm_config import LLMConfiguration
            from app.core.config import settings
            
            # è·å– Agent é…ç½®
            profile = db.query(AgentProfile).filter(AgentProfile.name == CORE_AGENT_SQL_GENERATOR).first()
            
            # è·å– LLM é…ç½®
            llm_config = None
            if profile and profile.llm_config_id:
                llm_config = db.query(LLMConfiguration).filter(
                    LLMConfiguration.id == profile.llm_config_id,
                    LLMConfiguration.is_active == True
                ).first()
            
            # ä½¿ç”¨ LLMWrapperï¼ˆç»Ÿä¸€é‡è¯•ç­–ç•¥ï¼Œæ— è¶…æ—¶é™åˆ¶ï¼‰
            # temperature=0.7 å¢åŠ å¤šæ ·æ€§ï¼Œé¿å…æ¯æ¬¡ç”Ÿæˆç»“æœç›¸åŒ
            llm = get_default_model(config_override=llm_config, caller="dashboard_mining", temperature=0.7)
            wrapper_config = LLMWrapperConfig(
                max_retries=3,
                retry_base_delay=2.0,
                enable_tracing=settings.LANGCHAIN_TRACING_V2,
            )
            wrapper = LLMWrapper(llm=llm, config=wrapper_config, name="dashboard_mining")
            
            response = await wrapper.ainvoke([
                SystemMessage(content="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚

ã€æå…¶é‡è¦çš„è§„åˆ™ - è¿åå°†å¯¼è‡´å»ºè®®è¢«æ‹’ç»ã€‘ï¼š
1. åªè¿”å› JSON æ ¼å¼çš„å“åº”
2. SQL å¿…é¡»æ˜¯çº¯ SELECT æŸ¥è¯¢ï¼Œç»å¯¹ç¦æ­¢ INSERT/UPDATE/DELETE/CREATE/DROP/ALTER
3. æ‰€æœ‰è¡¨åå¿…é¡»ç²¾ç¡®åŒ¹é…ç™½åå•ä¸­çš„è¡¨åï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
4. æ‰€æœ‰åˆ—åå¿…é¡»ç²¾ç¡®åŒ¹é…ç™½åå•ä¸­çš„åˆ—åï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
5. ç¦æ­¢åœ¨ SQL ä¸­åˆ›å»ºå­æŸ¥è¯¢è¡¨åˆ«ååä½¿ç”¨ä¸å­˜åœ¨çš„åˆ—
6. ç¦æ­¢ä½¿ç”¨ CTEï¼ˆWITH å­å¥ï¼‰åˆ›å»ºè™šæ‹Ÿè¡¨
7. å¦‚æœç™½åå•ä¸­æ²¡æœ‰éœ€è¦çš„è¡¨ï¼Œè¯·ç›´æ¥æ”¾å¼ƒè¯¥åˆ†æå»ºè®®

ä¸¥æ ¼éµå®ˆï¼šç™½åå•æ˜¯å”¯ä¸€å¯ç”¨çš„æ•°æ®æºï¼Œä¸èƒ½æƒ³è±¡æˆ–æ¨æµ‹ä»»ä½•è¡¨å/åˆ—åã€‚"""),
                HumanMessage(content=prompt)
            ])
            
            # è§£æ LLM è¿”å›çš„ JSON
            response_text = response.content if hasattr(response, 'content') else str(response)
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            parsed = json.loads(response_text)
            raw_suggestions = parsed.get("suggestions", [])
            logger.info(f"[Mining] LLM è¿”å› {len(raw_suggestions)} ä¸ªåŸå§‹å»ºè®®")
            
            # 5. éªŒè¯æ¯ä¸ª SQL å¹¶è¿‡æ»¤æ— æ•ˆçš„
            validated_suggestions = []
            invalid_count = 0
            syntax_error_count = 0
            
            for idx, s in enumerate(raw_suggestions):
                sql = s.get("sql", "")
                title = s.get("title", f"å»ºè®®{idx+1}")
                
                if not sql:
                    logger.warning(f"[Mining] å»ºè®® '{title}' æ—  SQLï¼Œè·³è¿‡")
                    invalid_count += 1
                    continue
                
                # éªŒè¯ SQL ç™½åå•
                is_valid, error_msg, invalid_refs = self._validate_sql_against_whitelist(
                    sql, valid_tables, valid_columns, db_type
                )
                
                if not is_valid:
                    logger.warning(f"[Mining] å»ºè®® '{title}' SQL éªŒè¯å¤±è´¥: {error_msg}")
                    for ref in invalid_refs[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ— æ•ˆå¼•ç”¨
                        logger.warning(f"[Mining]   - {ref}")
                    invalid_count += 1
                    # ç›´æ¥è·³è¿‡éªŒè¯å¤±è´¥çš„å»ºè®®ï¼Œä¸ä¿ç•™
                    continue
                
                # âœ¨ æ–°å¢: SQL è¯­æ³•éªŒè¯ï¼ˆä½¿ç”¨ EXPLAINï¼‰
                syntax_valid, syntax_error = self._validate_sql_syntax(
                    sql, request.connection_id, db_type
                )
                
                if not syntax_valid:
                    logger.warning(f"[Mining] å»ºè®® '{title}' SQL è¯­æ³•éªŒè¯å¤±è´¥: {syntax_error}")
                    syntax_error_count += 1
                    # è·³è¿‡è¯­æ³•é”™è¯¯çš„å»ºè®®
                    continue
                
                validated_suggestions.append(
                    schemas.MiningSuggestion(
                        title=s.get("title", ""),
                        description=s.get("description", ""),
                        chart_type=s.get("chart_type", "bar"),
                        sql=sql,
                        analysis_intent=s.get("analysis_intent", s.get("title", "æ•°æ®åˆ†æ")),
                        reasoning=s.get("reasoning", s.get("description", "")),
                        mining_dimension=s.get("mining_dimension", "business"),
                        confidence=float(s.get("confidence", 0.8)),
                        source_tables=s.get("source_tables", []),
                        key_fields=s.get("key_fields", []),
                        business_value=s.get("business_value", ""),
                        suggested_actions=s.get("suggested_actions", [])
                    )
                )
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œé«˜ç½®ä¿¡åº¦çš„æ’åœ¨å‰é¢
            validated_suggestions.sort(key=lambda x: x.confidence, reverse=True)
            
            valid_count = len(validated_suggestions)
            total_count = len(raw_suggestions)
            success_rate = (valid_count / total_count * 100) if total_count > 0 else 0
            
            logger.info(f"[Mining] æœ€ç»ˆè¿”å› {valid_count}/{total_count} ä¸ªæœ‰æ•ˆå»ºè®® ({success_rate:.1f}%), "
                        f"{invalid_count} ä¸ªç™½åå•éªŒè¯å¤±è´¥, {syntax_error_count} ä¸ªSQLè¯­æ³•é”™è¯¯è¢«è¿‡æ»¤")
            
            # å¦‚æœæœ‰æ•ˆå»ºè®®å¤ªå°‘ï¼Œè®°å½•è­¦å‘Š
            if success_rate < 50 and total_count > 0:
                logger.warning(f"[Mining] SQLéªŒè¯é€šè¿‡ç‡è¾ƒä½ ({success_rate:.1f}%)ï¼Œå»ºè®®æ£€æŸ¥LLMæ˜¯å¦æ­£ç¡®ç†è§£ç™½åå•çº¦æŸ")
            
            return schemas.MiningResponse(suggestions=validated_suggestions)
            
        except json.JSONDecodeError as e:
            logger.error(f"[Mining] JSON è§£æå¤±è´¥: {e}")
            logger.error(f"[Mining] åŸå§‹å“åº”: {response_text[:500]}...")
            return schemas.MiningResponse(suggestions=[])
        except Exception as e:
            logger.error(f"[Mining] å»ºè®®ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return schemas.MiningResponse(suggestions=[])
    
    async def _build_mining_context(
        self,
        db: Session,
        connection_id: int,
        dashboard_id: Optional[int],
        user_id: Optional[int],
        user_intent: Optional[str]
    ) -> Dict[str, Any]:
        """æ„å»ºæŒ–æ˜å»ºè®®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¸ªæ€§åŒ–æ ¸å¿ƒï¼‰"""
        context = {
            "user_intent": user_intent or "",
            "dashboard_context": {},
            "user_history": {},
            "suggested_tables": [],
            "suggested_dimensions": [],
            "context_description": ""
        }
        
        # 1. Dashboardä¸Šä¸‹æ–‡
        if dashboard_id:
            dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
            if dashboard:
                existing_analysis = []
                for widget in dashboard.widgets:
                    if widget.query_config:
                        intent = widget.query_config.get("query_intent", "")
                        if intent and intent not in existing_analysis:
                            existing_analysis.append(intent)
                
                context["dashboard_context"] = {
                    "name": dashboard.name,
                    "description": dashboard.description or "",
                    "widget_count": len(dashboard.widgets),
                    "existing_analysis": existing_analysis
                }
        
        # 2. ç”¨æˆ·å†å²ï¼ˆæŸ¥è¯¢ç”¨æˆ·æœ€è¿‘çš„æŸ¥è¯¢æ„å›¾ï¼‰
        if user_id:
            from app.models.dashboard import Dashboard
            recent_dashboards = db.query(Dashboard).filter(
                Dashboard.owner_id == user_id  # ä¿®å¤ï¼šä½¿ç”¨ owner_id è€Œä¸æ˜¯ created_by
            ).order_by(Dashboard.created_at.desc()).limit(5).all()
            
            user_intents = []
            user_tables = set()
            for dash in recent_dashboards:
                for widget in dash.widgets:
                    if widget.query_config:
                        intent = widget.query_config.get("query_intent")
                        if intent and intent not in user_intents:
                            user_intents.append(intent)
                        
                        if "source_tables" in widget.query_config:
                            user_tables.update(widget.query_config["source_tables"])
            
            context["user_history"] = {
                "recent_intents": user_intents[:10],
                "frequently_used_tables": list(user_tables)
            }
            
            context["suggested_tables"] = list(user_tables)[:5]
        
        # 3. åŸºäºæ„å›¾çš„ç»´åº¦å»ºè®®
        if user_intent:
            intent_lower = user_intent.lower()
            if any(kw in intent_lower for kw in ["è¶‹åŠ¿", "æ—¶é—´", "å˜åŒ–", "å¢é•¿", "trend", "time"]):
                context["suggested_dimensions"].append("trend")
            if any(kw in intent_lower for kw in ["å¼‚å¸¸", "é—®é¢˜", "é£é™©", "anomaly", "issue"]):
                context["suggested_dimensions"].append("anomaly")
            if any(kw in intent_lower for kw in ["ä¸šåŠ¡", "æŒ‡æ ‡", "KPI", "business", "metric"]):
                context["suggested_dimensions"].extend(["business", "metric"])
            if any(kw in intent_lower for kw in ["å…³è”", "å…³ç³»", "ç›¸å…³", "correlation", "relationship"]):
                context["suggested_dimensions"].append("semantic")
        
        if not context["suggested_dimensions"]:
            context["suggested_dimensions"] = ["business", "metric", "trend"]
        
        # 4. ç”Ÿæˆä¸Šä¸‹æ–‡æè¿°
        desc_parts = []
        if dashboard_id:
            desc_parts.append(f"å½“å‰çœ‹æ¿: {context['dashboard_context'].get('name', 'æœªå‘½å')}")
            existing = context["dashboard_context"].get("existing_analysis", [])
            if existing:
                desc_parts.append(f"å·²æœ‰{len(existing)}ä¸ªåˆ†æ")
        
        if context["user_history"].get("recent_intents"):
            desc_parts.append(f"ç”¨æˆ·åå¥½: {', '.join(context['user_history']['recent_intents'][:2])}")
        
        context["context_description"] = "; ".join(desc_parts) if desc_parts else "å…¨æ–°åˆ†æ"
        
        return context
    
    async def _get_relevant_schema_enhanced(
        self,
        db: Session,
        connection_id: int,
        user_intent: Optional[str],
        context_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è·å–ç›¸å…³Schemaï¼ˆå¢å¼ºç‰ˆï¼šè€ƒè™‘ä¸Šä¸‹æ–‡ï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·å†å²ä¸­çš„è¡¨
        suggested_tables = context_info.get("suggested_tables", [])
        
        if user_intent:
            # ä½¿ç”¨åŸæœ‰çš„è¯­ä¹‰æœç´¢
            schema_context = retrieve_relevant_schema(db, connection_id, user_intent)
        else:
            # ä½¿ç”¨æ¨èçš„è¡¨
            schema_context = self._get_schema_for_tables(db, connection_id, suggested_tables)
        
        return schema_context
    
    def _get_schema_for_tables(
        self,
        db: Session,
        connection_id: int,
        table_names: List[str]
    ) -> Dict[str, Any]:
        """è·å–æŒ‡å®šè¡¨çš„Schema"""
        if not table_names:
            return self._get_full_schema(db, connection_id)
        
        tables = crud.schema_table.get_by_connection(db=db, connection_id=connection_id)
        
        tables_list = []
        columns_list = []
        matched_table_names = []
        
        for table in tables:
            if table.table_name in table_names:
                matched_table_names.append(table.table_name)
                tables_list.append({
                    "id": table.id,
                    "name": table.table_name,
                    "description": table.description or ""
                })
                
                columns = crud.schema_column.get_by_table(db=db, table_id=table.id)
                for col in columns:
                    columns_list.append({
                        "id": col.id,
                        "name": col.column_name,
                        "type": col.data_type,
                        "description": col.description or "",
                        "is_primary_key": col.is_primary_key,
                        "is_foreign_key": col.is_foreign_key,
                        "table_id": table.id,
                        "table_name": table.table_name
                    })
        
        # è·å–è¡¨ä¹‹é—´çš„å…³ç³»
        relationships = []
        if matched_table_names:
            try:
                relationship_context = graph_relationship_service.query_table_relationships(
                    connection_id=connection_id,
                    table_names=matched_table_names
                )
                if relationship_context.get("direct_relationships"):
                    for rel in relationship_context["direct_relationships"]:
                        relationships.append({
                            "source_table": rel.get("source_table"),
                            "source_column": rel.get("source_column"),
                            "target_table": rel.get("target_table"),
                            "target_column": rel.get("target_column"),
                            "relationship_type": rel.get("relationship_type", "references")
                        })
            except Exception:
                pass
        
        return {
            "tables": tables_list,
            "columns": columns_list,
            "relationships": relationships
        }
    
    def _get_full_schema(self, db: Session, connection_id: int) -> Dict[str, Any]:
        """è·å–å®Œæ•´Schema"""
        tables = crud.schema_table.get_by_connection(db=db, connection_id=connection_id)
        
        if not tables:
            return {"tables": [], "columns": [], "relationships": []}
        
        tables_list = []
        columns_list = []
        table_names = []
        
        for table in tables:
            table_names.append(table.table_name)
            tables_list.append({
                "id": table.id,
                "name": table.table_name,
                "description": table.description or ""
            })
            
            columns = crud.schema_column.get_by_table(db=db, table_id=table.id)
            for col in columns:
                columns_list.append({
                    "id": col.id,
                    "name": col.column_name,
                    "type": col.data_type,
                    "description": col.description or "",
                    "is_primary_key": col.is_primary_key,
                    "is_foreign_key": col.is_foreign_key,
                    "table_id": table.id,
                    "table_name": table.table_name
                })
        
        # è·å–è¡¨ä¹‹é—´çš„å…³ç³»
        relationships = []
        try:
            relationship_context = graph_relationship_service.query_table_relationships(
                connection_id=connection_id,
                table_names=table_names
            )
            if relationship_context.get("direct_relationships"):
                for rel in relationship_context["direct_relationships"]:
                    relationships.append({
                        "source_table": rel.get("source_table"),
                        "source_column": rel.get("source_column"),
                        "target_table": rel.get("target_table"),
                        "target_column": rel.get("target_column"),
                        "relationship_type": rel.get("relationship_type", "references")
                    })
        except Exception:
            pass
        
        return {
            "tables": tables_list,
            "columns": columns_list,
            "relationships": relationships
        }
    
    def _build_mining_prompt_enhanced(
        self,
        db_type: str,
        schema_str: str,
        whitelist_str: str,
        context_info: Dict[str, Any],
        request: schemas.MiningRequest
    ) -> str:
        """æ„å»ºå¢å¼ºçš„æŒ–æ˜ Promptï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰"""
        
        # SQL è¯­æ³•æŒ‡å—
        sql_syntax_guides = {
            "MYSQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMySQLï¼‰ï¼š
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOINï¼Œè¯·ä½¿ç”¨ LEFT JOIN æˆ– RIGHT JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—
- å¸ƒå°”å€¼ä½¿ç”¨ 1/0 æˆ– TRUE/FALSE""",
            "POSTGRESQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆPostgreSQLï¼‰ï¼š
- å¯ä½¿ç”¨ LIMIT æˆ– FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- æ”¯æŒ ARRAY ç±»å‹å’Œ JSON æ“ä½œ""",
            "SQLITE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQLiteï¼‰ï¼š
- ä½¿ç”¨ LIMITï¼Œä¸æ”¯æŒ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ strftime()
- ä¸æ”¯æŒ FULL OUTER JOIN å’Œ RIGHT JOIN
- ä½¿ç”¨åŒå¼•å· " æˆ–æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ç±»å‹ç³»ç»Ÿçµæ´»ï¼Œæ— ä¸¥æ ¼ç±»å‹æ£€æŸ¥""",
            "SQLSERVER": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQL Server / MSSQLï¼‰ï¼š
- ä½¿ç”¨ TOP N æˆ– OFFSET...FETCH
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ + æ“ä½œç¬¦æˆ– CONCAT()
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ FORMAT() æˆ– CONVERT()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ GETDATE() è·å–å½“å‰æ—¶é—´""",
            "ORACLE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆOracleï¼‰ï¼š
- ä½¿ç”¨ ROWNUM æˆ– FETCH FIRSTï¼ˆ12c+ï¼‰
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ SYSDATE è·å–å½“å‰æ—¶é—´
- FROM å­å¥å¿…é¡»æœ‰è¡¨ï¼ˆå¯ç”¨ DUALï¼‰""",
            "MARIADB": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMariaDBï¼‰ï¼š
- è¯­æ³•ä¸ MySQL åŸºæœ¬å…¼å®¹
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—""",
            "CLICKHOUSE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆClickHouseï¼‰ï¼š
- ä½¿ç”¨ LIMIT è¿›è¡Œåˆ†é¡µ
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ concat() æˆ– ||
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ formatDateTime()
- æ”¯æŒ FULL OUTER JOIN (éƒ¨åˆ†ç‰ˆæœ¬)
- åŒºåˆ†å¤§å°å†™ï¼Œä½¿ç”¨åŒå¼•å·åŒ…è£¹
- ä¸“ä¸º OLAP ä¼˜åŒ–ï¼ŒèšåˆæŸ¥è¯¢æ€§èƒ½ä¼˜å¼‚""",
        }
        
        sql_syntax_guide = sql_syntax_guides.get(db_type, f"""
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆ{db_type}ï¼‰ï¼š
- è¯·ä½¿ç”¨æ ‡å‡† ANSI SQL è¯­æ³•
- é¿å…ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„æ‰©å±•è¯­æ³•
- ä½¿ç”¨é€šç”¨çš„èšåˆå‡½æ•°ï¼ˆSUM, COUNT, AVG, MAX, MINï¼‰
- ä½¿ç”¨æ ‡å‡†çš„ JOIN è¯­æ³•ï¼ˆINNER JOIN, LEFT JOINï¼‰
- æ—¥æœŸå‡½æ•°è¯·æ ¹æ®å®é™…æ•°æ®åº“è°ƒæ•´""")
        
        # âœ¨ ä¸Šä¸‹æ–‡æè¿°
        context_section = f"""
ã€é‡è¦ã€‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆè¯·åŸºäºæ­¤ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®ï¼‰ï¼š
- åˆ†æåœºæ™¯: {context_info.get("context_description", "é€šç”¨åˆ†æ")}
- ç”¨æˆ·æ„å›¾: {request.intent or context_info.get("user_intent") or "è‡ªåŠ¨å‘ç°å…³é”®æŒ‡æ ‡"}
"""
        
        # Dashboard å·²æœ‰åˆ†æ
        existing_analysis = context_info.get("dashboard_context", {}).get("existing_analysis", [])
        if existing_analysis:
            context_section += f"- å·²æœ‰åˆ†æ: {', '.join(existing_analysis[:5])}\n"
            context_section += "  ã€è¯·é¿å…æ¨èä¸å·²æœ‰åˆ†æé‡å¤çš„å†…å®¹ã€‘\n"
        
        # ç”¨æˆ·å†å²åå¥½
        user_intents = context_info.get("user_history", {}).get("recent_intents", [])
        if user_intents:
            context_section += f"- ç”¨æˆ·å†å²åå¥½: {', '.join(user_intents[:3])}\n"
        
        # æ¨èç»´åº¦
        suggested_dims = context_info.get("suggested_dimensions", ["business", "metric", "trend"])
        context_section += f"- æ¨èç»´åº¦: {', '.join(suggested_dims)}\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œæ¨è {request.limit} ä¸ª**ä¸ªæ€§åŒ–çš„**æ•°æ®åˆ†æè§†è§’ã€‚

{context_section}

ç›®æ ‡æ•°æ®åº“ç±»å‹ï¼š{db_type}
{sql_syntax_guide}

ã€â—æå…¶é‡è¦ - å¿…é¡»ä¸¥æ ¼éµå®ˆâ—ã€‘
{whitelist_str}

âš ï¸ ä¸¥é‡è­¦å‘Šï¼š
1. ä½ åªèƒ½ä½¿ç”¨ä¸Šè¿°ç™½åå•ä¸­æ˜ç¡®åˆ—å‡ºçš„è¡¨åå’Œå­—æ®µå
2. ç¦æ­¢åˆ›å»ºå­æŸ¥è¯¢ä¸­çš„ä¸´æ—¶è¡¨æˆ– CTEï¼ˆWITH å­å¥ï¼‰ä½¿ç”¨ä¸å­˜åœ¨çš„è¡¨å
3. ç¦æ­¢ä½¿ç”¨ä»»ä½• CREATEã€DROPã€ALTERã€INSERTã€UPDATEã€DELETE ç­‰æ“ä½œ
4. å¦‚æœæŸä¸ªåˆ†æéœ€è¦çš„è¡¨ä¸åœ¨ç™½åå•ä¸­ï¼Œè¯·æ”¾å¼ƒè¯¥åˆ†æï¼Œä¸è¦å°è¯•ç”Ÿæˆ
5. è¡¨åˆ«ååå¿…é¡»ä½¿ç”¨ç™½åå•ä¸­çš„çœŸå®åˆ—åï¼Œä¸èƒ½è‡ªå·±åˆ›é€ åˆ—å

âœ… æ­£ç¡®ç¤ºä¾‹ï¼ˆå‡è®¾ç™½åå•æœ‰ orders è¡¨ï¼ŒåŒ…å« order_id, amount, order_date åˆ—ï¼‰ï¼š
```sql
-- âœ… æ­£ç¡®ï¼šä¸ä½¿ç”¨ LIMITï¼Œè¿”å›å®Œæ•´æ•°æ®
SELECT 
    DATE_TRUNC('month', order_date) as month,
    SUM(amount) as total_amount,
    COUNT(order_id) as order_count
FROM orders
WHERE order_date >= '2024-01-01'
GROUP BY DATE_TRUNC('month', order_date)
ORDER BY month DESC
-- æ³¨æ„ï¼šä¸è¦æ·»åŠ  LIMITï¼Œè®©å‰ç«¯æ ¹æ®éœ€è¦æ˜¾ç¤º
```

âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆä½¿ç”¨äº†ä¸å­˜åœ¨çš„è¡¨/åˆ—ï¼‰ï¼š
```sql
-- é”™è¯¯1ï¼šä½¿ç”¨äº†ä¸åœ¨ç™½åå•çš„è¡¨ monthly_sales
SELECT * FROM monthly_sales  -- âŒ monthly_sales ä¸åœ¨ç™½åå•

-- é”™è¯¯2ï¼šä½¿ç”¨äº†ä¸å­˜åœ¨çš„åˆ—
SELECT order_id, customer_name FROM orders  -- âŒ customer_name ä¸åœ¨ç™½åå•

-- é”™è¯¯3ï¼šå­æŸ¥è¯¢ä½¿ç”¨è™šæ‹Ÿè¡¨
WITH sales_summary AS (  -- âŒ ä¸è¦ä½¿ç”¨ CTE åˆ›å»ºè™šæ‹Ÿè¡¨
    SELECT * FROM imaginary_table
)
SELECT * FROM sales_summary

-- é”™è¯¯4ï¼šä½¿ç”¨ UPDATE
UPDATE orders SET amount = 100  -- âŒ ç¦æ­¢ UPDATE
```

æ•°æ®åº“ç»“æ„è¯¦æƒ…ï¼š
{schema_str}

ã€æ ¸å¿ƒè¦æ±‚ - æŒ‰ä¼˜å…ˆçº§æ’åºã€‘ï¼š
1. âš ï¸ã€æœ€é«˜ä¼˜å…ˆçº§ã€‘SQL å¿…é¡»100%éµå®ˆç™½åå•çº¦æŸï¼š
   - æ¯ä¸ªè¡¨åéƒ½å¿…é¡»åœ¨ç™½åå•çš„"å¯ç”¨è¡¨"åˆ—è¡¨ä¸­
   - æ¯ä¸ªåˆ—åéƒ½å¿…é¡»åœ¨ç™½åå•çš„"è¡¨.åˆ—"åˆ—è¡¨ä¸­
   - ä¸èƒ½ä½¿ç”¨ç™½åå•ä¹‹å¤–çš„ä»»ä½•è¡¨åæˆ–åˆ—å
   - ä¸èƒ½ä½¿ç”¨ INSERT/UPDATE/DELETE/CREATE/DROP/ALTER
2. âš ï¸ã€é‡è¦ã€‘SQL ä¸è¦æ·»åŠ  LIMIT é™åˆ¶ï¼Œè¿”å›å®Œæ•´æ•°æ®é›†
3. åˆ†æå»ºè®®å¿…é¡»ç»“åˆä¸Šè¿°ä¸Šä¸‹æ–‡ï¼Œä½“ç°ä¸ªæ€§åŒ–
4. é¿å…æ¨èä¸"å·²æœ‰åˆ†æ"é‡å¤çš„å†…å®¹
5. ä¼˜å…ˆè¦†ç›–"æ¨èç»´åº¦"ä¸­çš„åˆ†æç±»å‹
6. æ¯ä¸ªæ¨èéƒ½è¦æœ‰æ˜ç¡®çš„ reasoningï¼ˆè§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™ä¸ªåˆ†æï¼‰
7. business_value è¦è¯´æ˜è¿™ä¸ªåˆ†æèƒ½å¸®åŠ©ç”¨æˆ·åšä»€ä¹ˆå†³ç­–

æŒ–æ˜ç»´åº¦è¯´æ˜ï¼š
- businessï¼ˆä¸šåŠ¡æ•°æ®ï¼‰ï¼šæ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ã€KPI
- metricï¼ˆæŒ‡æ ‡åˆ†æï¼‰ï¼šå…³é”®æ•°å€¼çš„ç»Ÿè®¡åˆ†å¸ƒ
- trendï¼ˆè¶‹åŠ¿åˆ†æï¼‰ï¼šæ—¶é—´åºåˆ—å˜åŒ–
- semanticï¼ˆè¯­ä¹‰å…³è”ï¼‰ï¼šåŸºäºå­—æ®µè¯­ä¹‰å‘ç°çš„å…³è”åˆ†æ

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "suggestions": [
    {{
      "title": "å›¾è¡¨æ ‡é¢˜",
      "description": "ç®€çŸ­æè¿°",
      "reasoning": "ä¸ºä»€ä¹ˆæ¨èè¿™ä¸ªåˆ†æï¼Ÿç»“åˆä¸Šä¸‹æ–‡è¯´æ˜",
      "mining_dimension": "business|metric|trend|semantic",
      "confidence": 0.85,
      "chart_type": "bar|line|pie|scatter|table",
      "sql": "SELECT ...",
      "source_tables": ["è¡¨å1"],
      "key_fields": ["å­—æ®µ1"],
      "business_value": "è¿™ä¸ªåˆ†æèƒ½å¸®åŠ©ä¸šåŠ¡åšä»€ä¹ˆå†³ç­–",
      "suggested_actions": ["å»ºè®®åŠ¨ä½œ1"],
      "analysis_intent": "åˆ†ææ„å›¾"
    }}
  ]
}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""
        
        return prompt

    def trigger_dashboard_insights(
        self,
        db: Session,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest
    ) -> schemas.DashboardInsightResponse:
        """
        è§¦å‘çœ‹æ¿æ´å¯Ÿç”Ÿæˆï¼ˆåˆ›å»ºå ä½Widgetï¼Œåç»­ç”±åå°ä»»åŠ¡å¤„ç†ï¼‰
        """
        # 1. æ£€æŸ¥æƒé™
        self._check_permission(db, dashboard_id, user_id)
        
        # 2. è·å–Dashboard
        dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
        if not dashboard:
            raise ValueError(f"Dashboard {dashboard_id} not found")
            
        # 3. åˆ›å»ºæˆ–æ›´æ–°Widgetä¸º"åˆ†æä¸­"çŠ¶æ€
        # åˆ›å»ºåˆå§‹çš„ç©ºç»“æœ
        initial_result = schemas.InsightResult(
            summary=schemas.InsightSummary(total_rows=0, key_metrics={}, time_range="åˆ†æä¸­..."),
            trends=None, anomalies=[], correlations=[], recommendations=[]
        )
        
        # åˆ›å»ºæˆ–æ›´æ–° Widget (åŒæ­¥)
        widget_id = self._create_or_update_insight_widget(
            db,
            dashboard_id,
            initial_result,
            request.conditions,
            request.use_graph_relationships,
            analyzed_widget_count=0,
            status="processing" # æ ‡è®°ä¸ºå¤„ç†ä¸­
        )
        
        return schemas.DashboardInsightResponse(
            widget_id=widget_id,
            insights=initial_result,
            analyzed_widget_count=0,
            analysis_timestamp=datetime.utcnow(),
            applied_conditions=request.conditions,
            relationship_count=0,
            status="processing" # æ–°å¢çŠ¶æ€å­—æ®µ
        )

    async def process_dashboard_insights_task(
        self,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest,
        widget_id: int
    ):
        """
        åå°ä»»åŠ¡ï¼šæ‰§è¡Œå®é™…çš„æ´å¯Ÿåˆ†æé€»è¾‘
        
        P1-FIX: ä¼˜åŒ–Sessionç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œæ›´å¥å£®çš„é”™è¯¯å¤„ç†
        æ³¨æ„: LLM é‡è¯•ç”± LLMWrapper ç»Ÿä¸€å¤„ç†ï¼Œæ­¤å¤„ä¸å†éœ€è¦å¤–å±‚é‡è¯•é€»è¾‘
        """
        import logging
        from contextlib import contextmanager
        
        logger = logging.getLogger(__name__)
        
        @contextmanager
        def get_db_session():
            """P1-FIX: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿Sessionæ­£ç¡®å…³é—­"""
            session = SessionLocal()
            try:
                yield session
            except Exception:
                session.rollback()
                raise
            finally:
                session.close()
        
        with get_db_session() as db:
            try:
                logger.info(f"ğŸš€ å¼€å§‹åå°æ´å¯Ÿåˆ†æ Task (Dashboard: {dashboard_id}, Widget: {widget_id})")
                
                # 1. è·å–æ•°æ®
                dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
                if not dashboard:
                    raise ValueError(f"Dashboard {dashboard_id} not found")
                
                widgets = dashboard.widgets
                
                # ç­›é€‰Widgets
                if request.included_widget_ids:
                    widgets = [w for w in widgets if w.id in request.included_widget_ids]
                
                data_widgets = [w for w in widgets if w.widget_type != "insight_analysis"]
                
                if not data_widgets:
                    logger.warning(f"âš ï¸ Dashboard {dashboard_id} æ— æœ‰æ•ˆæ•°æ®ç»„ä»¶ï¼Œè·³è¿‡åˆ†æ")
                    self._update_widget_status(db, widget_id, "completed", "æ— æ•°æ®ç»„ä»¶å¯åˆ†æ")
                    return

                if getattr(request, "force_requery", False):
                    self._refresh_data_widgets(db, data_widgets, user_id)
                    refreshed_widgets = []
                    for w in data_widgets:
                        w2 = crud.crud_dashboard_widget.get(db, id=w.id)
                        if w2 and w2.widget_type != "insight_analysis":
                            refreshed_widgets.append(w2)
                    data_widgets = refreshed_widgets

                # 2. èšåˆæ•°æ®
                aggregated_data = self._aggregate_widget_data(data_widgets, request.conditions)
                logger.info(f"ğŸ“Š èšåˆæ•°æ®å®Œæˆ: {aggregated_data['total_rows']} è¡Œ, {len(aggregated_data['table_names'])} ä¸ªè¡¨")
                
                # âœ¨ 3. è°ƒç”¨ LangGraph å·¥ä½œæµè¿›è¡Œæ™ºèƒ½åˆ†æ
                from app.agents.dashboard_insight_graph import analyze_dashboard
                
                connection_id = data_widgets[0].connection_id
                user_intent = self._extract_user_intent(dashboard, data_widgets)
                
                logger.info(f"ğŸ¤– è°ƒç”¨ LangGraph è¿›è¡Œæ™ºèƒ½åˆ†æ, intent: {user_intent[:50]}...")
                
                analysis_result = await analyze_dashboard(
                    dashboard=dashboard,
                    aggregated_data=aggregated_data,
                    use_graph_relationships=request.use_graph_relationships,
                    analysis_dimensions=request.analysis_dimensions,
                    connection_id=connection_id,
                    user_intent=user_intent
                )
                
                # 4. æå–ç»“æœ
                insights = analysis_result.get("insights")
                lineage = analysis_result.get("lineage") or {}
                relationship_context = analysis_result.get("relationship_context")
                relationship_count = relationship_context.get("relationship_count", 0) if relationship_context else 0
                
                logger.info(f"ğŸ¯ LangGraph åˆ†æå®Œæˆ, relationship_count={relationship_count}")
                
                # 5. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæº¯æºä¿¡æ¯ï¼‰
                confidence = self._calculate_confidence_from_lineage(lineage, aggregated_data)
                
                # 6. ç”ŸæˆåŠ¨æ€åˆ†ææ–¹æ³•è¯´æ˜
                analysis_method = self._generate_dynamic_analysis_method(lineage, insights, aggregated_data)
                
                # 7. æ›´æ–° Widget çŠ¶æ€ä¸ºå®Œæˆ
                self._update_insight_widget_result(
                    db, 
                    widget_id, 
                    insights, 
                    len(data_widgets),
                    status="completed",
                    analysis_method=analysis_method,
                    confidence_score=confidence,
                    relationship_count=relationship_count,
                    source_tables=aggregated_data.get("table_names"),
                    extra_metrics=lineage
                )
                
                logger.info(f"âœ… åå°æ´å¯Ÿåˆ†æå®Œæˆ (Widget: {widget_id})")
                
            except Exception as e:
                logger.exception(f"âŒ åå°æ´å¯Ÿåˆ†æå¤±è´¥: dashboard_id={dashboard_id}, widget_id={widget_id}")
                # P1-FIX: åœ¨åŒä¸€ä¸ªSessionä¸­æ›´æ–°å¤±è´¥çŠ¶æ€
                try:
                    db.rollback()  # å…ˆå›æ»šä¹‹å‰çš„ä»»ä½•æœªæäº¤çš„æ›´æ”¹
                    self._update_widget_status(db, widget_id, "failed", str(e))
                except Exception as update_error:
                    logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {update_error}")
    
    def _extract_user_intent(self, dashboard: Any, widgets: List[DashboardWidget]) -> str:
        """ä»Dashboardå’ŒWidgetä¸Šä¸‹æ–‡ä¸­æå–ç”¨æˆ·æ„å›¾"""
        intent_parts = []
        
        # 1. Dashboardæè¿°
        if dashboard.description:
            intent_parts.append(f"çœ‹æ¿ä¸»é¢˜: {dashboard.description}")
        
        # 2. Widgetç±»å‹åˆ†å¸ƒ
        widget_types = {}
        for w in widgets:
            widget_types[w.widget_type] = widget_types.get(w.widget_type, 0) + 1
        
        if widget_types:
            type_desc = ", ".join([f"{k}({v}ä¸ª)" for k, v in widget_types.items()])
            intent_parts.append(f"åŒ…å«ç»„ä»¶: {type_desc}")
        
        # 3. è¡¨åå’Œåˆ†ææ„å›¾
        table_names = set()
        query_intents = []
        for w in widgets:
            if w.query_config:
                # æå–æŸ¥è¯¢æ„å›¾
                if "query_intent" in w.query_config:
                    intent = w.query_config["query_intent"]
                    if intent and intent not in query_intents:
                        query_intents.append(intent)
                
                # æå–è¡¨å
                if "source_tables" in w.query_config:
                    table_names.update(w.query_config["source_tables"])
        
        if query_intents:
            intent_parts.append(f"å·²æœ‰åˆ†æ: {', '.join(query_intents[:3])}")
        
        if table_names:
            intent_parts.append(f"æ•°æ®æ¥æº: {', '.join(list(table_names)[:5])}")
        
        return "; ".join(intent_parts) if intent_parts else "è‡ªåŠ¨å‘ç°å…³é”®ä¸šåŠ¡æŒ‡æ ‡å’Œè¶‹åŠ¿"
    
    
    def _calculate_confidence_from_lineage(self, lineage: Optional[Dict], aggregated_data: Dict) -> float:
        """åŸºäºæ•°æ®æº¯æºä¿¡æ¯è®¡ç®—ç½®ä¿¡åº¦"""
        base_confidence = 0.7
        
        if not lineage:
            return base_confidence
        
        # 1. æ•°æ®é‡åŠ åˆ†
        total_rows = aggregated_data.get("total_rows", 0)
        if total_rows >= 200:
            base_confidence += 0.15
        elif total_rows >= 50:
            base_confidence += 0.1
        elif total_rows < 10:
            base_confidence -= 0.2
        
        # 2. å…³ç³»å›¾è°±åŠ åˆ†
        exec_meta = lineage.get("execution_metadata", {})
        if isinstance(exec_meta, dict):
            relationship_count = exec_meta.get("relationship_count", 0)
            if relationship_count > 0:
                base_confidence += 0.05
        
        # 3. LLMåˆ†æåŠ åˆ†
        insight_analysis = lineage.get("insight_analysis", {})
        if isinstance(insight_analysis, dict):
            analysis_method = insight_analysis.get("method", "rule_based")
            if analysis_method == "llm":
                base_confidence += 0.15  # LLMåˆ†æè´¨é‡æ›´é«˜
        
        # 4. é¢„æµ‹å‡†ç¡®åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        trend_meta = aggregated_data.get("_trend_metadata", {})
        if isinstance(trend_meta, dict) and "accuracy_mape" in trend_meta:
            mape = float(trend_meta["accuracy_mape"])
            quality_boost = (1 - min(100.0, max(0.0, mape)) / 100.0) * 0.1
            base_confidence += quality_boost
        
        return max(0.3, min(0.95, round(base_confidence, 2)))
    
    
    def _generate_dynamic_analysis_method(
        self, 
        lineage: Optional[Dict], 
        insights: Any,
        aggregated_data: Dict
    ) -> str:
        """åŠ¨æ€ç”Ÿæˆåˆ†ææ–¹æ³•è¯´æ˜ï¼ˆå¯è§£é‡Šæ€§ï¼‰"""
        method_parts = []
        
        if not lineage:
            return "langgraph_workflow"
        
        # 1. æ•°æ®æºæè¿°
        source_tables = lineage.get("source_tables", [])
        if source_tables:
            method_parts.append(f"sources={len(source_tables)}_tables")
        
        # 2. SQLç”Ÿæˆæ–¹æ³•
        sql_gen = lineage.get("sql_generation_trace", {})
        if isinstance(sql_gen, dict):
            gen_method = sql_gen.get("generation_method", "standard")
            if gen_method != "standard":
                method_parts.append(f"sql={gen_method}")
        
        # 3. åˆ†ææ–¹æ³•ï¼ˆæ ¸å¿ƒï¼‰
        insight_analysis = lineage.get("insight_analysis", {})
        if isinstance(insight_analysis, dict):
            analysis_method = insight_analysis.get("method", "rule_based")
            method_parts.append(f"analysis={analysis_method}")
            
            # æ•°æ®è¡Œæ•°
            data_rows = insight_analysis.get("data_rows_analyzed", 0)
            if data_rows > 0:
                method_parts.append(f"rows={data_rows}")
        
        # 4. æ•°æ®å¤„ç†æ­¥éª¤
        transformations = lineage.get("data_transformations", [])
        if transformations and len(transformations) > 0:
            method_parts.append(f"transforms={len(transformations)}")
        
        # 5. ç‰¹æ®Šèƒ½åŠ›æ ‡è®°
        exec_meta = lineage.get("execution_metadata", {})
        if isinstance(exec_meta, dict):
            if exec_meta.get("from_cache"):
                method_parts.append("cached")
            
            # å…³ç³»å›¾è°±
            rel_count = exec_meta.get("relationship_count", 0)
            if rel_count > 0:
                method_parts.append(f"graph_rels={rel_count}")
        
        # 6. è¶‹åŠ¿åˆ†æè´¨é‡æŒ‡æ ‡
        trend_meta = aggregated_data.get("_trend_metadata", {})
        if isinstance(trend_meta, dict):
            if trend_meta.get("r_squared"):
                r2 = float(trend_meta["r_squared"])
                method_parts.append(f"trend_r2={r2:.2f}")
            if trend_meta.get("accuracy_mape"):
                mape = float(trend_meta["accuracy_mape"])
                method_parts.append(f"mape={mape:.1f}%")
        
        # 7. å¼‚å¸¸æ£€æµ‹æ–¹æ³•
        if insights and hasattr(insights, 'anomalies') and insights.anomalies:
            method_parts.append(f"anomalies={len(insights.anomalies)}")
        
        return "+".join(method_parts) if method_parts else "langgraph_analysis"
    
    def _extract_key_metrics(self, aggregated_data: dict) -> dict:
        """ä»èšåˆæ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡"""
        key_metrics = {}

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        widget_groups = aggregated_data.get("by_widget")
        if widget_groups:
            total_added = 0
            for g in widget_groups:
                data = g.get("data") or []
                numeric_columns = g.get("numeric_columns") or []
                if not data or not numeric_columns:
                    continue
                prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")
                for col in numeric_columns[:5]:
                    values = [_as_float(row.get(col)) for row in data]
                    numeric_values = [v for v in values if v is not None]
                    if not numeric_values:
                        continue
                    key = f"{prefix}.{col}"
                    key_metrics[key] = {
                        "sum": round(sum(numeric_values), 2),
                        "avg": round(sum(numeric_values) / len(numeric_values), 2),
                        "max": round(max(numeric_values), 2),
                        "min": round(min(numeric_values), 2),
                        "count": len(numeric_values)
                    }
                    total_added += 1
                    if total_added >= 12:
                        return key_metrics
            return key_metrics

        data = aggregated_data.get("data", [])
        numeric_columns = aggregated_data.get("numeric_columns", [])

        if not data or not numeric_columns:
            return key_metrics

        for col in numeric_columns[:5]:
            numeric_values = [_as_float(row.get(col)) for row in data]
            numeric_values = [v for v in numeric_values if v is not None]
            if numeric_values:
                key_metrics[col] = {
                    "sum": round(sum(numeric_values), 2),
                    "avg": round(sum(numeric_values) / len(numeric_values), 2),
                    "max": round(max(numeric_values), 2),
                    "min": round(min(numeric_values), 2),
                    "count": len(numeric_values)
                }
        
        return key_metrics
    
    def _analyze_trends(self, aggregated_data: dict) -> Optional[schemas.InsightTrend]:
        """åˆ†ææ•°æ®è¶‹åŠ¿"""
        try:
            from datetime import datetime, date

            def _try_parse_datetime(value: Any):
                if value is None:
                    return None
                if isinstance(value, datetime):
                    return value
                if isinstance(value, date):
                    return datetime.combine(value, datetime.min.time())
                s = str(value).strip()
                if not s:
                    return None
                try:
                    return datetime.fromisoformat(s.replace("Z", "+00:00"))
                except Exception:
                    return None

            def _as_float(value: Any):
                if value is None:
                    return None
                if isinstance(value, (int, float)):
                    return float(value)
                s = str(value).strip()
                if not s:
                    return None
                try:
                    return float(s.replace(",", ""))
                except Exception:
                    return None

            def _pick_date_column(cols: List[str]) -> str:
                if not cols:
                    return ""
                for c in cols:
                    if any(kw in c.lower() for kw in ("created", "updated", "date", "time", "at", "æ—¥æœŸ", "æ—¶é—´")):
                        return c
                return cols[0]

            def _r_squared(values: List[float]) -> float:
                n = len(values)
                if n < 3:
                    return 0.0
                y_mean = sum(values) / n
                ss_tot = sum((y - y_mean) ** 2 for y in values)
                if ss_tot == 0:
                    return 1.0
                x_mean = (n - 1) / 2
                ss_xx = sum((i - x_mean) ** 2 for i in range(n))
                if ss_xx == 0:
                    return 0.0
                ss_xy = sum((i - x_mean) * (values[i] - y_mean) for i in range(n))
                slope = ss_xy / ss_xx
                intercept = y_mean - slope * x_mean
                predicted = [intercept + slope * i for i in range(n)]
                ss_res = sum((values[i] - predicted[i]) ** 2 for i in range(n))
                r2 = 1 - (ss_res / ss_tot)
                return max(0.0, min(1.0, float(r2)))

            widget_groups = aggregated_data.get("by_widget")
            best = None
            if widget_groups:
                for g in widget_groups:
                    data = g.get("data") or []
                    date_columns = g.get("date_columns") or []
                    numeric_columns = g.get("numeric_columns") or []
                    if not date_columns or not numeric_columns or len(data) < 2:
                        continue
                    date_col = _pick_date_column(date_columns)
                    prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")

                    for num_col in numeric_columns:
                        points = []
                        for row in data:
                            dt = _try_parse_datetime(row.get(date_col))
                            val = _as_float(row.get(num_col))
                            if dt is not None and val is not None:
                                points.append((dt, val))
                        if len(points) < 2:
                            continue
                        points.sort(key=lambda x: x[0])
                        first_dt, first_val = points[0]
                        last_dt, last_val = points[-1]
                        values = [p[1] for p in points]
                        delta = last_val - first_val
                        if first_val != 0:
                            rate = (delta / first_val) * 100
                            score = abs(rate)
                        else:
                            rate = None
                            score = abs(delta)
                        metric_name = f"{prefix}.{num_col}"
                        r2 = _r_squared(values)
                        candidate = {
                            "score": score,
                            "metric_name": metric_name,
                            "first_val": first_val,
                            "last_val": last_val,
                            "rate": rate,
                            "first_dt": first_dt,
                            "last_dt": last_dt,
                            "values": values,
                            "r_squared": r2,
                        }
                        if best is None or candidate["score"] > best["score"]:
                            best = candidate

            if best is None:
                date_columns = aggregated_data.get("date_columns", [])
                numeric_columns = aggregated_data.get("numeric_columns", [])
                data = aggregated_data.get("data", [])
                if not date_columns or not numeric_columns or len(data) < 2:
                    return None
                date_col = _pick_date_column(date_columns)
                for num_col in numeric_columns:
                    points = []
                    for row in data:
                        dt = _try_parse_datetime(row.get(date_col))
                        val = _as_float(row.get(num_col))
                        if dt is not None and val is not None:
                            points.append((dt, val))
                    if len(points) < 2:
                        continue
                    points.sort(key=lambda x: x[0])
                    first_dt, first_val = points[0]
                    last_dt, last_val = points[-1]
                    values = [p[1] for p in points]
                    delta = last_val - first_val
                    if first_val != 0:
                        rate = (delta / first_val) * 100
                        score = abs(rate)
                    else:
                        rate = None
                        score = abs(delta)
                    r2 = _r_squared(values)
                    candidate = {
                        "score": score,
                        "metric_name": num_col,
                        "first_val": first_val,
                        "last_val": last_val,
                        "rate": rate,
                        "first_dt": first_dt,
                        "last_dt": last_dt,
                        "values": values,
                        "r_squared": r2,
                    }
                    if best is None or candidate["score"] > best["score"]:
                        best = candidate

            if best is None:
                return None

            metric_name = best["metric_name"]
            first_val = best["first_val"]
            last_val = best["last_val"]
            rate = best["rate"]
            first_dt = best["first_dt"]
            last_dt = best["last_dt"]
            r2 = best["r_squared"]
            direction = "up" if last_val > first_val else ("down" if last_val < first_val else "stable")
            if rate is not None:
                rate = round(rate, 2)
                desc = f"{metric_name} ä» {first_val} å˜åŒ–åˆ° {last_val}ï¼ˆ{first_dt.date()}â†’{last_dt.date()}ï¼‰ï¼Œå˜åŒ–ç‡ {rate}%ï¼ˆRÂ²={r2:.2f}ï¼‰"
            else:
                desc = f"{metric_name} ä» {first_val} å˜åŒ–åˆ° {last_val}ï¼ˆ{first_dt.date()}â†’{last_dt.date()}ï¼‰ï¼Œå˜åŒ–é‡ {round(last_val - first_val, 2)}ï¼ˆRÂ²={r2:.2f}ï¼‰"

            aggregated_data["_trend_metadata"] = {
                "metric": metric_name,
                "r_squared": round(r2, 4),
                "values": best.get("values") or [],
                "point_count": len(best.get("values") or []),
            }

            return schemas.InsightTrend(
                trend_direction=direction,
                total_growth_rate=rate,
                description=desc
            )
        except Exception:
            pass
        
        return None
    
    def _detect_anomalies(self, aggregated_data: dict) -> List[schemas.InsightAnomaly]:
        """æ£€æµ‹æ•°æ®å¼‚å¸¸"""
        anomalies = []

        def _severity_score(s: Optional[str]) -> int:
            if s == "high":
                return 3
            if s == "medium":
                return 2
            return 1

        def _detect_for_series(metric_name: str, values: List[float]) -> List[schemas.InsightAnomaly]:
            if len(values) < 8:
                return []
            values_sorted = sorted(values)

            def _quantile(sorted_vals: List[float], q: float) -> float:
                n = len(sorted_vals)
                if n == 1:
                    return sorted_vals[0]
                pos = (n - 1) * q
                lo = int(pos)
                hi = min(lo + 1, n - 1)
                w = pos - lo
                return sorted_vals[lo] * (1 - w) + sorted_vals[hi] * w

            q1 = _quantile(values_sorted, 0.25)
            q3 = _quantile(values_sorted, 0.75)
            iqr = q3 - q1
            if iqr <= 0:
                return []

            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr

            max_val = values_sorted[-1]
            min_val = values_sorted[0]

            found = []
            if max_val > upper:
                exceed = (max_val - upper) / iqr
                severity = "high" if exceed >= 3 else ("medium" if exceed >= 1.5 else "low")
                found.append(schemas.InsightAnomaly(
                    type="outlier",
                    metric=metric_name,
                    description=f"{metric_name} å­˜åœ¨å¼‚å¸¸é«˜å€¼ {max_val}ï¼ˆä¸Šç•Œ {round(upper, 2)}ï¼‰",
                    severity=severity
                ))
            if min_val < lower:
                exceed = (lower - min_val) / iqr
                severity = "high" if exceed >= 3 else ("medium" if exceed >= 1.5 else "low")
                found.append(schemas.InsightAnomaly(
                    type="outlier",
                    metric=metric_name,
                    description=f"{metric_name} å­˜åœ¨å¼‚å¸¸ä½å€¼ {min_val}ï¼ˆä¸‹ç•Œ {round(lower, 2)}ï¼‰",
                    severity=severity
                ))
            return found

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        widget_groups = aggregated_data.get("by_widget")
        if widget_groups:
            for g in widget_groups:
                data = g.get("data") or []
                numeric_columns = g.get("numeric_columns") or []
                if not data or not numeric_columns:
                    continue
                prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")
                for col in numeric_columns[:3]:
                    vals = [_as_float(row.get(col)) for row in data]
                    vals = [v for v in vals if v is not None]
                    anomalies.extend(_detect_for_series(f"{prefix}.{col}", vals))
        else:
            data = aggregated_data.get("data", [])
            numeric_columns = aggregated_data.get("numeric_columns", [])
            if not data or not numeric_columns:
                return anomalies
            for col in numeric_columns[:3]:
                vals = [_as_float(row.get(col)) for row in data]
                vals = [v for v in vals if v is not None]
                anomalies.extend(_detect_for_series(col, vals))

        anomalies.sort(key=lambda a: _severity_score(a.severity), reverse=True)
        return anomalies[:5]
    
    def _find_correlations(self, aggregated_data: dict, relationship_context: Optional[dict]) -> List[schemas.InsightCorrelation]:
        """å‘ç°æ•°æ®å…³è”"""
        correlations = []
        
        # åŸºäºå›¾è°±å…³ç³»ç”Ÿæˆå…³è”æ´å¯Ÿ
        if relationship_context:
            direct_rels = relationship_context.get("direct_relationships", [])
            for rel in direct_rels[:3]:
                src_table = rel.get("source_table", "")
                tgt_table = rel.get("target_table", "")
                if src_table and tgt_table:
                    correlations.append(schemas.InsightCorrelation(
                        type="cross_table",
                        entities=[src_table, tgt_table],
                        description=f"{src_table} ä¸ {tgt_table} å­˜åœ¨å¤–é”®å…³è”",
                        strength=0.8
                    ))
        
        return correlations

    def _check_permission(self, db: Session, dashboard_id: int, user_id: int):
        has_permission = crud.crud_dashboard.check_permission(
            db, dashboard_id=dashboard_id, user_id=user_id, required_level="viewer"
        )
        if not has_permission:
            raise PermissionError("No permission to view this dashboard")

    def _aggregate_widget_data(
        self,
        widgets: List[DashboardWidget],
        conditions: Optional[schemas.InsightConditions]
    ) -> Dict[str, Any]:
        """èšåˆWidgetæ•°æ®"""
        from datetime import datetime, date

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        def _try_parse_datetime(value: Any):
            if value is None:
                return None
            if isinstance(value, datetime):
                return value
            if isinstance(value, date):
                return datetime.combine(value, datetime.min.time())
            s = str(value).strip()
            if not s:
                return None
            try:
                return datetime.fromisoformat(s.replace("Z", "+00:00"))
            except Exception:
                return None

        def _infer_columns(rows: List[Dict[str, Any]]) -> Dict[str, List[str]]:
            numeric = set()
            dates = set()
            if not rows:
                return {"numeric": [], "dates": []}

            sample = rows[: min(20, len(rows))]
            keys = set()
            for r in sample:
                if isinstance(r, dict):
                    keys.update(r.keys())

            for k in keys:
                k_lower = str(k).lower()
                if any(keyword in k_lower for keyword in ("date", "time", "created", "updated", "at", "æ—¥æœŸ", "æ—¶é—´")):
                    for r in sample:
                        dt = _try_parse_datetime(r.get(k)) if isinstance(r, dict) else None
                        if dt is not None:
                            dates.add(k)
                            break

                for r in sample:
                    if not isinstance(r, dict):
                        continue
                    v = r.get(k)
                    fv = _as_float(v)
                    if fv is not None:
                        numeric.add(k)
                        break

            return {"numeric": list(numeric), "dates": list(dates)}

        aggregated_rows = []
        table_names = set()
        numeric_columns = set()
        date_columns = set()
        widget_summaries = []
        by_widget = []
        
        for widget in widgets:
            # æå–widgetæ•°æ®
            if not widget.data_cache or "data" not in widget.data_cache:
                continue
            
            data = widget.data_cache["data"]
            if not data or not isinstance(data, list):
                continue
            
            # åº”ç”¨æ¡ä»¶è¿‡æ»¤
            filtered_data = self._apply_conditions(data, conditions)

            table_name = None
            if widget.query_config and isinstance(widget.query_config, dict):
                table_name = widget.query_config.get("table_name")

            inferred = _infer_columns(filtered_data)
            widget_numeric_columns = inferred["numeric"]
            widget_date_columns = inferred["dates"]

            by_widget.append({
                "widget_id": widget.id,
                "title": getattr(widget, "title", None),
                "widget_type": getattr(widget, "widget_type", None),
                "table_name": table_name,
                "row_count": len(filtered_data),
                "numeric_columns": widget_numeric_columns,
                "date_columns": widget_date_columns,
                "data": filtered_data,
            })
            
            aggregated_rows.extend(filtered_data)
            
            # æå–è¡¨å
            if table_name:
                table_names.add(table_name)
            
            # æå–åˆ—ä¿¡æ¯
            for c in widget_numeric_columns:
                numeric_columns.add(c)
            for c in widget_date_columns:
                date_columns.add(c)
            
            widget_summaries.append({
                "id": widget.id,
                "type": widget.widget_type,
                "title": widget.title,
                "row_count": len(filtered_data)
            })
        
        return {
            "data": aggregated_rows,
            "total_rows": len(aggregated_rows),
            "table_names": list(table_names),
            "numeric_columns": list(numeric_columns),
            "date_columns": list(date_columns),
            "by_widget": by_widget,
            "widget_summaries": widget_summaries
        }

    def _refresh_data_widgets(self, db: Session, widgets: List[DashboardWidget], user_id: int) -> None:
        from app.services.dashboard_widget_service import dashboard_widget_service

        for w in widgets:
            try:
                dashboard_widget_service.refresh_widget(db, widget_id=w.id, user_id=user_id)
            except Exception:
                logger.exception("åˆ·æ–°æ•°æ®ç»„ä»¶å¤±è´¥: widget_id=%s", w.id)
    
    def _apply_conditions(
        self,
        data: List[Dict[str, Any]],
        conditions: Optional[schemas.InsightConditions]
    ) -> List[Dict[str, Any]]:
        """åº”ç”¨æŸ¥è¯¢æ¡ä»¶è¿‡æ»¤æ•°æ®"""
        if not conditions:
            return data
        
        filtered_data = data.copy()
        
        def _try_parse_datetime(value: Any):
            if value is None:
                return None
            from datetime import datetime, date
            if isinstance(value, datetime):
                return value
            if isinstance(value, date):
                return datetime.combine(value, datetime.min.time())
            s = str(value).strip()
            if not s:
                return None
            try:
                return datetime.fromisoformat(s.replace("Z", "+00:00"))
            except Exception:
                pass
            for fmt in (
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y/%m/%dT%H:%M:%S",
            ):
                try:
                    return datetime.strptime(s, fmt)
                except Exception:
                    continue
            return None

        def _calc_relative_range(relative_range: str):
            from datetime import datetime, timedelta
            now = datetime.utcnow()
            key = (relative_range or "").strip().lower()
            if not key:
                return None, None
            if key in {"last_7_days", "7d", "last7days"}:
                return now - timedelta(days=7), now
            if key in {"last_30_days", "30d", "last30days"}:
                return now - timedelta(days=30), now
            if key in {"last_90_days", "90d", "last90days"}:
                return now - timedelta(days=90), now
            if key in {"this_month", "month_to_date", "mtd"}:
                start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                return start, now
            if key in {"this_year", "year_to_date", "ytd"}:
                start = now.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
                return start, now
            return None, None

        def _select_date_column(rows: List[Dict[str, Any]]) -> Optional[str]:
            if not rows:
                return None
            sample = rows[:50]
            keys = list(sample[0].keys())
            keyword_keys = [
                k for k in keys
                if any(kw in k.lower() for kw in ("date", "time", "created", "updated", "at", "æ—¥æœŸ", "æ—¶é—´"))
            ]
            candidates = keyword_keys + [k for k in keys if k not in keyword_keys]
            best_key = None
            best_ratio = 0.0
            for k in candidates:
                parsed = 0
                seen = 0
                for r in sample:
                    if k not in r:
                        continue
                    seen += 1
                    if _try_parse_datetime(r.get(k)) is not None:
                        parsed += 1
                if seen == 0:
                    continue
                ratio = parsed / seen
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_key = k
            if best_ratio >= 0.6:
                return best_key
            return None

        def _coerce_number(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        def _values_match(row_val: Any, expected: Any) -> bool:
            if row_val is None and expected is None:
                return True
            if row_val is None or expected is None:
                return False
            if isinstance(expected, str) or isinstance(row_val, str):
                row_num = _coerce_number(row_val)
                exp_num = _coerce_number(expected)
                if row_num is not None and exp_num is not None:
                    return row_num == exp_num
                return str(row_val).strip() == str(expected).strip()
            return row_val == expected
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if conditions.time_range:
            date_column = _select_date_column(filtered_data)
            
            if date_column:
                start_dt = _try_parse_datetime(conditions.time_range.start) if conditions.time_range.start else None
                end_dt = _try_parse_datetime(conditions.time_range.end) if conditions.time_range.end else None
                if (start_dt is None and end_dt is None) and getattr(conditions.time_range, "relative_range", None):
                    start_dt, end_dt = _calc_relative_range(conditions.time_range.relative_range)
                
                if start_dt or end_dt:
                    def _in_range(row: Dict[str, Any]) -> bool:
                        row_dt = _try_parse_datetime(row.get(date_column))
                        if row_dt is None:
                            return False
                        if start_dt and row_dt < start_dt:
                            return False
                        if end_dt and row_dt > end_dt:
                            return False
                        return True
                    
                    filtered_data = [row for row in filtered_data if _in_range(row)]
        
        # ç»´åº¦ç­›é€‰
        if conditions.dimension_filters:
            for column, value in conditions.dimension_filters.items():
                if isinstance(value, (list, tuple, set)):
                    allowed = list(value)
                    filtered_data = [
                        row for row in filtered_data
                        if any(_values_match(row.get(column), v) for v in allowed)
                    ]
                else:
                    filtered_data = [row for row in filtered_data if _values_match(row.get(column), value)]
        
        return filtered_data
    
    def _create_or_update_insight_widget(
        self,
        db: Session,
        dashboard_id: int,
        insights: schemas.InsightResult,
        conditions: Optional[schemas.InsightConditions],
        use_graph_relationships: bool,
        analyzed_widget_count: int,
        status: str = "completed",
        lineage: Optional[Dict[str, Any]] = None
    ) -> int:
        """åˆ›å»ºæˆ–æ›´æ–°æ´å¯ŸWidgetï¼Œä¿å­˜æº¯æºä¿¡æ¯"""
        existing_widgets = crud.crud_dashboard_widget.get_by_dashboard(db, dashboard_id=dashboard_id)
        
        insight_widget = None
        # P0-FIX: ä»ç°æœ‰æ•°æ®Widgetä¸­è·å–connection_idï¼Œé¿å…ç¡¬ç¼–ç 
        default_connection_id = None
        for widget in existing_widgets:
            if widget.widget_type == "insight_analysis":
                insight_widget = widget
            elif default_connection_id is None and widget.connection_id:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®Widgetçš„connection_idä½œä¸ºé»˜è®¤å€¼
                default_connection_id = widget.connection_id
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®Widgetï¼Œå°è¯•ä»Dashboardå…³è”çš„connectionè·å–
        if default_connection_id is None:
            dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
            if dashboard and dashboard.widgets:
                for w in dashboard.widgets:
                    if w.widget_type != "insight_analysis" and w.connection_id:
                        default_connection_id = w.connection_id
                        break
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ° connection_idï¼Œè®°å½•è­¦å‘Šï¼ˆä¸å†ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼ï¼‰
        if default_connection_id is None:
            logger.warning(f"Dashboard {dashboard_id} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ connection_idï¼Œæ´å¯Ÿåˆ†æå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        
        # P0: å°†æº¯æºä¿¡æ¯åˆå¹¶åˆ° query_config
        query_config = {
            "analysis_scope": "all_widgets",
            "analysis_dimensions": ["summary", "trends", "correlations", "recommendations"],
            "refresh_strategy": "manual",
            "last_analysis_at": datetime.utcnow().isoformat(),
            "use_graph_relationships": use_graph_relationships,
            "analyzed_widget_count": analyzed_widget_count,
            "status": status,
        }
        
        if conditions:
            query_config["current_conditions"] = conditions.dict(exclude_none=True)
        
        # P0: ä¿å­˜æº¯æºä¿¡æ¯
        if lineage:
            query_config["source_tables"] = lineage.get("source_tables", [])
            query_config["generated_sql"] = lineage.get("generated_sql")
            query_config["user_intent"] = lineage.get("sql_generation_trace", {}).get("user_intent")
            query_config["few_shot_samples_count"] = lineage.get("sql_generation_trace", {}).get("few_shot_samples_count", 0)
            query_config["generation_method"] = lineage.get("sql_generation_trace", {}).get("generation_method", "standard")
            query_config["execution_time_ms"] = lineage.get("execution_metadata", {}).get("execution_time_ms", 0)
            query_config["from_cache"] = lineage.get("execution_metadata", {}).get("from_cache", False)
            query_config["row_count"] = lineage.get("execution_metadata", {}).get("row_count", 0)
            query_config["db_type"] = lineage.get("execution_metadata", {}).get("db_type")
            query_config["data_transformations"] = lineage.get("data_transformations", [])
            query_config["confidence_score"] = lineage.get("confidence_score", 0.8)
            query_config["analysis_method"] = lineage.get("analysis_method", "auto")
        
        data_cache = insights.dict(exclude_none=True)
        
        if insight_widget:
            crud.crud_dashboard_widget.update(
                db,
                db_obj=insight_widget,
                obj_in=schemas.WidgetUpdate(title="çœ‹æ¿æ´å¯Ÿåˆ†æ")
            )
            insight_widget.query_config = query_config
            insight_widget.data_cache = data_cache
            insight_widget.last_refresh_at = datetime.utcnow()
            db.commit()
            db.refresh(insight_widget)
            return insight_widget.id
        else:
            widget_create = schemas.WidgetCreate(
                widget_type="insight_analysis",
                title="çœ‹æ¿æ´å¯Ÿåˆ†æ",
                connection_id=default_connection_id,  # P0-FIX: ä½¿ç”¨åŠ¨æ€è·å–çš„connection_id
                query_config=query_config,
                chart_config=None,
                position_config={"x": 0, "y": 0, "w": 12, "h": 6},
                refresh_interval=0
            )
            
            new_widget = crud.crud_dashboard_widget.create_widget(
                db,
                dashboard_id=dashboard_id,
                obj_in=widget_create
            )
            new_widget.data_cache = data_cache
            db.commit()
            db.refresh(new_widget)
            return new_widget.id

    def _update_insight_widget_result(
        self,
        db: Session,
        widget_id: int,
        insights: schemas.InsightResult,
        count: int,
        status: str,
        analysis_method: Optional[str] = None,
        confidence_score: Optional[float] = None,
        relationship_count: Optional[int] = None,
        source_tables: Optional[List[str]] = None,
        extra_metrics: Optional[Dict[str, Any]] = None,
    ):
        """æ›´æ–°æ´å¯Ÿ Widget çš„åˆ†æç»“æœ"""
        try:
            widget = crud.crud_dashboard_widget.get(db, id=widget_id)
            if widget:
                query_config = widget.query_config or {}
                query_config["status"] = status
                query_config["analyzed_widget_count"] = count
                query_config["last_analysis_at"] = datetime.utcnow().isoformat()
                if analysis_method is not None:
                    query_config["analysis_method"] = analysis_method
                if confidence_score is not None:
                    query_config["confidence_score"] = confidence_score
                if relationship_count is not None:
                    query_config["relationship_count"] = relationship_count
                if source_tables is not None:
                    query_config["source_tables"] = source_tables
                if extra_metrics is not None:
                    query_config["trend_metrics"] = extra_metrics
                
                widget.query_config = query_config
                widget.data_cache = insights.dict(exclude_none=True)
                widget.last_refresh_at = datetime.utcnow()
                db.commit()
        except Exception as e:
            db.rollback()
            raise

    def _update_widget_status(self, db: Session, widget_id: int, status: str, error: str = None):
        """æ›´æ–° Widget çŠ¶æ€"""
        try:
            widget = crud.crud_dashboard_widget.get(db, id=widget_id)
            if widget:
                query_config = widget.query_config or {}
                query_config["status"] = status
                query_config["last_updated_at"] = datetime.utcnow().isoformat()
                if error:
                    # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦ï¼Œé¿å…å­˜å‚¨è¿‡å¤§
                    query_config["error"] = str(error)[:1000]
                widget.query_config = query_config
                db.commit()
        except Exception as e:
            db.rollback()
            raise


# åˆ›å»ºå…¨å±€å®ä¾‹
dashboard_insight_service = DashboardInsightService()
