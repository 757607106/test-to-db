"""
Dashboardæ´å¯Ÿåˆ†ææœåŠ¡
è´Ÿè´£æ•°æ®èšåˆã€æ¡ä»¶åº”ç”¨ã€æ´å¯Ÿç”Ÿæˆç¼–æ’
ä¼˜åŒ–ï¼šæ”¯æŒå¼‚æ­¥åå°å¤„ç†
"""
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
from sqlalchemy.orm import Session

from app import crud, schemas
from app.models.dashboard_widget import DashboardWidget
from app.services.graph_relationship_service import graph_relationship_service
from app.db.session import SessionLocal
from app.services.text2sql_utils import retrieve_relevant_schema, format_schema_for_prompt
from app.core.agent_config import get_agent_llm, CORE_AGENT_SQL_GENERATOR
from langchain_core.messages import SystemMessage, HumanMessage

class DashboardInsightService:
    """Dashboardæ´å¯Ÿåˆ†ææœåŠ¡"""
    
    def _build_table_column_whitelist(self, schema_context: dict) -> tuple[str, set, dict]:
        """
        æ„å»ºè¡¨/åˆ—ç™½åå•ï¼Œé˜²æ­¢ LLM å¹»è§‰
        
        Returns:
            whitelist_str: æ ¼å¼åŒ–çš„ç™½åå•å­—ç¬¦ä¸²
            valid_tables: æœ‰æ•ˆè¡¨åé›†åˆ
            valid_columns: {table_name: [column_names]} å­—å…¸
        """
        valid_tables = set()
        valid_columns = {}  # {table_name: [column_names]}
        
        tables = schema_context.get("tables", [])
        columns = schema_context.get("columns", [])
        relationships = schema_context.get("relationships", [])
        
        # æ„å»ºè¡¨åé›†åˆ
        for t in tables:
            table_name = t.get("name", "")
            if table_name:
                valid_tables.add(table_name)
                valid_columns[table_name] = []
        
        # æ„å»ºåˆ—åæ˜ å°„
        for c in columns:
            table_name = c.get("table_name", "")
            col_name = c.get("name", "")
            if table_name and col_name:
                if table_name not in valid_columns:
                    valid_columns[table_name] = []
                valid_columns[table_name].append(col_name)
        
        # æ„å»ºç™½åå•å­—ç¬¦ä¸²
        whitelist_parts = []
        whitelist_parts.append("=" * 60)
        whitelist_parts.append("ã€é‡è¦ã€‘å¯ç”¨è¡¨å’Œå­—æ®µç™½åå•ï¼ˆä»…å…è®¸ä½¿ç”¨ä»¥ä¸‹è¡¨å’Œå­—æ®µï¼‰")
        whitelist_parts.append("=" * 60)
        
        for table_name in sorted(valid_tables):
            cols = valid_columns.get(table_name, [])
            # æ‰¾åˆ°è¡¨çš„æè¿°
            table_desc = ""
            for t in tables:
                if t.get("name") == table_name:
                    table_desc = t.get("description", "")
                    break
            
            whitelist_parts.append(f"\nè¡¨å: {table_name}")
            if table_desc:
                whitelist_parts.append(f"  æè¿°: {table_desc}")
            whitelist_parts.append(f"  å¯ç”¨å­—æ®µ: {', '.join(cols)}")
        
        # æ·»åŠ å…³ç³»ä¿¡æ¯
        if relationships:
            whitelist_parts.append("\n" + "-" * 40)
            whitelist_parts.append("è¡¨é—´å…³ç³»ï¼ˆJOIN æ—¶å¿…é¡»ä½¿ç”¨è¿™äº›å…³è”å­—æ®µï¼‰:")
            # éå†æ‰€æœ‰å…³ç³»ï¼Œä¸é™åˆ¶æ•°é‡ä»¥ç¡®ä¿ JOIN å‡†ç¡®æ€§
            for rel in relationships:
                src = f"{rel.get('source_table', '')}.{rel.get('source_column', '')}"
                tgt = f"{rel.get('target_table', '')}.{rel.get('target_column', '')}"
                whitelist_parts.append(f"  - {src} -> {tgt}")
        
        whitelist_parts.append("\n" + "=" * 60)
        whitelist_parts.append("ã€è­¦å‘Šã€‘ä¸¥ç¦ä½¿ç”¨ä¸Šè¿°ç™½åå•ä¹‹å¤–çš„ä»»ä½•è¡¨æˆ–å­—æ®µï¼")
        whitelist_parts.append("=" * 60)
        
        return "\n".join(whitelist_parts), valid_tables, valid_columns
    
    def _validate_sql_against_whitelist(
        self, 
        sql: str, 
        valid_tables: set, 
        valid_columns: dict,
        db_type: str = "MYSQL"
    ) -> tuple[bool, str, list]:
        """
        éªŒè¯ SQL æ˜¯å¦åªä½¿ç”¨äº†ç™½åå•ä¸­çš„è¡¨å’Œåˆ—
        
        Returns:
            is_valid: æ˜¯å¦æœ‰æ•ˆ
            error_msg: é”™è¯¯ä¿¡æ¯
            invalid_refs: æ— æ•ˆå¼•ç”¨åˆ—è¡¨
        """
        import re
        
        sql_upper = sql.upper()
        invalid_refs = []
        
        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ SELECT è¯­å¥ï¼ˆæ”¯æŒ WITH CTEï¼‰
        sql_stripped = sql_upper.strip()
        if not (sql_stripped.startswith("SELECT") or sql_stripped.startswith("WITH")):
            return False, "SQL å¿…é¡»æ˜¯ SELECT è¯­å¥æˆ– WITH CTE", ["non-select"]
        
        # 2. æ£€æŸ¥å±é™©å…³é”®è¯
        dangerous_keywords = ["DROP", "DELETE", "TRUNCATE", "UPDATE", "INSERT", "ALTER", "CREATE"]
        for kw in dangerous_keywords:
            if kw in sql_upper and "SELECT" not in sql_upper[:20]:
                return False, f"æ£€æµ‹åˆ°å±é™©æ“ä½œ: {kw}", [kw]
        
        # 3. æå– SQL ä¸­çš„è¡¨å
        # åŒ¹é… FROM/JOIN åçš„è¡¨å
        table_pattern = r'(?:FROM|JOIN)\s+[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_tables = re.findall(table_pattern, sql, re.IGNORECASE)
        
        # æ£€æŸ¥è¡¨åæ˜¯å¦åœ¨ç™½åå•ä¸­
        valid_tables_lower = {t.lower() for t in valid_tables}
        for table in found_tables:
            if table.lower() not in valid_tables_lower:
                invalid_refs.append(f"è¡¨ '{table}' ä¸åœ¨ç™½åå•ä¸­")
        
        # 4. æå–å¹¶æ£€æŸ¥åˆ—åï¼ˆç®€åŒ–æ£€æŸ¥ï¼Œåªæ£€æŸ¥ table.column æ ¼å¼ï¼‰
        # åŒ¹é… table.column æˆ– alias.column æ ¼å¼
        col_pattern = r'([a-zA-Z_][a-zA-Z0-9_]*)\s*\.\s*[`"\[]?([a-zA-Z_][a-zA-Z0-9_]*)[`"\]]?'
        found_cols = re.findall(col_pattern, sql, re.IGNORECASE)
        
        # æ„å»ºæ‰€æœ‰æœ‰æ•ˆåˆ—åçš„å°å†™é›†åˆ
        all_valid_cols_lower = set()
        for cols in valid_columns.values():
            for col in cols:
                all_valid_cols_lower.add(col.lower())
        
        # æ£€æŸ¥åˆ—åï¼ˆå®¹å¿ä¸€äº›å¸¸è§çš„åˆ«åï¼‰
        common_aliases = {'t', 't1', 't2', 'a', 'b', 'c', 's', 'm', 'o', 'p', 'd', 'main', 'sub'}
        for table_or_alias, col in found_cols:
            # å¦‚æœæ˜¯å¸¸è§åˆ«åï¼Œåªæ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
            if table_or_alias.lower() in common_aliases:
                if col.lower() not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{col}' ä¸åœ¨ç™½åå•ä¸­")
            else:
                # æ£€æŸ¥è¡¨åå’Œåˆ—å
                table_lower = table_or_alias.lower()
                col_lower = col.lower()
                
                # åœ¨æ‰€æœ‰è¡¨ä¸­æŸ¥æ‰¾è¯¥åˆ—
                col_found = False
                for t_name, t_cols in valid_columns.items():
                    if col_lower in [c.lower() for c in t_cols]:
                        col_found = True
                        break
                
                if not col_found and col_lower not in all_valid_cols_lower:
                    invalid_refs.append(f"åˆ— '{table_or_alias}.{col}' ä¸åœ¨ç™½åå•ä¸­")
        
        if invalid_refs:
            return False, f"å‘ç° {len(invalid_refs)} ä¸ªæ— æ•ˆå¼•ç”¨", invalid_refs
        
        return True, "", []
    
    async def generate_mining_suggestions(self, db: Session, request: schemas.MiningRequest) -> schemas.MiningResponse:
        """ç”Ÿæˆæ™ºèƒ½æŒ–æ˜å»ºè®®ï¼ˆä¼˜åŒ–ç‰ˆï¼šé˜²å¹»è§‰ + SQL éªŒè¯ï¼‰"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"[Mining] å¼€å§‹ç”ŸæˆæŒ–æ˜å»ºè®®, connection_id={request.connection_id}, intent={request.intent}")
        
        # 0. è·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
        from app.models.db_connection import DBConnection
        connection = db.query(DBConnection).filter(DBConnection.id == request.connection_id).first()
        db_type = connection.db_type.upper() if connection else "MYSQL"
        logger.info(f"[Mining] æ•°æ®åº“ç±»å‹: {db_type}")
        
        # 1. è·å–ä¸Šä¸‹æ–‡
        if request.intent:
            schema_context = retrieve_relevant_schema(db, request.connection_id, request.intent)
        else:
            tables = crud.schema_table.get_by_connection(db=db, connection_id=request.connection_id)
            
            if not tables:
                logger.warning(f"[Mining] æœªæ‰¾åˆ°è¡¨, connection_id={request.connection_id}")
                return schemas.MiningResponse(suggestions=[])
            
            logger.info(f"[Mining] æ‰¾åˆ° {len(tables)} ä¸ªè¡¨")
            
            tables_list = []
            columns_list = []
            table_names = []
            
            # éå†æ‰€æœ‰è¡¨ï¼Œä¸é™åˆ¶æ•°é‡ä»¥ç¡®ä¿ SQL ç”Ÿæˆå‡†ç¡®æ€§
            for table in tables:
                table_names.append(table.table_name)
                tables_list.append({
                    "id": table.id,
                    "name": table.table_name,
                    "description": table.description or ""
                })
                
                columns = crud.schema_column.get_by_table(db=db, table_id=table.id)
                for col in columns:
                    columns_list.append({
                        "id": col.id,
                        "name": col.column_name,
                        "type": col.data_type,
                        "description": col.description or "",
                        "is_primary_key": col.is_primary_key,
                        "is_foreign_key": col.is_foreign_key,
                        "table_id": table.id,
                        "table_name": table.table_name
                    })
            
            # è·å–è¡¨ä¹‹é—´çš„å…³ç³»
            relationships = []
            try:
                relationship_context = graph_relationship_service.query_table_relationships(
                    connection_id=request.connection_id,
                    table_names=table_names
                )
                if relationship_context.get("direct_relationships"):
                    for rel in relationship_context["direct_relationships"]:
                        relationships.append({
                            "source_table": rel.get("source_table"),
                            "source_column": rel.get("source_column"),
                            "target_table": rel.get("target_table"),
                            "target_column": rel.get("target_column"),
                            "relationship_type": rel.get("relationship_type", "references")
                        })
                    logger.info(f"[Mining] æ‰¾åˆ° {len(relationships)} ä¸ªè¡¨é—´å…³ç³»")
            except Exception as e:
                logger.warning(f"[Mining] è·å–è¡¨å…³ç³»å¤±è´¥: {e}")
            
            schema_context = {
                "tables": tables_list,
                "columns": columns_list,
                "relationships": relationships
            }
        
        if not schema_context.get("tables"):
            logger.warning("[Mining] schema_context ä¸­æ— è¡¨")
            return schemas.MiningResponse(suggestions=[])
        
        # 2. æ„å»ºè¡¨/åˆ—ç™½åå•ï¼ˆé˜²å¹»è§‰æ ¸å¿ƒï¼‰
        whitelist_str, valid_tables, valid_columns = self._build_table_column_whitelist(schema_context)
        logger.info(f"[Mining] ç™½åå•åŒ…å« {len(valid_tables)} ä¸ªè¡¨, å…± {sum(len(cols) for cols in valid_columns.values())} ä¸ªå­—æ®µ")
        
        # 3. æ ¼å¼åŒ– Schema
        schema_str = format_schema_for_prompt(schema_context)
        
        # 3. æ„å»º Promptï¼ˆè¦æ±‚è¿”å› JSON æ ¼å¼ï¼‰
        # æ ¹æ®æ•°æ®åº“ç±»å‹æä¾› SQL è¯­æ³•æŒ‡å—
        sql_syntax_guides = {
            "MYSQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMySQLï¼‰ï¼š
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOINï¼Œè¯·ä½¿ç”¨ LEFT JOIN æˆ– RIGHT JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—
- å¸ƒå°”å€¼ä½¿ç”¨ 1/0 æˆ– TRUE/FALSE""",
            "POSTGRESQL": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆPostgreSQLï¼‰ï¼š
- å¯ä½¿ç”¨ LIMIT æˆ– FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- æ”¯æŒ ARRAY ç±»å‹å’Œ JSON æ“ä½œ""",
            "SQLITE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQLiteï¼‰ï¼š
- ä½¿ç”¨ LIMITï¼Œä¸æ”¯æŒ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ strftime()
- ä¸æ”¯æŒ FULL OUTER JOIN å’Œ RIGHT JOIN
- ä½¿ç”¨åŒå¼•å· " æˆ–æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ç±»å‹ç³»ç»Ÿçµæ´»ï¼Œæ— ä¸¥æ ¼ç±»å‹æ£€æŸ¥""",
            "SQLSERVER": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆSQL Server / MSSQLï¼‰ï¼š
- ä½¿ç”¨ TOP N æˆ– OFFSET...FETCH
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ + æ“ä½œç¬¦æˆ– CONCAT()
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ FORMAT() æˆ– CONVERT()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨æ–¹æ‹¬å· [] åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ GETDATE() è·å–å½“å‰æ—¶é—´""",
            "ORACLE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆOracleï¼‰ï¼š
- ä½¿ç”¨ ROWNUM æˆ– FETCH FIRSTï¼ˆ12c+ï¼‰
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ || æ“ä½œç¬¦
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ TO_CHAR()
- æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åŒå¼•å· " åŒ…è£¹ä¿ç•™å­—
- ä½¿ç”¨ SYSDATE è·å–å½“å‰æ—¶é—´
- FROM å­å¥å¿…é¡»æœ‰è¡¨ï¼ˆå¯ç”¨ DUALï¼‰""",
            "MARIADB": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆMariaDBï¼‰ï¼š
- è¯­æ³•ä¸ MySQL åŸºæœ¬å…¼å®¹
- ä½¿ç”¨ LIMIT è€Œä¸æ˜¯ FETCH FIRST
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ CONCAT() å‡½æ•°
- æ—¥æœŸæ ¼å¼åŒ–ä½¿ç”¨ DATE_FORMAT()
- ä¸æ”¯æŒ FULL OUTER JOIN
- ä½¿ç”¨åå¼•å· ` åŒ…è£¹ä¿ç•™å­—""",
            "CLICKHOUSE": """
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆClickHouseï¼‰ï¼š
- ä½¿ç”¨ LIMIT è¿›è¡Œåˆ†é¡µ
- å­—ç¬¦ä¸²è¿æ¥ä½¿ç”¨ concat() æˆ– ||
- æ—¥æœŸå‡½æ•°ä½¿ç”¨ formatDateTime()
- æ”¯æŒ FULL OUTER JOIN (éƒ¨åˆ†ç‰ˆæœ¬)
- åŒºåˆ†å¤§å°å†™ï¼Œä½¿ç”¨åŒå¼•å·åŒ…è£¹
- ä¸“ä¸º OLAP ä¼˜åŒ–ï¼ŒèšåˆæŸ¥è¯¢æ€§èƒ½ä¼˜å¼‚""",
        }
        
        db_type_upper = db_type.upper()
        # å°è¯•åŒ¹é…æ•°æ®åº“ç±»å‹ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…
        sql_syntax_guide = ""
        for key, guide in sql_syntax_guides.items():
            if key in db_type_upper or db_type_upper in key:
                sql_syntax_guide = guide
                break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œæä¾›é€šç”¨æŒ‡å—
        if not sql_syntax_guide:
            sql_syntax_guide = f"""
SQL è¯­æ³•æ³¨æ„äº‹é¡¹ï¼ˆ{db_type}ï¼‰ï¼š
- è¯·ä½¿ç”¨æ ‡å‡† ANSI SQL è¯­æ³•
- é¿å…ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„æ‰©å±•è¯­æ³•
- ä½¿ç”¨é€šç”¨çš„èšåˆå‡½æ•°ï¼ˆSUM, COUNT, AVG, MAX, MINï¼‰
- ä½¿ç”¨æ ‡å‡†çš„ JOIN è¯­æ³•ï¼ˆINNER JOIN, LEFT JOINï¼‰
- æ—¥æœŸå‡½æ•°è¯·æ ¹æ®å®é™…æ•°æ®åº“è°ƒæ•´"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æ•°æ®åº“ç»“æ„ï¼Œæ¨è {request.limit} ä¸ªæœ‰ä»·å€¼çš„æ•°æ®åˆ†æè§†è§’ï¼ˆå›¾è¡¨ï¼‰ã€‚

ç›®æ ‡æ•°æ®åº“ç±»å‹ï¼š{db_type}
{sql_syntax_guide}

ç”¨æˆ·æ„å›¾ï¼š{request.intent or "è‡ªåŠ¨å‘ç°å…³é”®ä¸šåŠ¡æŒ‡æ ‡å’Œè¶‹åŠ¿"}

{whitelist_str}

æ•°æ®åº“ç»“æ„è¯¦æƒ…ï¼š
{schema_str}

æŒ–æ˜ç»´åº¦è¦æ±‚ï¼ˆè¯·è¦†ç›–å¤šä¸ªç»´åº¦ï¼‰ï¼š
- businessï¼ˆä¸šåŠ¡æ•°æ®ï¼‰ï¼šæ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡ã€KPI
- metricï¼ˆæŒ‡æ ‡åˆ†æï¼‰ï¼šå…³é”®æ•°å€¼çš„ç»Ÿè®¡åˆ†å¸ƒ
- trendï¼ˆè¶‹åŠ¿åˆ†æï¼‰ï¼šæ—¶é—´åºåˆ—å˜åŒ–
- semanticï¼ˆè¯­ä¹‰å…³è”ï¼‰ï¼šåŸºäºå­—æ®µè¯­ä¹‰å‘ç°çš„å…³è”åˆ†æ

ã€æ ¸å¿ƒçº¦æŸ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘ï¼š
1. SQL ä¸­çš„è¡¨åå’Œåˆ—åå¿…é¡»ä¸¥æ ¼åŒ¹é…ä¸Šè¿°ç™½åå•ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•ç™½åå•ä¹‹å¤–çš„è¡¨æˆ–å­—æ®µ
2. JOIN æ—¶å¿…é¡»ä½¿ç”¨ç™½åå•ä¸­æŒ‡å®šçš„å…³è”å­—æ®µï¼Œä¸å¾—è‡ªè¡Œæ¨æµ‹
3. æ¨èçš„ SQL å¿…é¡»æ˜¯åˆæ³•çš„ {db_type} SELECT è¯­å¥
4. å›¾è¡¨ç±»å‹ä»ä»¥ä¸‹é€‰æ‹©ï¼šbar, line, pie, scatter, table
5. æ¯ä¸ªæ¨èéƒ½è¦æœ‰æ˜ç¡®çš„ä¸šåŠ¡ä»·å€¼å’Œæ¨èç†ç”±
6. SQL å°½é‡åŒ…å«èšåˆåˆ†æï¼ˆSUM, COUNT, AVG, GROUP BYï¼‰
7. ä¸¥æ ¼éµå¾ª {db_type} çš„ SQL è¯­æ³•è§„èŒƒ

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{{{
  "suggestions": [
    {{{{
      "title": "å›¾è¡¨æ ‡é¢˜",
      "description": "ç®€çŸ­æè¿°ï¼ˆä¸€å¥è¯ï¼‰",
      "reasoning": "è¯¦ç»†æ¨èç†ç”±ï¼šä¸ºä»€ä¹ˆè¿™ä¸ªåˆ†æå¯¹ä¸šåŠ¡æœ‰ä»·å€¼ï¼Œæ•°æ®é€»è¾‘æ˜¯ä»€ä¹ˆ",
      "mining_dimension": "business|metric|trend|semantic",
      "confidence": 0.85,
      "chart_type": "bar|line|pie|scatter|table",
      "sql": "SELECT ...",
      "source_tables": ["è¡¨å1", "è¡¨å2"],
      "key_fields": ["å…³é”®å­—æ®µ1", "å…³é”®å­—æ®µ2"],
      "business_value": "è¿™ä¸ªåˆ†æèƒ½å¸®åŠ©ä¸šåŠ¡åšä»€ä¹ˆå†³ç­–",
      "suggested_actions": ["å»ºè®®åŠ¨ä½œ1", "å»ºè®®åŠ¨ä½œ2"],
      "analysis_intent": "åˆ†ææ„å›¾æè¿°"
    }}}}
  ]
}}}}

åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""
        
        # 4. è°ƒç”¨ LLMï¼ˆä½¿ç”¨ LLMWrapper ç»Ÿä¸€å¤„ç†é‡è¯•å’Œè¶…æ—¶ï¼‰
        try:
            import json
            from app.core.llm_wrapper import LLMWrapper, LLMWrapperConfig
            from app.core.llms import get_default_model
            from app.models.agent_profile import AgentProfile
            from app.models.llm_config import LLMConfiguration
            from app.core.config import settings
            
            # è·å– Agent é…ç½®
            profile = db.query(AgentProfile).filter(AgentProfile.name == CORE_AGENT_SQL_GENERATOR).first()
            
            # è·å– LLM é…ç½®
            llm_config = None
            if profile and profile.llm_config_id:
                llm_config = db.query(LLMConfiguration).filter(
                    LLMConfiguration.id == profile.llm_config_id,
                    LLMConfiguration.is_active == True
                ).first()
            
            # ä½¿ç”¨ LLMWrapperï¼ˆç»Ÿä¸€é‡è¯•ç­–ç•¥ï¼Œæ— è¶…æ—¶é™åˆ¶ï¼‰
            llm = get_default_model(config_override=llm_config, caller="dashboard_mining")
            wrapper_config = LLMWrapperConfig(
                max_retries=3,
                retry_base_delay=2.0,
                enable_tracing=settings.LANGCHAIN_TRACING_V2,
            )
            wrapper = LLMWrapper(llm=llm, config=wrapper_config, name="dashboard_mining")
            
            response = await wrapper.ainvoke([
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚åªè¿”å› JSON æ ¼å¼çš„å“åº”ã€‚"),
                HumanMessage(content=prompt)
            ])
            
            # è§£æ LLM è¿”å›çš„ JSON
            response_text = response.content if hasattr(response, 'content') else str(response)
            # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
            response_text = response_text.strip()
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            parsed = json.loads(response_text)
            raw_suggestions = parsed.get("suggestions", [])
            logger.info(f"[Mining] LLM è¿”å› {len(raw_suggestions)} ä¸ªåŸå§‹å»ºè®®")
            
            # 5. éªŒè¯æ¯ä¸ª SQL å¹¶è¿‡æ»¤æ— æ•ˆçš„
            validated_suggestions = []
            invalid_count = 0
            
            for idx, s in enumerate(raw_suggestions):
                sql = s.get("sql", "")
                title = s.get("title", f"å»ºè®®{idx+1}")
                
                if not sql:
                    logger.warning(f"[Mining] å»ºè®® '{title}' æ—  SQLï¼Œè·³è¿‡")
                    invalid_count += 1
                    continue
                
                # éªŒè¯ SQL
                is_valid, error_msg, invalid_refs = self._validate_sql_against_whitelist(
                    sql, valid_tables, valid_columns, db_type
                )
                
                if not is_valid:
                    logger.warning(f"[Mining] å»ºè®® '{title}' SQL éªŒè¯å¤±è´¥: {error_msg}")
                    for ref in invalid_refs[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ— æ•ˆå¼•ç”¨
                        logger.warning(f"[Mining]   - {ref}")
                    invalid_count += 1
                    # é™ä½ç½®ä¿¡åº¦ä½†ä»ç„¶ä¿ç•™ï¼ˆè®©ç”¨æˆ·å†³å®šï¼‰
                    s["confidence"] = max(0.3, float(s.get("confidence", 0.8)) - 0.4)
                    s["reasoning"] = f"ã€è­¦å‘Šã€‘{error_msg}\n\n" + s.get("reasoning", "")
                
                validated_suggestions.append(
                    schemas.MiningSuggestion(
                        title=s.get("title", ""),
                        description=s.get("description", ""),
                        chart_type=s.get("chart_type", "bar"),
                        sql=sql,
                        analysis_intent=s.get("analysis_intent", s.get("title", "æ•°æ®åˆ†æ")),
                        reasoning=s.get("reasoning", s.get("description", "")),
                        mining_dimension=s.get("mining_dimension", "business"),
                        confidence=float(s.get("confidence", 0.8)),
                        source_tables=s.get("source_tables", []),
                        key_fields=s.get("key_fields", []),
                        business_value=s.get("business_value", ""),
                        suggested_actions=s.get("suggested_actions", [])
                    )
                )
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œé«˜ç½®ä¿¡åº¦çš„æ’åœ¨å‰é¢
            validated_suggestions.sort(key=lambda x: x.confidence, reverse=True)
            
            logger.info(f"[Mining] æœ€ç»ˆè¿”å› {len(validated_suggestions)} ä¸ªå»ºè®®, {invalid_count} ä¸ª SQL éªŒè¯å¤±è´¥")
            return schemas.MiningResponse(suggestions=validated_suggestions)
            
        except json.JSONDecodeError as e:
            logger.error(f"[Mining] JSON è§£æå¤±è´¥: {e}")
            logger.error(f"[Mining] åŸå§‹å“åº”: {response_text[:500]}...")
            return schemas.MiningResponse(suggestions=[])
        except Exception as e:
            logger.error(f"[Mining] å»ºè®®ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return schemas.MiningResponse(suggestions=[])

    def trigger_dashboard_insights(
        self,
        db: Session,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest
    ) -> schemas.DashboardInsightResponse:
        """
        è§¦å‘çœ‹æ¿æ´å¯Ÿç”Ÿæˆï¼ˆåˆ›å»ºå ä½Widgetï¼Œåç»­ç”±åå°ä»»åŠ¡å¤„ç†ï¼‰
        """
        # 1. æ£€æŸ¥æƒé™
        self._check_permission(db, dashboard_id, user_id)
        
        # 2. è·å–Dashboard
        dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
        if not dashboard:
            raise ValueError(f"Dashboard {dashboard_id} not found")
            
        # 3. åˆ›å»ºæˆ–æ›´æ–°Widgetä¸º"åˆ†æä¸­"çŠ¶æ€
        # åˆ›å»ºåˆå§‹çš„ç©ºç»“æœ
        initial_result = schemas.InsightResult(
            summary=schemas.InsightSummary(total_rows=0, key_metrics={}, time_range="åˆ†æä¸­..."),
            trends=None, anomalies=[], correlations=[], recommendations=[]
        )
        
        # åˆ›å»ºæˆ–æ›´æ–° Widget (åŒæ­¥)
        widget_id = self._create_or_update_insight_widget(
            db,
            dashboard_id,
            initial_result,
            request.conditions,
            request.use_graph_relationships,
            analyzed_widget_count=0,
            status="processing" # æ ‡è®°ä¸ºå¤„ç†ä¸­
        )
        
        return schemas.DashboardInsightResponse(
            widget_id=widget_id,
            insights=initial_result,
            analyzed_widget_count=0,
            analysis_timestamp=datetime.utcnow(),
            applied_conditions=request.conditions,
            relationship_count=0,
            status="processing" # æ–°å¢çŠ¶æ€å­—æ®µ
        )

    async def process_dashboard_insights_task(
        self,
        dashboard_id: int,
        user_id: int,
        request: schemas.DashboardInsightRequest,
        widget_id: int
    ):
        """
        åå°ä»»åŠ¡ï¼šæ‰§è¡Œå®é™…çš„æ´å¯Ÿåˆ†æé€»è¾‘
        
        P1-FIX: ä¼˜åŒ–Sessionç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œæ›´å¥å£®çš„é”™è¯¯å¤„ç†
        æ³¨æ„: LLM é‡è¯•ç”± LLMWrapper ç»Ÿä¸€å¤„ç†ï¼Œæ­¤å¤„ä¸å†éœ€è¦å¤–å±‚é‡è¯•é€»è¾‘
        """
        import logging
        from contextlib import contextmanager
        
        logger = logging.getLogger(__name__)
        
        @contextmanager
        def get_db_session():
            """P1-FIX: ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿Sessionæ­£ç¡®å…³é—­"""
            session = SessionLocal()
            try:
                yield session
            except Exception:
                session.rollback()
                raise
            finally:
                session.close()
        
        with get_db_session() as db:
            try:
                logger.info(f"ğŸš€ å¼€å§‹åå°æ´å¯Ÿåˆ†æ Task (Dashboard: {dashboard_id}, Widget: {widget_id})")
                
                # 1. è·å–æ•°æ®
                dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
                if not dashboard:
                    raise ValueError(f"Dashboard {dashboard_id} not found")
                
                widgets = dashboard.widgets
                
                # ç­›é€‰Widgets
                if request.included_widget_ids:
                    widgets = [w for w in widgets if w.id in request.included_widget_ids]
                
                data_widgets = [w for w in widgets if w.widget_type != "insight_analysis"]
                
                if not data_widgets:
                    logger.warning(f"âš ï¸ Dashboard {dashboard_id} æ— æœ‰æ•ˆæ•°æ®ç»„ä»¶ï¼Œè·³è¿‡åˆ†æ")
                    self._update_widget_status(db, widget_id, "completed", "æ— æ•°æ®ç»„ä»¶å¯åˆ†æ")
                    return

                if getattr(request, "force_requery", False):
                    self._refresh_data_widgets(db, data_widgets, user_id)
                    refreshed_widgets = []
                    for w in data_widgets:
                        w2 = crud.crud_dashboard_widget.get(db, id=w.id)
                        if w2 and w2.widget_type != "insight_analysis":
                            refreshed_widgets.append(w2)
                    data_widgets = refreshed_widgets

                # 2. èšåˆæ•°æ®
                aggregated_data = self._aggregate_widget_data(data_widgets, request.conditions)
                logger.info(f"ğŸ“Š èšåˆæ•°æ®å®Œæˆ: {aggregated_data['total_rows']} è¡Œ, {len(aggregated_data['table_names'])} ä¸ªè¡¨")
                
                # 3. å›¾è°±æŸ¥è¯¢ï¼ˆå¸¦é‡è¯•ï¼‰
                relationship_context = None
                relationship_count = 0
                if request.use_graph_relationships and aggregated_data["table_names"]:
                    try:
                        connection_id = data_widgets[0].connection_id
                        relationship_context = graph_relationship_service.query_table_relationships(
                            connection_id,
                            aggregated_data["table_names"]
                        )
                        relationship_count = relationship_context.get("relationship_count", 0)
                        logger.info(f"ğŸ”— å›¾è°±å…³ç³»æŸ¥è¯¢å®Œæˆ: {relationship_count} ä¸ªå…³ç³»")
                    except Exception as e:
                        logger.warning(f"âš ï¸ å›¾è°±å…³ç³»æŸ¥è¯¢å¤±è´¥: {e}")

                # 4. æ´å¯Ÿåˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                insights = None
                retry_count = 0
                max_retries = 0  # å½“å‰ä½¿ç”¨è§„åˆ™å¼•æ“ï¼Œæ— éœ€é‡è¯•
                while retry_count <= max_retries:
                    try:
                        analysis_method_parts = [
                            "service_rule_based",
                            "widget_grouped",
                            "adaptive_time_filter",
                            "time_sorted_trend",
                            "iqr_anomaly",
                            "coerced_dimension_filters",
                        ]
                        if request.use_graph_relationships:
                            analysis_method_parts.append("graph_relationships")
                        analysis_method = "+".join(analysis_method_parts)

                        widget_groups = aggregated_data.get("by_widget") or []
                        has_time_series = any(
                            (g.get("date_columns") and g.get("numeric_columns") and (g.get("row_count") or 0) >= 2)
                            for g in widget_groups
                        )
                        confidence = 0.8
                        total_rows = int(aggregated_data.get("total_rows") or 0)
                        if total_rows < 10:
                            confidence = 0.5
                        elif total_rows < 50:
                            confidence = 0.65
                        elif total_rows < 200:
                            confidence = 0.75
                        else:
                            confidence = 0.82
                        if widget_groups and len(widget_groups) > 1:
                            confidence -= 0.02
                        if not has_time_series:
                            confidence -= 0.08
                        if relationship_count > 0:
                            confidence += 0.05
                        confidence = max(0.3, min(0.95, round(confidence, 2)))

                        insights = schemas.InsightResult(
                            summary=schemas.InsightSummary(
                                total_rows=aggregated_data["total_rows"],
                                key_metrics=self._extract_key_metrics(aggregated_data),
                                time_range="å·²åˆ†æ"
                            ),
                            trends=self._analyze_trends(aggregated_data),
                            anomalies=self._detect_anomalies(aggregated_data),
                            correlations=self._find_correlations(aggregated_data, relationship_context),
                            recommendations=[
                                schemas.InsightRecommendation(
                                    type="info",
                                    content=f"å·²åˆ†æ {len(data_widgets)} ä¸ªæ•°æ®ç»„ä»¶ï¼Œå…± {aggregated_data['total_rows']} æ¡æ•°æ®",
                                    priority="medium"
                                ),
                                schemas.InsightRecommendation(
                                    type="info",
                                    content="è¶‹åŠ¿ï¼šæŒ‰ç»„ä»¶åˆ†åˆ«è¯†åˆ«æ—¶é—´åˆ—å¹¶æŒ‰æ—¶é—´æ’åºï¼Œå¯¹æ•°å€¼åˆ—è®¡ç®—å˜åŒ–å¹…åº¦åé€‰æœ€æ˜¾è‘—é¡¹",
                                    priority="low"
                                ),
                                schemas.InsightRecommendation(
                                    type="info",
                                    content="å¼‚å¸¸ï¼šä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹ç¦»ç¾¤å€¼ï¼ˆä¸‹ç•Œ=Q1-1.5Ã—IQRï¼Œä¸Šç•Œ=Q3+1.5Ã—IQRï¼‰",
                                    priority="low"
                                ),
                            ]
                        )

                        trend_meta = aggregated_data.get("_trend_metadata") or {}
                        if isinstance(trend_meta.get("values"), list) and len(trend_meta["values"]) >= 5:
                            try:
                                from app.services.prediction_service import prediction_service

                                accuracy = prediction_service._calculate_accuracy_enhanced(
                                    trend_meta["values"],
                                    "linear",
                                    {}
                                )
                                trend_meta["accuracy_mape"] = accuracy.mape
                                trend_meta["accuracy_rmse"] = accuracy.rmse
                                trend_meta["accuracy_mae"] = accuracy.mae
                                trend_meta["accuracy_r_squared"] = accuracy.r_squared

                                quality_conf = 1 - min(100.0, max(0.0, float(accuracy.mape))) / 100.0
                                confidence = 0.6 * confidence + 0.4 * quality_conf
                                confidence = max(0.3, min(0.95, round(confidence, 2)))

                                if trend_meta.get("r_squared") is not None:
                                    analysis_method = (
                                        f"{analysis_method}"
                                        f"+trend_r2={float(trend_meta['r_squared']):.2f}"
                                        f"+mape={float(accuracy.mape):.1f}%"
                                    )
                            except Exception:
                                pass
                        break
                    except Exception as e:
                        retry_count += 1
                        if retry_count > max_retries:
                            logger.error(f"âŒ æ´å¯Ÿåˆ†æå¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")
                            raise
                        logger.warning(f"âš ï¸ æ´å¯Ÿåˆ†æå¤±è´¥ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•: {e}")
                        await asyncio.sleep(1)  # é‡è¯•å‰ç­‰å¾…
                
                # 5. æ›´æ–° Widget çŠ¶æ€ä¸ºå®Œæˆ
                self._update_insight_widget_result(
                    db, 
                    widget_id, 
                    insights, 
                    len(data_widgets),
                    status="completed",
                    analysis_method=analysis_method,
                    confidence_score=confidence,
                    relationship_count=relationship_count,
                    source_tables=aggregated_data.get("table_names"),
                    extra_metrics=aggregated_data.get("_trend_metadata")
                )
                
                logger.info(f"âœ… åå°æ´å¯Ÿåˆ†æå®Œæˆ (Widget: {widget_id})")
                
            except Exception as e:
                logger.exception(f"âŒ åå°æ´å¯Ÿåˆ†æå¤±è´¥: dashboard_id={dashboard_id}, widget_id={widget_id}")
                # P1-FIX: åœ¨åŒä¸€ä¸ªSessionä¸­æ›´æ–°å¤±è´¥çŠ¶æ€
                try:
                    db.rollback()  # å…ˆå›æ»šä¹‹å‰çš„ä»»ä½•æœªæäº¤çš„æ›´æ”¹
                    self._update_widget_status(db, widget_id, "failed", str(e))
                except Exception as update_error:
                    logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {update_error}")
    
    def _extract_key_metrics(self, aggregated_data: dict) -> dict:
        """ä»èšåˆæ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡"""
        key_metrics = {}

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        widget_groups = aggregated_data.get("by_widget")
        if widget_groups:
            total_added = 0
            for g in widget_groups:
                data = g.get("data") or []
                numeric_columns = g.get("numeric_columns") or []
                if not data or not numeric_columns:
                    continue
                prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")
                for col in numeric_columns[:5]:
                    values = [_as_float(row.get(col)) for row in data]
                    numeric_values = [v for v in values if v is not None]
                    if not numeric_values:
                        continue
                    key = f"{prefix}.{col}"
                    key_metrics[key] = {
                        "sum": round(sum(numeric_values), 2),
                        "avg": round(sum(numeric_values) / len(numeric_values), 2),
                        "max": round(max(numeric_values), 2),
                        "min": round(min(numeric_values), 2),
                        "count": len(numeric_values)
                    }
                    total_added += 1
                    if total_added >= 12:
                        return key_metrics
            return key_metrics

        data = aggregated_data.get("data", [])
        numeric_columns = aggregated_data.get("numeric_columns", [])

        if not data or not numeric_columns:
            return key_metrics

        for col in numeric_columns[:5]:
            numeric_values = [_as_float(row.get(col)) for row in data]
            numeric_values = [v for v in numeric_values if v is not None]
            if numeric_values:
                key_metrics[col] = {
                    "sum": round(sum(numeric_values), 2),
                    "avg": round(sum(numeric_values) / len(numeric_values), 2),
                    "max": round(max(numeric_values), 2),
                    "min": round(min(numeric_values), 2),
                    "count": len(numeric_values)
                }
        
        return key_metrics
    
    def _analyze_trends(self, aggregated_data: dict) -> Optional[schemas.InsightTrend]:
        """åˆ†ææ•°æ®è¶‹åŠ¿"""
        try:
            from datetime import datetime, date

            def _try_parse_datetime(value: Any):
                if value is None:
                    return None
                if isinstance(value, datetime):
                    return value
                if isinstance(value, date):
                    return datetime.combine(value, datetime.min.time())
                s = str(value).strip()
                if not s:
                    return None
                try:
                    return datetime.fromisoformat(s.replace("Z", "+00:00"))
                except Exception:
                    return None

            def _as_float(value: Any):
                if value is None:
                    return None
                if isinstance(value, (int, float)):
                    return float(value)
                s = str(value).strip()
                if not s:
                    return None
                try:
                    return float(s.replace(",", ""))
                except Exception:
                    return None

            def _pick_date_column(cols: List[str]) -> str:
                if not cols:
                    return ""
                for c in cols:
                    if any(kw in c.lower() for kw in ("created", "updated", "date", "time", "at", "æ—¥æœŸ", "æ—¶é—´")):
                        return c
                return cols[0]

            def _r_squared(values: List[float]) -> float:
                n = len(values)
                if n < 3:
                    return 0.0
                y_mean = sum(values) / n
                ss_tot = sum((y - y_mean) ** 2 for y in values)
                if ss_tot == 0:
                    return 1.0
                x_mean = (n - 1) / 2
                ss_xx = sum((i - x_mean) ** 2 for i in range(n))
                if ss_xx == 0:
                    return 0.0
                ss_xy = sum((i - x_mean) * (values[i] - y_mean) for i in range(n))
                slope = ss_xy / ss_xx
                intercept = y_mean - slope * x_mean
                predicted = [intercept + slope * i for i in range(n)]
                ss_res = sum((values[i] - predicted[i]) ** 2 for i in range(n))
                r2 = 1 - (ss_res / ss_tot)
                return max(0.0, min(1.0, float(r2)))

            widget_groups = aggregated_data.get("by_widget")
            best = None
            if widget_groups:
                for g in widget_groups:
                    data = g.get("data") or []
                    date_columns = g.get("date_columns") or []
                    numeric_columns = g.get("numeric_columns") or []
                    if not date_columns or not numeric_columns or len(data) < 2:
                        continue
                    date_col = _pick_date_column(date_columns)
                    prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")

                    for num_col in numeric_columns:
                        points = []
                        for row in data:
                            dt = _try_parse_datetime(row.get(date_col))
                            val = _as_float(row.get(num_col))
                            if dt is not None and val is not None:
                                points.append((dt, val))
                        if len(points) < 2:
                            continue
                        points.sort(key=lambda x: x[0])
                        first_dt, first_val = points[0]
                        last_dt, last_val = points[-1]
                        values = [p[1] for p in points]
                        delta = last_val - first_val
                        if first_val != 0:
                            rate = (delta / first_val) * 100
                            score = abs(rate)
                        else:
                            rate = None
                            score = abs(delta)
                        metric_name = f"{prefix}.{num_col}"
                        r2 = _r_squared(values)
                        candidate = {
                            "score": score,
                            "metric_name": metric_name,
                            "first_val": first_val,
                            "last_val": last_val,
                            "rate": rate,
                            "first_dt": first_dt,
                            "last_dt": last_dt,
                            "values": values,
                            "r_squared": r2,
                        }
                        if best is None or candidate["score"] > best["score"]:
                            best = candidate

            if best is None:
                date_columns = aggregated_data.get("date_columns", [])
                numeric_columns = aggregated_data.get("numeric_columns", [])
                data = aggregated_data.get("data", [])
                if not date_columns or not numeric_columns or len(data) < 2:
                    return None
                date_col = _pick_date_column(date_columns)
                for num_col in numeric_columns:
                    points = []
                    for row in data:
                        dt = _try_parse_datetime(row.get(date_col))
                        val = _as_float(row.get(num_col))
                        if dt is not None and val is not None:
                            points.append((dt, val))
                    if len(points) < 2:
                        continue
                    points.sort(key=lambda x: x[0])
                    first_dt, first_val = points[0]
                    last_dt, last_val = points[-1]
                    values = [p[1] for p in points]
                    delta = last_val - first_val
                    if first_val != 0:
                        rate = (delta / first_val) * 100
                        score = abs(rate)
                    else:
                        rate = None
                        score = abs(delta)
                    r2 = _r_squared(values)
                    candidate = {
                        "score": score,
                        "metric_name": num_col,
                        "first_val": first_val,
                        "last_val": last_val,
                        "rate": rate,
                        "first_dt": first_dt,
                        "last_dt": last_dt,
                        "values": values,
                        "r_squared": r2,
                    }
                    if best is None or candidate["score"] > best["score"]:
                        best = candidate

            if best is None:
                return None

            metric_name = best["metric_name"]
            first_val = best["first_val"]
            last_val = best["last_val"]
            rate = best["rate"]
            first_dt = best["first_dt"]
            last_dt = best["last_dt"]
            r2 = best["r_squared"]
            direction = "up" if last_val > first_val else ("down" if last_val < first_val else "stable")
            if rate is not None:
                rate = round(rate, 2)
                desc = f"{metric_name} ä» {first_val} å˜åŒ–åˆ° {last_val}ï¼ˆ{first_dt.date()}â†’{last_dt.date()}ï¼‰ï¼Œå˜åŒ–ç‡ {rate}%ï¼ˆRÂ²={r2:.2f}ï¼‰"
            else:
                desc = f"{metric_name} ä» {first_val} å˜åŒ–åˆ° {last_val}ï¼ˆ{first_dt.date()}â†’{last_dt.date()}ï¼‰ï¼Œå˜åŒ–é‡ {round(last_val - first_val, 2)}ï¼ˆRÂ²={r2:.2f}ï¼‰"

            aggregated_data["_trend_metadata"] = {
                "metric": metric_name,
                "r_squared": round(r2, 4),
                "values": best.get("values") or [],
                "point_count": len(best.get("values") or []),
            }

            return schemas.InsightTrend(
                trend_direction=direction,
                total_growth_rate=rate,
                description=desc
            )
        except Exception:
            pass
        
        return None
    
    def _detect_anomalies(self, aggregated_data: dict) -> List[schemas.InsightAnomaly]:
        """æ£€æµ‹æ•°æ®å¼‚å¸¸"""
        anomalies = []

        def _severity_score(s: Optional[str]) -> int:
            if s == "high":
                return 3
            if s == "medium":
                return 2
            return 1

        def _detect_for_series(metric_name: str, values: List[float]) -> List[schemas.InsightAnomaly]:
            if len(values) < 8:
                return []
            values_sorted = sorted(values)

            def _quantile(sorted_vals: List[float], q: float) -> float:
                n = len(sorted_vals)
                if n == 1:
                    return sorted_vals[0]
                pos = (n - 1) * q
                lo = int(pos)
                hi = min(lo + 1, n - 1)
                w = pos - lo
                return sorted_vals[lo] * (1 - w) + sorted_vals[hi] * w

            q1 = _quantile(values_sorted, 0.25)
            q3 = _quantile(values_sorted, 0.75)
            iqr = q3 - q1
            if iqr <= 0:
                return []

            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr

            max_val = values_sorted[-1]
            min_val = values_sorted[0]

            found = []
            if max_val > upper:
                exceed = (max_val - upper) / iqr
                severity = "high" if exceed >= 3 else ("medium" if exceed >= 1.5 else "low")
                found.append(schemas.InsightAnomaly(
                    type="outlier",
                    metric=metric_name,
                    description=f"{metric_name} å­˜åœ¨å¼‚å¸¸é«˜å€¼ {max_val}ï¼ˆä¸Šç•Œ {round(upper, 2)}ï¼‰",
                    severity=severity
                ))
            if min_val < lower:
                exceed = (lower - min_val) / iqr
                severity = "high" if exceed >= 3 else ("medium" if exceed >= 1.5 else "low")
                found.append(schemas.InsightAnomaly(
                    type="outlier",
                    metric=metric_name,
                    description=f"{metric_name} å­˜åœ¨å¼‚å¸¸ä½å€¼ {min_val}ï¼ˆä¸‹ç•Œ {round(lower, 2)}ï¼‰",
                    severity=severity
                ))
            return found

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        widget_groups = aggregated_data.get("by_widget")
        if widget_groups:
            for g in widget_groups:
                data = g.get("data") or []
                numeric_columns = g.get("numeric_columns") or []
                if not data or not numeric_columns:
                    continue
                prefix = g.get("table_name") or (g.get("title") or f"widget_{g.get('widget_id')}")
                for col in numeric_columns[:3]:
                    vals = [_as_float(row.get(col)) for row in data]
                    vals = [v for v in vals if v is not None]
                    anomalies.extend(_detect_for_series(f"{prefix}.{col}", vals))
        else:
            data = aggregated_data.get("data", [])
            numeric_columns = aggregated_data.get("numeric_columns", [])
            if not data or not numeric_columns:
                return anomalies
            for col in numeric_columns[:3]:
                vals = [_as_float(row.get(col)) for row in data]
                vals = [v for v in vals if v is not None]
                anomalies.extend(_detect_for_series(col, vals))

        anomalies.sort(key=lambda a: _severity_score(a.severity), reverse=True)
        return anomalies[:5]
    
    def _find_correlations(self, aggregated_data: dict, relationship_context: Optional[dict]) -> List[schemas.InsightCorrelation]:
        """å‘ç°æ•°æ®å…³è”"""
        correlations = []
        
        # åŸºäºå›¾è°±å…³ç³»ç”Ÿæˆå…³è”æ´å¯Ÿ
        if relationship_context:
            direct_rels = relationship_context.get("direct_relationships", [])
            for rel in direct_rels[:3]:
                src_table = rel.get("source_table", "")
                tgt_table = rel.get("target_table", "")
                if src_table and tgt_table:
                    correlations.append(schemas.InsightCorrelation(
                        type="cross_table",
                        entities=[src_table, tgt_table],
                        description=f"{src_table} ä¸ {tgt_table} å­˜åœ¨å¤–é”®å…³è”",
                        strength=0.8
                    ))
        
        return correlations

    def _check_permission(self, db: Session, dashboard_id: int, user_id: int):
        has_permission = crud.crud_dashboard.check_permission(
            db, dashboard_id=dashboard_id, user_id=user_id, required_level="viewer"
        )
        if not has_permission:
            raise PermissionError("No permission to view this dashboard")

    def _aggregate_widget_data(
        self,
        widgets: List[DashboardWidget],
        conditions: Optional[schemas.InsightConditions]
    ) -> Dict[str, Any]:
        """èšåˆWidgetæ•°æ®"""
        from datetime import datetime, date

        def _as_float(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        def _try_parse_datetime(value: Any):
            if value is None:
                return None
            if isinstance(value, datetime):
                return value
            if isinstance(value, date):
                return datetime.combine(value, datetime.min.time())
            s = str(value).strip()
            if not s:
                return None
            try:
                return datetime.fromisoformat(s.replace("Z", "+00:00"))
            except Exception:
                return None

        def _infer_columns(rows: List[Dict[str, Any]]) -> Dict[str, List[str]]:
            numeric = set()
            dates = set()
            if not rows:
                return {"numeric": [], "dates": []}

            sample = rows[: min(20, len(rows))]
            keys = set()
            for r in sample:
                if isinstance(r, dict):
                    keys.update(r.keys())

            for k in keys:
                k_lower = str(k).lower()
                if any(keyword in k_lower for keyword in ("date", "time", "created", "updated", "at", "æ—¥æœŸ", "æ—¶é—´")):
                    for r in sample:
                        dt = _try_parse_datetime(r.get(k)) if isinstance(r, dict) else None
                        if dt is not None:
                            dates.add(k)
                            break

                for r in sample:
                    if not isinstance(r, dict):
                        continue
                    v = r.get(k)
                    fv = _as_float(v)
                    if fv is not None:
                        numeric.add(k)
                        break

            return {"numeric": list(numeric), "dates": list(dates)}

        aggregated_rows = []
        table_names = set()
        numeric_columns = set()
        date_columns = set()
        widget_summaries = []
        by_widget = []
        
        for widget in widgets:
            # æå–widgetæ•°æ®
            if not widget.data_cache or "data" not in widget.data_cache:
                continue
            
            data = widget.data_cache["data"]
            if not data or not isinstance(data, list):
                continue
            
            # åº”ç”¨æ¡ä»¶è¿‡æ»¤
            filtered_data = self._apply_conditions(data, conditions)

            table_name = None
            if widget.query_config and isinstance(widget.query_config, dict):
                table_name = widget.query_config.get("table_name")

            inferred = _infer_columns(filtered_data)
            widget_numeric_columns = inferred["numeric"]
            widget_date_columns = inferred["dates"]

            by_widget.append({
                "widget_id": widget.id,
                "title": getattr(widget, "title", None),
                "widget_type": getattr(widget, "widget_type", None),
                "table_name": table_name,
                "row_count": len(filtered_data),
                "numeric_columns": widget_numeric_columns,
                "date_columns": widget_date_columns,
                "data": filtered_data,
            })
            
            aggregated_rows.extend(filtered_data)
            
            # æå–è¡¨å
            if table_name:
                table_names.add(table_name)
            
            # æå–åˆ—ä¿¡æ¯
            for c in widget_numeric_columns:
                numeric_columns.add(c)
            for c in widget_date_columns:
                date_columns.add(c)
            
            widget_summaries.append({
                "id": widget.id,
                "type": widget.widget_type,
                "title": widget.title,
                "row_count": len(filtered_data)
            })
        
        return {
            "data": aggregated_rows,
            "total_rows": len(aggregated_rows),
            "table_names": list(table_names),
            "numeric_columns": list(numeric_columns),
            "date_columns": list(date_columns),
            "by_widget": by_widget,
            "widget_summaries": widget_summaries
        }

    def _refresh_data_widgets(self, db: Session, widgets: List[DashboardWidget], user_id: int) -> None:
        from app.services.dashboard_widget_service import dashboard_widget_service

        for w in widgets:
            try:
                dashboard_widget_service.refresh_widget(db, widget_id=w.id, user_id=user_id)
            except Exception:
                logger.exception("åˆ·æ–°æ•°æ®ç»„ä»¶å¤±è´¥: widget_id=%s", w.id)
    
    def _apply_conditions(
        self,
        data: List[Dict[str, Any]],
        conditions: Optional[schemas.InsightConditions]
    ) -> List[Dict[str, Any]]:
        """åº”ç”¨æŸ¥è¯¢æ¡ä»¶è¿‡æ»¤æ•°æ®"""
        if not conditions:
            return data
        
        filtered_data = data.copy()
        
        def _try_parse_datetime(value: Any):
            if value is None:
                return None
            from datetime import datetime, date
            if isinstance(value, datetime):
                return value
            if isinstance(value, date):
                return datetime.combine(value, datetime.min.time())
            s = str(value).strip()
            if not s:
                return None
            try:
                return datetime.fromisoformat(s.replace("Z", "+00:00"))
            except Exception:
                pass
            for fmt in (
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y/%m/%dT%H:%M:%S",
            ):
                try:
                    return datetime.strptime(s, fmt)
                except Exception:
                    continue
            return None

        def _calc_relative_range(relative_range: str):
            from datetime import datetime, timedelta
            now = datetime.utcnow()
            key = (relative_range or "").strip().lower()
            if not key:
                return None, None
            if key in {"last_7_days", "7d", "last7days"}:
                return now - timedelta(days=7), now
            if key in {"last_30_days", "30d", "last30days"}:
                return now - timedelta(days=30), now
            if key in {"last_90_days", "90d", "last90days"}:
                return now - timedelta(days=90), now
            if key in {"this_month", "month_to_date", "mtd"}:
                start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                return start, now
            if key in {"this_year", "year_to_date", "ytd"}:
                start = now.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)
                return start, now
            return None, None

        def _select_date_column(rows: List[Dict[str, Any]]) -> Optional[str]:
            if not rows:
                return None
            sample = rows[:50]
            keys = list(sample[0].keys())
            keyword_keys = [
                k for k in keys
                if any(kw in k.lower() for kw in ("date", "time", "created", "updated", "at", "æ—¥æœŸ", "æ—¶é—´"))
            ]
            candidates = keyword_keys + [k for k in keys if k not in keyword_keys]
            best_key = None
            best_ratio = 0.0
            for k in candidates:
                parsed = 0
                seen = 0
                for r in sample:
                    if k not in r:
                        continue
                    seen += 1
                    if _try_parse_datetime(r.get(k)) is not None:
                        parsed += 1
                if seen == 0:
                    continue
                ratio = parsed / seen
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_key = k
            if best_ratio >= 0.6:
                return best_key
            return None

        def _coerce_number(value: Any):
            if value is None:
                return None
            if isinstance(value, (int, float)):
                return float(value)
            s = str(value).strip()
            if not s:
                return None
            try:
                return float(s.replace(",", ""))
            except Exception:
                return None

        def _values_match(row_val: Any, expected: Any) -> bool:
            if row_val is None and expected is None:
                return True
            if row_val is None or expected is None:
                return False
            if isinstance(expected, str) or isinstance(row_val, str):
                row_num = _coerce_number(row_val)
                exp_num = _coerce_number(expected)
                if row_num is not None and exp_num is not None:
                    return row_num == exp_num
                return str(row_val).strip() == str(expected).strip()
            return row_val == expected
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if conditions.time_range:
            date_column = _select_date_column(filtered_data)
            
            if date_column:
                start_dt = _try_parse_datetime(conditions.time_range.start) if conditions.time_range.start else None
                end_dt = _try_parse_datetime(conditions.time_range.end) if conditions.time_range.end else None
                if (start_dt is None and end_dt is None) and getattr(conditions.time_range, "relative_range", None):
                    start_dt, end_dt = _calc_relative_range(conditions.time_range.relative_range)
                
                if start_dt or end_dt:
                    def _in_range(row: Dict[str, Any]) -> bool:
                        row_dt = _try_parse_datetime(row.get(date_column))
                        if row_dt is None:
                            return False
                        if start_dt and row_dt < start_dt:
                            return False
                        if end_dt and row_dt > end_dt:
                            return False
                        return True
                    
                    filtered_data = [row for row in filtered_data if _in_range(row)]
        
        # ç»´åº¦ç­›é€‰
        if conditions.dimension_filters:
            for column, value in conditions.dimension_filters.items():
                if isinstance(value, (list, tuple, set)):
                    allowed = list(value)
                    filtered_data = [
                        row for row in filtered_data
                        if any(_values_match(row.get(column), v) for v in allowed)
                    ]
                else:
                    filtered_data = [row for row in filtered_data if _values_match(row.get(column), value)]
        
        return filtered_data
    
    def _create_or_update_insight_widget(
        self,
        db: Session,
        dashboard_id: int,
        insights: schemas.InsightResult,
        conditions: Optional[schemas.InsightConditions],
        use_graph_relationships: bool,
        analyzed_widget_count: int,
        status: str = "completed",
        lineage: Optional[Dict[str, Any]] = None
    ) -> int:
        """åˆ›å»ºæˆ–æ›´æ–°æ´å¯ŸWidgetï¼Œä¿å­˜æº¯æºä¿¡æ¯"""
        existing_widgets = crud.crud_dashboard_widget.get_by_dashboard(db, dashboard_id=dashboard_id)
        
        insight_widget = None
        # P0-FIX: ä»ç°æœ‰æ•°æ®Widgetä¸­è·å–connection_idï¼Œé¿å…ç¡¬ç¼–ç 
        default_connection_id = None
        for widget in existing_widgets:
            if widget.widget_type == "insight_analysis":
                insight_widget = widget
            elif default_connection_id is None and widget.connection_id:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®Widgetçš„connection_idä½œä¸ºé»˜è®¤å€¼
                default_connection_id = widget.connection_id
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®Widgetï¼Œå°è¯•ä»Dashboardå…³è”çš„connectionè·å–
        if default_connection_id is None:
            dashboard = crud.crud_dashboard.get(db, id=dashboard_id)
            if dashboard and dashboard.widgets:
                for w in dashboard.widgets:
                    if w.widget_type != "insight_analysis" and w.connection_id:
                        default_connection_id = w.connection_id
                        break
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ° connection_idï¼Œè®°å½•è­¦å‘Šï¼ˆä¸å†ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼ï¼‰
        if default_connection_id is None:
            logger.warning(f"Dashboard {dashboard_id} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ connection_idï¼Œæ´å¯Ÿåˆ†æå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        
        # P0: å°†æº¯æºä¿¡æ¯åˆå¹¶åˆ° query_config
        query_config = {
            "analysis_scope": "all_widgets",
            "analysis_dimensions": ["summary", "trends", "correlations", "recommendations"],
            "refresh_strategy": "manual",
            "last_analysis_at": datetime.utcnow().isoformat(),
            "use_graph_relationships": use_graph_relationships,
            "analyzed_widget_count": analyzed_widget_count,
            "status": status,
        }
        
        if conditions:
            query_config["current_conditions"] = conditions.dict(exclude_none=True)
        
        # P0: ä¿å­˜æº¯æºä¿¡æ¯
        if lineage:
            query_config["source_tables"] = lineage.get("source_tables", [])
            query_config["generated_sql"] = lineage.get("generated_sql")
            query_config["user_intent"] = lineage.get("sql_generation_trace", {}).get("user_intent")
            query_config["few_shot_samples_count"] = lineage.get("sql_generation_trace", {}).get("few_shot_samples_count", 0)
            query_config["generation_method"] = lineage.get("sql_generation_trace", {}).get("generation_method", "standard")
            query_config["execution_time_ms"] = lineage.get("execution_metadata", {}).get("execution_time_ms", 0)
            query_config["from_cache"] = lineage.get("execution_metadata", {}).get("from_cache", False)
            query_config["row_count"] = lineage.get("execution_metadata", {}).get("row_count", 0)
            query_config["db_type"] = lineage.get("execution_metadata", {}).get("db_type")
            query_config["data_transformations"] = lineage.get("data_transformations", [])
            query_config["confidence_score"] = lineage.get("confidence_score", 0.8)
            query_config["analysis_method"] = lineage.get("analysis_method", "auto")
        
        data_cache = insights.dict(exclude_none=True)
        
        if insight_widget:
            crud.crud_dashboard_widget.update(
                db,
                db_obj=insight_widget,
                obj_in=schemas.WidgetUpdate(title="çœ‹æ¿æ´å¯Ÿåˆ†æ")
            )
            insight_widget.query_config = query_config
            insight_widget.data_cache = data_cache
            insight_widget.last_refresh_at = datetime.utcnow()
            db.commit()
            db.refresh(insight_widget)
            return insight_widget.id
        else:
            widget_create = schemas.WidgetCreate(
                widget_type="insight_analysis",
                title="çœ‹æ¿æ´å¯Ÿåˆ†æ",
                connection_id=default_connection_id,  # P0-FIX: ä½¿ç”¨åŠ¨æ€è·å–çš„connection_id
                query_config=query_config,
                chart_config=None,
                position_config={"x": 0, "y": 0, "w": 12, "h": 6},
                refresh_interval=0
            )
            
            new_widget = crud.crud_dashboard_widget.create_widget(
                db,
                dashboard_id=dashboard_id,
                obj_in=widget_create
            )
            new_widget.data_cache = data_cache
            db.commit()
            db.refresh(new_widget)
            return new_widget.id

    def _update_insight_widget_result(
        self,
        db: Session,
        widget_id: int,
        insights: schemas.InsightResult,
        count: int,
        status: str,
        analysis_method: Optional[str] = None,
        confidence_score: Optional[float] = None,
        relationship_count: Optional[int] = None,
        source_tables: Optional[List[str]] = None,
        extra_metrics: Optional[Dict[str, Any]] = None,
    ):
        """æ›´æ–°æ´å¯Ÿ Widget çš„åˆ†æç»“æœ"""
        try:
            widget = crud.crud_dashboard_widget.get(db, id=widget_id)
            if widget:
                query_config = widget.query_config or {}
                query_config["status"] = status
                query_config["analyzed_widget_count"] = count
                query_config["last_analysis_at"] = datetime.utcnow().isoformat()
                if analysis_method is not None:
                    query_config["analysis_method"] = analysis_method
                if confidence_score is not None:
                    query_config["confidence_score"] = confidence_score
                if relationship_count is not None:
                    query_config["relationship_count"] = relationship_count
                if source_tables is not None:
                    query_config["source_tables"] = source_tables
                if extra_metrics is not None:
                    query_config["trend_metrics"] = extra_metrics
                
                widget.query_config = query_config
                widget.data_cache = insights.dict(exclude_none=True)
                widget.last_refresh_at = datetime.utcnow()
                db.commit()
        except Exception as e:
            db.rollback()
            raise

    def _update_widget_status(self, db: Session, widget_id: int, status: str, error: str = None):
        """æ›´æ–° Widget çŠ¶æ€"""
        try:
            widget = crud.crud_dashboard_widget.get(db, id=widget_id)
            if widget:
                query_config = widget.query_config or {}
                query_config["status"] = status
                query_config["last_updated_at"] = datetime.utcnow().isoformat()
                if error:
                    # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦ï¼Œé¿å…å­˜å‚¨è¿‡å¤§
                    query_config["error"] = str(error)[:1000]
                widget.query_config = query_config
                db.commit()
        except Exception as e:
            db.rollback()
            raise


# åˆ›å»ºå…¨å±€å®ä¾‹
dashboard_insight_service = DashboardInsightService()
