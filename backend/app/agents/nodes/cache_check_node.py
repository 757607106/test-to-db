"""
ç¼“å­˜æ£€æŸ¥èŠ‚ç‚¹ (Cache Check Node)

åœ¨ clarification ä¹‹åã€supervisor ä¹‹å‰æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜ã€‚
å¦‚æœå‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç»“æœï¼Œè·³è¿‡åç»­æµç¨‹ã€‚

å·¥ä½œæµç¨‹:
1. ä»æ¶ˆæ¯ä¸­æå–ç”¨æˆ·æŸ¥è¯¢
2. æ£€æŸ¥ç²¾ç¡®åŒ¹é…ç¼“å­˜ï¼ˆL1ï¼‰
3. æ£€æŸ¥è¯­ä¹‰åŒ¹é…ç¼“å­˜ï¼ˆL2ï¼‰
4. å¦‚æœå‘½ä¸­ï¼Œè®¾ç½®ç»“æœå¹¶æ ‡è®°è·³è¿‡ supervisor
5. å¦‚æœæœªå‘½ä¸­ï¼Œç»§ç»­æ­£å¸¸æµç¨‹

ç¼“å­˜ç­–ç•¥:
- ç²¾ç¡®åŒ¹é…ï¼šç›¸åŒæŸ¥è¯¢ + ç›¸åŒè¿æ¥ID
- è¯­ä¹‰åŒ¹é…ï¼šç›¸ä¼¼åº¦ >= 0.95 çš„å†å² QA å¯¹
"""
import logging
import json
from typing import Dict, Any, Optional

from langchain_core.messages import AIMessage, HumanMessage

from app.core.state import SQLMessageState, SQLExecutionResult
from app.services.query_cache_service import get_cache_service, CacheHit

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def extract_user_query(messages: list) -> Optional[str]:
    """
    ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–æœ€æ–°çš„ç”¨æˆ·æŸ¥è¯¢
    
    Args:
        messages: LangChain æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
    """
    for message in reversed(messages):
        if hasattr(message, 'type') and message.type == 'human':
            return _normalize_query_content(message.content)
        elif isinstance(message, HumanMessage):
            return _normalize_query_content(message.content)
    return None


def _normalize_query_content(content: Any) -> Optional[str]:
    """
    è§„èŒƒåŒ–ç”¨æˆ·æŸ¥è¯¢å†…å®¹ï¼Œå…¼å®¹å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
    """
    if content is None:
        return None
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for item in content:
            if isinstance(item, dict):
                if item.get("type") == "text" and item.get("text"):
                    parts.append(str(item.get("text")))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(p for p in parts if p).strip() or None
    if isinstance(content, dict):
        if content.get("type") == "text" and content.get("text"):
            return str(content.get("text"))
    return str(content)


def format_cached_response(cache_hit: CacheHit, connection_id: int) -> str:
    """
    æ ¼å¼åŒ–ç¼“å­˜å‘½ä¸­çš„å“åº”
    
    Args:
        cache_hit: ç¼“å­˜å‘½ä¸­ç»“æœ
        connection_id: æ•°æ®åº“è¿æ¥ID
        
    Returns:
        æ ¼å¼åŒ–çš„å“åº”å­—ç¬¦ä¸²
    """
    hit_type_label = "ç²¾ç¡®åŒ¹é…" if cache_hit.hit_type == "exact" else f"è¯­ä¹‰åŒ¹é… (ç›¸ä¼¼åº¦: {cache_hit.similarity:.1%})"
    
    response_parts = [
        f"âœ¨ **ç¼“å­˜å‘½ä¸­** ({hit_type_label})",
        "",
        f"**SQL æŸ¥è¯¢:**",
        f"```sql",
        f"{cache_hit.sql}",
        f"```",
    ]
    
    # æ·»åŠ æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if cache_hit.result is not None:
        result = cache_hit.result
        if isinstance(result, dict):
            if result.get("success"):
                raw_data = result.get("data", [])
                
                # âœ… å…¼å®¹ä¸¤ç§æ•°æ®æ ¼å¼ï¼š
                # 1. ç›´æ¥åˆ—è¡¨: [{"col1": val1, ...}, ...]
                # 2. åµŒå¥—æ ¼å¼: {"columns": [...], "data": [[val1, val2, ...], ...]}
                if isinstance(raw_data, dict) and "columns" in raw_data and "data" in raw_data:
                    # åµŒå¥—æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    columns = raw_data.get("columns", [])
                    rows = raw_data.get("data", [])
                    data = [dict(zip(columns, row)) for row in rows] if columns and rows else []
                elif isinstance(raw_data, list):
                    data = raw_data
                else:
                    data = []
                
                if len(data) > 0:
                    response_parts.extend([
                        "",
                        f"**æŸ¥è¯¢ç»“æœ:** (å…± {len(data)} æ¡è®°å½•)",
                        "",
                    ])
                    # æ ¼å¼åŒ–è¡¨æ ¼
                    if isinstance(data[0], dict):
                        headers = list(data[0].keys())
                        response_parts.append("| " + " | ".join(headers) + " |")
                        response_parts.append("| " + " | ".join(["---"] * len(headers)) + " |")
                        for row in data[:10]:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                            values = [str(row.get(h, ""))[:30] for h in headers]  # æˆªæ–­é•¿å€¼
                            response_parts.append("| " + " | ".join(values) + " |")
                        if len(data) > 10:
                            response_parts.append(f"... è¿˜æœ‰ {len(data) - 10} æ¡è®°å½•")
                else:
                    response_parts.extend([
                        "",
                        "**æŸ¥è¯¢ç»“æœ:** æ— æ•°æ®",
                    ])
            else:
                response_parts.extend([
                    "",
                    f"**æ‰§è¡Œé”™è¯¯:** {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                ])
    else:
        response_parts.extend([
            "",
            "ğŸ“ *SQL å·²ä»ç¼“å­˜è·å–ï¼Œæ­£åœ¨æ‰§è¡Œ...*",
        ])
    
    return "\n".join(response_parts)


async def cache_check_node(state: SQLMessageState) -> Dict[str, Any]:
    """
    ç¼“å­˜æ£€æŸ¥èŠ‚ç‚¹ - LangGraph å¼‚æ­¥èŠ‚ç‚¹å‡½æ•°
    
    åœ¨ clarification ä¹‹åã€supervisor ä¹‹å‰æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜ã€‚
    å¦‚æœå‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç»“æœï¼Œè·³è¿‡ supervisorã€‚
    
    Args:
        state: å½“å‰çš„ SQL æ¶ˆæ¯çŠ¶æ€
        
    Returns:
        Dict[str, Any]: çŠ¶æ€æ›´æ–°
            - cache_hit: æ˜¯å¦å‘½ä¸­ç¼“å­˜
            - generated_sql: ç¼“å­˜çš„ SQLï¼ˆå¦‚æœå‘½ä¸­ï¼‰
            - execution_result: ç¼“å­˜çš„æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            - messages: æ·»åŠ  AI å“åº”æ¶ˆæ¯ï¼ˆå¦‚æœå‘½ä¸­ï¼‰
            
    çŠ¶æ€å­—æ®µ:
        è¯»å–:
        - messages: è·å–ç”¨æˆ·æŸ¥è¯¢
        - connection_id: æ•°æ®åº“è¿æ¥ID
        - pending_clarification: æ˜¯å¦æ­£åœ¨ç­‰å¾…æ¾„æ¸…ï¼ˆè·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼‰
        
        æ›´æ–°:
        - cache_hit: æ˜¯å¦å‘½ä¸­ç¼“å­˜
        - cache_hit_type: å‘½ä¸­ç±»å‹ ("exact" / "semantic" / None)
        - generated_sql: SQL è¯­å¥
        - execution_result: æ‰§è¡Œç»“æœ
        - current_stage: å½“å‰é˜¶æ®µ
    """
    logger.info("=== è¿›å…¥ç¼“å­˜æ£€æŸ¥èŠ‚ç‚¹ ===")
    
    # 0. æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç­‰å¾…æ¾„æ¸…å›å¤
    pending_clarification = state.get("pending_clarification", False)
    if pending_clarification:
        logger.info("æ­£åœ¨ç­‰å¾…ç”¨æˆ·æ¾„æ¸…å›å¤ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥")
        return {
            "cache_hit": False,
            "cache_hit_type": None
        }
    
    # 1. è·å–æ¶ˆæ¯å’Œè¿æ¥ID
    messages = state.get("messages", [])
    connection_id = state.get("connection_id", 15)
    
    # æå–ç”¨æˆ·æŸ¥è¯¢
    user_query = extract_user_query(messages)
    if not user_query:
        logger.warning("æ— æ³•æå–ç”¨æˆ·æŸ¥è¯¢ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥")
        return {
            "cache_hit": False,
            "cache_hit_type": None
        }
    
    logger.info(f"ç¼“å­˜æ£€æŸ¥: query='{user_query[:50]}...', connection_id={connection_id}")
    
    # 2. æ£€æŸ¥ç¼“å­˜
    try:
        cache_service = get_cache_service()
        cache_hit = await cache_service.check_cache(user_query, connection_id)
        
        if cache_hit:
            logger.info(f"ç¼“å­˜å‘½ä¸­! type={cache_hit.hit_type}, similarity={cache_hit.similarity:.3f}")

            # å¦‚æœæ²¡æœ‰æ‰§è¡Œç»“æœï¼Œç›´æ¥åœ¨æ­¤èŠ‚ç‚¹æ‰§è¡Œ SQL å¹¶è¿”å›ç»“æœ
            if cache_hit.result is None:
                # âœ… æ¸…ç†å¯èƒ½è¢«æ±¡æŸ“çš„ SQLï¼ˆä¿®å¤ Milvus å­˜å‚¨æ—¶çš„æ±¡æŸ“é—®é¢˜ï¼‰
                clean_sql = cache_hit.sql
                if clean_sql:
                    # ç§»é™¤å¯èƒ½çš„ JSON æ±¡æŸ“: ;", "connection_id": xxx; æˆ–ç±»ä¼¼æ¨¡å¼
                    import re
                    # åŒ¹é… SQL è¯­å¥æœ«å°¾çš„æ±¡æŸ“éƒ¨åˆ†
                    clean_sql = re.sub(r';\s*"\s*,\s*"connection_id"\s*:\s*\d+\s*;?\s*$', ';', clean_sql)
                    clean_sql = clean_sql.strip()
                    # ç¡®ä¿ SQL ä»¥åˆ†å·ç»“å°¾
                    if clean_sql and not clean_sql.endswith(';'):
                        clean_sql += ';'
                
                # âœ… ç›´æ¥æ‰§è¡Œ SQLï¼Œé¿å…èµ°å®Œæ•´çš„ supervisor æµç¨‹
                try:
                    from app.agents.agents.sql_executor_agent import execute_sql_query
                    
                    exec_result_str = execute_sql_query.invoke({
                        "sql_query": clean_sql,  # ä½¿ç”¨æ¸…ç†åçš„ SQL
                        "connection_id": connection_id,
                        "timeout": 30
                    })
                    
                    # âœ… execute_sql_query è¿”å›çš„æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                    exec_result = json.loads(exec_result_str) if isinstance(exec_result_str, str) else exec_result_str
                    
                    if exec_result.get("success"):
                        # æ„å»ºæ‰§è¡Œç»“æœ
                        execution_result = SQLExecutionResult(
                            success=True,
                            data=exec_result.get("data"),
                            error=None,
                            execution_time=exec_result.get("execution_time", 0),
                            rows_affected=exec_result.get("data", {}).get("row_count", 0) if isinstance(exec_result.get("data"), dict) else 0
                        )
                        
                        # æ„å»ºç¼“å­˜å‘½ä¸­å“åº”
                        cache_hit.result = {
                            "success": True,
                            "data": exec_result.get("data")
                        }
                        response_content = format_cached_response(cache_hit, connection_id)
                        ai_message = AIMessage(content=response_content)
                        
                        return {
                            "cache_hit": True,
                            "cache_hit_type": cache_hit.hit_type,
                            "generated_sql": cache_hit.sql,
                            "execution_result": execution_result,
                            "current_stage": "completed",
                            "messages": list(messages) + [ai_message]
                        }
                    else:
                        # SQL æ‰§è¡Œå¤±è´¥ï¼Œé‡æ–°å¼€å§‹å®Œæ•´æµç¨‹ï¼ˆæ•°æ®åº“schemaå¯èƒ½å·²å˜æ›´ï¼‰
                        logger.warning(f"ç¼“å­˜ SQL æ‰§è¡Œå¤±è´¥: {exec_result.get('error')}")
                        logger.info("ç¼“å­˜SQLå¯èƒ½å·²è¿‡æ—¶ï¼Œå°†é‡æ–°åˆ†ææ•°æ®åº“schemaå¹¶ç”Ÿæˆæ–°çš„SQL")
                        
                        # âœ… æ¸…ç†å¹¶éªŒè¯æ¶ˆæ¯å†å²ï¼Œç§»é™¤ä¸å®Œæ•´çš„tool_calls
                        from app.core.message_utils import validate_and_fix_message_history
                        clean_messages = validate_and_fix_message_history(list(messages))
                        
                        return {
                            "cache_hit": False,
                            "cache_hit_type": None,  # æ ‡è®°ä¸ºå®Œå…¨æœªå‘½ä¸­
                            "current_stage": "schema_analysis",  # ä»schemaåˆ†æé‡æ–°å¼€å§‹
                            "messages": clean_messages  # è¿”å›æ¸…ç†åçš„æ¶ˆæ¯å†å²
                        }
                        
                except Exception as e:
                    logger.error(f"ç¼“å­˜ SQL æ‰§è¡Œå¼‚å¸¸: {e}")
                    logger.info("ç¼“å­˜SQLæ‰§è¡Œå¼‚å¸¸ï¼Œå°†é‡æ–°åˆ†ææ•°æ®åº“schemaå¹¶ç”Ÿæˆæ–°çš„SQL")
                    
                    # âœ… æ¸…ç†å¹¶éªŒè¯æ¶ˆæ¯å†å²ï¼Œç§»é™¤ä¸å®Œæ•´çš„tool_calls
                    from app.core.message_utils import validate_and_fix_message_history
                    clean_messages = validate_and_fix_message_history(list(messages))
                    
                    return {
                        "cache_hit": False,
                        "cache_hit_type": None,  # æ ‡è®°ä¸ºå®Œå…¨æœªå‘½ä¸­
                        "current_stage": "schema_analysis",  # ä»schemaåˆ†æé‡æ–°å¼€å§‹
                        "messages": clean_messages  # è¿”å›æ¸…ç†åçš„æ¶ˆæ¯å†å²
                    }

            # æœ‰æ‰§è¡Œç»“æœï¼Œç›´æ¥è¿”å›ç¼“å­˜ç»“æœå¹¶ç»“æŸ
            # âœ… æ¸…ç†å¹¶éªŒè¯æ¶ˆæ¯å†å²ï¼Œç§»é™¤ä¸å®Œæ•´çš„tool_calls
            from app.core.message_utils import validate_and_fix_message_history
            clean_messages = validate_and_fix_message_history(list(messages))
            
            response_content = format_cached_response(cache_hit, connection_id)
            ai_message = AIMessage(content=response_content)
            
            updates = {
                "cache_hit": True,
                "cache_hit_type": cache_hit.hit_type,
                "generated_sql": cache_hit.sql,
                "current_stage": "completed",  # âœ… ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„stageå€¼
                "execution_result": SQLExecutionResult(
                    success=cache_hit.result.get("success", True) if isinstance(cache_hit.result, dict) else True,
                    data=cache_hit.result.get("data") if isinstance(cache_hit.result, dict) else cache_hit.result,
                    error=cache_hit.result.get("error") if isinstance(cache_hit.result, dict) else None
                ),
                "messages": clean_messages + [ai_message]  # âœ… ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯å†å²
            }
            
            return updates
        
        else:
            logger.info("ç¼“å­˜æœªå‘½ä¸­ï¼Œç»§ç»­æ­£å¸¸æµç¨‹")
            
            # âœ… å³ä½¿ç¼“å­˜æœªå‘½ä¸­ï¼Œä¹Ÿæ¸…ç†æ¶ˆæ¯å†å²ä¸­çš„ä¸å®Œæ•´tool_calls
            from app.core.message_utils import validate_and_fix_message_history
            clean_messages = validate_and_fix_message_history(list(messages))
            
            return {
                "cache_hit": False,
                "cache_hit_type": None,
                "messages": clean_messages
            }
            
    except Exception as e:
        logger.error(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
        
        # âœ… å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿæ¸…ç†æ¶ˆæ¯å†å²
        from app.core.message_utils import validate_and_fix_message_history
        messages = state.get("messages", [])
        clean_messages = validate_and_fix_message_history(list(messages))
        
        return {
            "cache_hit": False,
            "cache_hit_type": None,
            "messages": clean_messages
        }


def cache_check_node_sync(state: SQLMessageState) -> Dict[str, Any]:
    """
    ç¼“å­˜æ£€æŸ¥èŠ‚ç‚¹çš„åŒæ­¥åŒ…è£…å™¨
    
    ç”¨äºåœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨å¼‚æ­¥çš„ cache_check_node
    """
    import asyncio
    
    try:
        loop = asyncio.get_running_loop()
        # æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨ run_coroutine_threadsafe
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(
                lambda: asyncio.run(cache_check_node(state))
            )
            return future.result(timeout=10)
    except RuntimeError:
        # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
        return asyncio.run(cache_check_node(state))
