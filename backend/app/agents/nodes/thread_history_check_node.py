"""
Thread å†å²æ£€æŸ¥èŠ‚ç‚¹ (Thread History Check Node)

æ£€æŸ¥åŒä¸€å¯¹è¯ï¼ˆthreadï¼‰å†…æ˜¯å¦æœ‰ç›¸åŒé—®é¢˜çš„å†å²å›ç­”ã€‚
å¦‚æœæ‰¾åˆ°ï¼Œç›´æ¥è¿”å›å†å²ç»“æœï¼Œè·³è¿‡å®Œæ•´æ‰§è¡Œæµç¨‹ã€‚

è¿™æ˜¯ä¸‰çº§ç¼“å­˜ç­–ç•¥çš„ç¬¬ä¸€çº§ï¼š
1. Thread å†å²æ£€æŸ¥ (æœ¬æ–‡ä»¶) - åŒä¸€å¯¹è¯å†…ç›¸åŒé—®é¢˜
2. å…¨å±€ç²¾ç¡®ç¼“å­˜ - query_cache_service
3. å…¨å±€è¯­ä¹‰ç¼“å­˜ - Milvus å‘é‡æ£€ç´¢

å·¥ä½œæµç¨‹:
1. ä»æ¶ˆæ¯å†å²ä¸­æå–å½“å‰ç”¨æˆ·æŸ¥è¯¢
2. éå†å†å²æ¶ˆæ¯ï¼ŒæŸ¥æ‰¾ç›¸åŒé—®é¢˜çš„ Human-AI æ¶ˆæ¯å¯¹
3. å¦‚æœæ‰¾åˆ°ï¼Œå‘é€æµå¼äº‹ä»¶å¹¶è¿”å›å†å²ç»“æœ
4. å¦‚æœæœªæ‰¾åˆ°ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

LangGraph å®˜æ–¹è§„èŒƒ:
- ä½¿ç”¨ StreamWriter å‚æ•°æ³¨å…¥å‘é€æµå¼äº‹ä»¶
- å‚è€ƒ: https://langchain-ai.github.io/langgraph/concepts/streaming/
"""
import logging
import time
import re
from typing import Dict, Any, Optional, List

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.types import StreamWriter

from app.core.state import SQLMessageState

logger = logging.getLogger(__name__)


def normalize_query(content: Any) -> str:
    """
    è§„èŒƒåŒ–æŸ¥è¯¢å†…å®¹ï¼Œç”¨äºæ¯”è¾ƒæ˜¯å¦æ˜¯ç›¸åŒé—®é¢˜
    
    å¤„ç†é€»è¾‘:
    1. æå–æ–‡æœ¬å†…å®¹ï¼ˆæ”¯æŒå­—ç¬¦ä¸²ã€åˆ—è¡¨ã€å­—å…¸æ ¼å¼ï¼‰
    2. è½¬å°å†™
    3. ç§»é™¤å¤šä½™ç©ºæ ¼
    4. ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰
    
    Args:
        content: æ¶ˆæ¯å†…å®¹ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å­—å…¸ï¼‰
        
    Returns:
        è§„èŒƒåŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    if content is None:
        return ""
    
    # æå–æ–‡æœ¬å†…å®¹
    if isinstance(content, str):
        text = content
    elif isinstance(content, list):
        parts = []
        for item in content:
            if isinstance(item, dict):
                if item.get("type") == "text" and item.get("text"):
                    parts.append(str(item.get("text")))
            elif isinstance(item, str):
                parts.append(item)
        text = " ".join(p for p in parts if p).strip()
    elif isinstance(content, dict):
        if content.get("type") == "text" and content.get("text"):
            text = str(content.get("text"))
        else:
            text = str(content)
    else:
        text = str(content)
    
    # è§„èŒƒåŒ–å¤„ç†
    text = text.lower().strip()
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤å¸¸è§æ ‡ç‚¹ï¼ˆä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼‰
    text = re.sub(r'[,.?!;:\'\"ã€‚ï¼Œï¼Ÿï¼ï¼›ï¼š""'']+', '', text)
    
    return text


def extract_current_query(state: SQLMessageState) -> Optional[str]:
    """
    ä»çŠ¶æ€ä¸­æå–å½“å‰ç”¨æˆ·æŸ¥è¯¢ï¼ˆæœ€æ–°çš„ Human æ¶ˆæ¯ï¼‰
    
    Args:
        state: å½“å‰çŠ¶æ€
        
    Returns:
        å½“å‰ç”¨æˆ·æŸ¥è¯¢ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    messages = state.get("messages", [])
    
    # ä»åå‘å‰æŸ¥æ‰¾æœ€æ–°çš„ Human æ¶ˆæ¯
    for msg in reversed(messages):
        if hasattr(msg, 'type') and msg.type == 'human':
            return normalize_query(msg.content)
        elif isinstance(msg, HumanMessage):
            return normalize_query(msg.content)
    
    return None


def find_historical_response(
    messages: List, 
    query: str,
    current_index: int
) -> Optional[Dict[str, Any]]:
    """
    åœ¨å†å²æ¶ˆæ¯ä¸­æŸ¥æ‰¾ä¸æŒ‡å®šæŸ¥è¯¢ç›¸åŒçš„é—®é¢˜åŠå…¶å›ç­”
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        query: è§„èŒƒåŒ–åçš„å½“å‰æŸ¥è¯¢
        current_index: å½“å‰æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆæ’é™¤å®ƒä¹‹åçš„æ¶ˆæ¯ï¼‰
        
    Returns:
        å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›åŒ…å«å†å²å›ç­”ä¿¡æ¯çš„å­—å…¸ï¼›å¦åˆ™è¿”å› None
    """
    for i, msg in enumerate(messages[:current_index]):
        # åªæ£€æŸ¥ Human æ¶ˆæ¯
        if not (hasattr(msg, 'type') and msg.type == 'human'):
            if not isinstance(msg, HumanMessage):
                continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸åŒé—®é¢˜
        historical_query = normalize_query(msg.content)
        if historical_query != query:
            continue
        
        logger.debug(f"æ‰¾åˆ°å†å²ç›¸åŒé—®é¢˜: index={i}, query='{query[:50]}...'")
        
        # æŸ¥æ‰¾è¿™ä¸ªé—®é¢˜ä¹‹åçš„ AI å›ç­”
        ai_responses = []
        tool_messages = []
        execution_result = None
        generated_sql = None
        
        for j in range(i + 1, len(messages)):
            next_msg = messages[j]
            
            # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ª Human æ¶ˆæ¯ï¼Œåœæ­¢æœç´¢
            if hasattr(next_msg, 'type') and next_msg.type == 'human':
                break
            if isinstance(next_msg, HumanMessage):
                break
            
            # æ”¶é›† AI æ¶ˆæ¯
            if hasattr(next_msg, 'type') and next_msg.type == 'ai':
                ai_responses.append(next_msg)
                # å°è¯•ä» AI æ¶ˆæ¯ä¸­æå– SQL
                if hasattr(next_msg, 'content') and '```sql' in str(next_msg.content).lower():
                    sql_match = re.search(r'```sql\s*(.*?)\s*```', str(next_msg.content), re.DOTALL | re.IGNORECASE)
                    if sql_match:
                        generated_sql = sql_match.group(1).strip()
            elif isinstance(next_msg, AIMessage):
                ai_responses.append(next_msg)
                if '```sql' in str(next_msg.content).lower():
                    sql_match = re.search(r'```sql\s*(.*?)\s*```', str(next_msg.content), re.DOTALL | re.IGNORECASE)
                    if sql_match:
                        generated_sql = sql_match.group(1).strip()
            
            # æ”¶é›† Tool æ¶ˆæ¯
            if isinstance(next_msg, ToolMessage):
                tool_messages.append(next_msg)
                # å°è¯•ä» ToolMessage ä¸­æå–æ‰§è¡Œç»“æœ
                if getattr(next_msg, 'name', '') == 'execute_sql_query':
                    try:
                        import json
                        tool_content = next_msg.content
                        if isinstance(tool_content, str):
                            parsed = json.loads(tool_content)
                            if isinstance(parsed, dict) and parsed.get("success"):
                                execution_result = parsed
                    except Exception:
                        pass
        
        # å¦‚æœæ‰¾åˆ°äº† AI å›ç­”
        if ai_responses:
            return {
                "found": True,
                "historical_index": i,
                "ai_responses": ai_responses,
                "tool_messages": tool_messages,
                "execution_result": execution_result,
                "generated_sql": generated_sql
            }
    
    return None


def thread_history_check_node(state: SQLMessageState, writer: StreamWriter) -> Dict[str, Any]:
    """
    Thread å†å²æ£€æŸ¥èŠ‚ç‚¹ - LangGraph æ ‡å‡†èŠ‚ç‚¹å‡½æ•°
    
    éµå¾ª LangGraph å®˜æ–¹è§„èŒƒï¼š
    - ä½¿ç”¨ StreamWriter å‚æ•°æ³¨å…¥å‘é€æµå¼äº‹ä»¶
    - èŠ‚ç‚¹ç­¾å: (state, writer) -> dict
    - å‚è€ƒ: https://langchain-ai.github.io/langgraph/concepts/streaming/
    
    æ£€æŸ¥åŒä¸€ thread å†…æ˜¯å¦æœ‰ç›¸åŒé—®é¢˜çš„å†å²å›ç­”ã€‚
    å¦‚æœæ‰¾åˆ°ï¼Œç›´æ¥è¿”å›å†å²ç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œã€‚
    
    Args:
        state: å½“å‰çš„ SQL æ¶ˆæ¯çŠ¶æ€
        writer: LangGraph StreamWriterï¼Œç”¨äºå‘é€æµå¼äº‹ä»¶
        
    Returns:
        Dict[str, Any]: çŠ¶æ€æ›´æ–°
            - thread_history_hit: æ˜¯å¦å‘½ä¸­å†å²
            - cached_response: å†å²å›ç­”å†…å®¹ï¼ˆå¦‚æœå‘½ä¸­ï¼‰
            - current_stage: å¦‚æœå‘½ä¸­åˆ™ä¸º "completed"
            
    çŠ¶æ€å­—æ®µ:
        è¯»å–:
        - messages: æ¶ˆæ¯å†å²
        - connection_id: æ•°æ®åº“è¿æ¥ID
        
        æ›´æ–°:
        - thread_history_hit: æ˜¯å¦å‘½ä¸­å†å²
        - generated_sql: å†å²ç”Ÿæˆçš„ SQLï¼ˆå¦‚æœæœ‰ï¼‰
        - execution_result: å†å²æ‰§è¡Œç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    """
    logger.info("=== è¿›å…¥ Thread å†å²æ£€æŸ¥èŠ‚ç‚¹ ===")
    
    start_time = time.time()
    
    # 1. æå–å½“å‰æŸ¥è¯¢
    current_query = extract_current_query(state)
    if not current_query:
        logger.warning("æ— æ³•æå–å½“å‰æŸ¥è¯¢ï¼Œè·³è¿‡å†å²æ£€æŸ¥")
        return {"thread_history_hit": False}
    
    logger.debug(f"å½“å‰æŸ¥è¯¢: '{current_query[:50]}...'")
    
    # 2. è·å–æ¶ˆæ¯å†å²
    messages = state.get("messages", [])
    if len(messages) <= 1:
        logger.info("æ¶ˆæ¯å†å²ä¸è¶³ï¼Œè·³è¿‡å†å²æ£€æŸ¥")
        return {"thread_history_hit": False}
    
    # 3. æŸ¥æ‰¾å½“å‰æ¶ˆæ¯çš„ç´¢å¼•ï¼ˆæœ€åä¸€ä¸ª Human æ¶ˆæ¯ï¼‰
    current_index = len(messages) - 1
    for i in range(len(messages) - 1, -1, -1):
        msg = messages[i]
        if hasattr(msg, 'type') and msg.type == 'human':
            current_index = i
            break
        if isinstance(msg, HumanMessage):
            current_index = i
            break
    
    # 4. åœ¨å†å²ä¸­æŸ¥æ‰¾ç›¸åŒé—®é¢˜
    historical = find_historical_response(messages, current_query, current_index)
    
    elapsed_ms = int((time.time() - start_time) * 1000)
    
    if not historical or not historical.get("found"):
        logger.info(f"Thread å†å²æœªå‘½ä¸­ (è€—æ—¶: {elapsed_ms}ms)")
        return {"thread_history_hit": False}
    
    # 5. å‘½ä¸­å†å²ï¼Œå‘é€æµå¼äº‹ä»¶ï¼ˆä½¿ç”¨æ³¨å…¥çš„ StreamWriterï¼‰
    logger.info(f"Thread å†å²å‘½ä¸­! æ‰¾åˆ°å†å²å›ç­” (è€—æ—¶: {elapsed_ms}ms)")
    
    from app.schemas.stream_events import create_cache_hit_event, create_data_query_event
    
    # ä½¿ç”¨æ³¨å…¥çš„ writer å‘é€ç¼“å­˜å‘½ä¸­äº‹ä»¶
    writer(create_cache_hit_event(
        hit_type="thread_history",
        similarity=1.0,
        original_query=current_query[:100],
        time_ms=elapsed_ms
    ))
    
    # å¦‚æœæœ‰æ‰§è¡Œç»“æœï¼Œå‘é€æ•°æ®æŸ¥è¯¢äº‹ä»¶
    exec_result = historical.get("execution_result")
    if exec_result and exec_result.get("success"):
        data = exec_result.get("data", {})
        columns = data.get("columns", [])
        raw_rows = data.get("data", [])
        row_count = data.get("row_count", len(raw_rows))
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        rows = []
        for raw_row in raw_rows:
            if isinstance(raw_row, list) and len(raw_row) == len(columns):
                rows.append(dict(zip(columns, raw_row)))
            elif isinstance(raw_row, dict):
                rows.append(raw_row)
        
        writer(create_data_query_event(
            columns=columns,
            rows=rows[:100],
            row_count=row_count,
            chart_config=None,
            title="å†å²æŸ¥è¯¢ç»“æœ"
        ))
    
    # 6. æ„å»ºè¿”å›ç»“æœ
    # å¤åˆ¶å†å² AI å›ç­”åˆ°å½“å‰æ¶ˆæ¯
    ai_responses = historical.get("ai_responses", [])
    tool_messages = historical.get("tool_messages", [])
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ AI æ¶ˆæ¯ï¼Œè¡¨æ˜è¿™æ˜¯ä»å†å²ä¸­è·å–çš„
    if ai_responses:
        last_ai = ai_responses[-1]
        content = last_ai.content if hasattr(last_ai, 'content') else ""
        
        # æ·»åŠ å†å²å›ç­”æ ‡è®°
        history_note = "\n\n> ğŸ’¡ *æ­¤å›ç­”æ¥è‡ªå†å²å¯¹è¯è®°å½•*"
        if isinstance(content, str) and history_note not in content:
            content = content + history_note
        
        new_ai_message = AIMessage(content=content)
        
        # ä¼˜åŒ–ï¼šæˆªæ–­ execution_result æ•°æ®ï¼Œå‡å°‘ checkpoint å­˜å‚¨
        MAX_CHECKPOINT_ROWS = 100
        historical_exec_result = historical.get("execution_result")
        if historical_exec_result and isinstance(historical_exec_result, dict):
            raw_data = historical_exec_result.get("data")
            if raw_data:
                truncated_data = None
                if isinstance(raw_data, dict):
                    truncated_data = {
                        "columns": raw_data.get("columns", []),
                        "data": raw_data.get("data", [])[:MAX_CHECKPOINT_ROWS],
                        "row_count": raw_data.get("row_count", 0)
                    }
                elif isinstance(raw_data, list):
                    truncated_data = raw_data[:MAX_CHECKPOINT_ROWS]
                else:
                    truncated_data = raw_data
                historical_exec_result = {**historical_exec_result, "data": truncated_data}
        
        return {
            "thread_history_hit": True,
            "messages": [new_ai_message],
            "generated_sql": historical.get("generated_sql"),
            "execution_result": historical_exec_result,
            "current_stage": "completed"
        }
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° AI å›ç­”ï¼Œä»ç„¶æ ‡è®°ä¸ºæœªå‘½ä¸­
    logger.warning("æ‰¾åˆ°å†å²é—®é¢˜ä½†æ²¡æœ‰ AI å›ç­”ï¼Œæ ‡è®°ä¸ºæœªå‘½ä¸­")
    return {"thread_history_hit": False}


# ============================================================================
# å¯¼å‡º
# ============================================================================

__all__ = [
    "thread_history_check_node",
    "normalize_query",
    "extract_current_query",
    "find_historical_response",
]
