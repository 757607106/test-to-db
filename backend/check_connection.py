import sys
import os
from pathlib import Path

# æ·»åŠ  backend ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from app.core.llms import get_default_model
from langchain_core.messages import HumanMessage

def check():
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ LLM è¿æ¥...")
    try:
        # è·å–é…ç½®çš„æ¨¡å‹
        llm = get_default_model()
        print(f"ğŸ¤– æ¨¡å‹: {llm.model_name}")
        print(f"ğŸ“¦ ç±»å‹: {type(llm).__name__}")
        
        # æ‰“å°è¿æ¥åœ°å€ä¿¡æ¯
        if hasattr(llm, "base_url"):
             print(f"ğŸŒ Base URL: {llm.base_url}")
        elif hasattr(llm, "api_base"):
             print(f"ğŸŒ API Base: {llm.api_base}")
        
        print("ğŸ“¨ å‘é€æµ‹è¯•æ¶ˆæ¯...")
        response = llm.invoke([HumanMessage(content="Hello, return 'OK' if you see this.")])
        
        print("-" * 30)
        print(f"âœ… è¿æ¥æˆåŠŸ! å›å¤: {response.content}")
        print("-" * 30)
        
    except Exception as e:
        print("\nâŒ è¿æ¥å¤±è´¥!")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        print("\nå»ºè®®:")
        print("1. æ£€æŸ¥ .env ä¸­çš„ OPENAI_API_KEY å’Œ OPENAI_API_BASE")
        print("2. ç¡®ä¿ç½‘ç»œå¯ä»¥è®¿é—®è¯¥ API åœ°å€")
        print("3. å¦‚æœä½¿ç”¨ DeepSeekï¼Œå°è¯•å°† LLM_PROVIDER è®¾ç½®ä¸º openai å¹¶ä½¿ç”¨ DeepSeek çš„ URL")

if __name__ == "__main__":
    check()
